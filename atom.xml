<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Software, Physics, Data, Mountains]]></title>
  <link href="http://markmims.com/atom.xml" rel="self"/>
  <link href="http://markmims.com/"/>
  <updated>2023-10-05T14:41:44+00:00</updated>
  <id>http://markmims.com/</id>
  <author>
    <name><![CDATA[Mark M Mims, Ph.D.]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Editing Screencasts on Ubuntu using `blender`]]></title>
    <link href="http://markmims.com/mids/ubuntu/2020/08/03/edit-screencasts-with-blender.html"/>
    <updated>2020-08-03T00:00:00+00:00</updated>
    <id>http://markmims.com/mids/ubuntu/2020/08/03/edit-screencasts-with-blender</id>
    <content type="html"><![CDATA[<p>In this post I describe how we used <a href="blender.org">blender</a> on an Ubuntu desktop
to edit screencasts for use as async materials for our online Data Engineering
course.
This one just describes the editing process itself.
Screencast capture and the overall screencast development process are covered
in separate posts.</p>

<!--more-->

<h2 id="contents">Contents</h2>

<ul>
  <li>Overview</li>
  <li>Blender</li>
  <li>Setup</li>
  <li>Imports</li>
  <li>Audio</li>
  <li>Story and Context</li>
  <li>Transitions</li>
  <li>Render</li>
  <li>Transcode</li>
  <li>Cover</li>
  <li>Uploads</li>
</ul>

<h2 id="overview">Overview</h2>

<p>In a
<a href="https://markmims.com/mids/ubuntu/2020/07/29/screencasts-with-ffmpeg.html">previous post</a>
I described how to capture multiple monitors worth of screencast material.  One
video stream for slides, one for camera footage of the presenter, and a third
for interactive demos of the terminal and web interfaces.  We also recorded a
single audio stream of narrative.</p>

<p>Each of these video streams alone don&#8217;t tell the full story because we might be
switching context from slides to a demo and back to slides.  Also, there are
times when it might be useful to see some hands waving from a talking head :-)</p>

<p>Sure, the simplest model is to just record a single stream and swap windows
back and forth as the context changes.  However, I&#8217;ve learned my lessons trying
to fix videos and music over the years and wanted the flexibility to adjust
context during post-production.  What if I wanted to add a new diagram to
better explain something?  What if I forgot to go to the next slide trasition
to indicate a new topic?</p>

<p>Turns out it&#8217;s not too hard, or expensive, to keep that flexibility by simply
capturing the multiple video streams first, and then editing these together as
a separate stepinto a polished presentation that effectively communicates the
material.</p>

<h2 id="blender">Blender</h2>

<p>You might have heard of blender.blender</p>

<h2 id="setup">Setup</h2>

<h2 id="inputs">Inputs</h2>

<p>The folder structure we use follows a pattern of
<code>scene -&gt; shot -&gt; take</code> so here we&#8217;ll have
<code>screencast_name -&gt; shot -&gt; take</code>:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">- intro-to-data-science
</span><span class="line">  - shot-010
</span><span class="line">    - take-2018-03-12-165010
</span><span class="line">      - audio.log
</span><span class="line">      - audio.wav
</span><span class="line">      - slides.log
</span><span class="line">      - slides.mkv
</span><span class="line">      - terminal.log
</span><span class="line">      - terminal.mkv
</span><span class="line">      - webcam.log
</span><span class="line">      - webcam.mkv
</span><span class="line">    - take-2018-03-12-165422
</span><span class="line">      - ...
</span><span class="line">  - shot-020
</span><span class="line">  - ...
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>I usually label the best take of a shot as <code>finalized</code> as they&#8217;re captured.</p>

<p>For editing, import the streams from the finalized take of each shot of the
screencast.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Capturing Screencasts on Ubuntu using `ffmpeg`]]></title>
    <link href="http://markmims.com/mids/ubuntu/2020/07/29/screencasts-with-ffmpeg.html"/>
    <updated>2020-07-29T00:00:00+00:00</updated>
    <id>http://markmims.com/mids/ubuntu/2020/07/29/screencasts-with-ffmpeg</id>
    <content type="html"><![CDATA[<p>In this post I describe the capture setup we used to create screencasts using
an Ubuntu Linux desktop. This one just describes the recording process itself.
I&#8217;ll go over the overall screencast development process and editing in separate
posts.</p>

<!--more-->

<h2 id="contents">Contents</h2>

<ul>
  <li>Overview</li>
  <li>Desktop</li>
  <li>Slides</li>
  <li>Terminal</li>
  <li>Camera</li>
  <li>Audio</li>
  <li>Putting it all together</li>
</ul>

<h2 id="overview">Overview</h2>

<p>These scripts were created to record screencasts for a class on Data
Engineering, so they&#8217;ll need to cover both high-level conceptual material as
well as detailed examples or tutorials.</p>

<p>To do that we really wanted to have the flexibility of showing both slides as
well as terminal or web interactions at the same time. We also figured it&#8217;s a
good idea to have the ability to overlay a talking head when there&#8217;s not much
other detailed interaction going on, so we also wanted to be sure we captured
camera footage during the recordings as well.</p>

<p>This setup is designed to capture raw footage of all of those channels at once.
We looked around, but couldn&#8217;t find any off-the-shelf tools that really met our
needs for this.  It turns out this is actually pretty easy to accomplish just
using <code>ffmpeg</code> directly from a script.</p>

<h2 id="desktop">Desktop</h2>

<p>Note the desktop setup:</p>

<ul>
  <li>
    <p>Ubuntu desktop with three monitors set up within a single X session.  You&#8217;ll
want at least two to capture both slides and terminal/web at once</p>
  </li>
  <li>
    <p>Each desktop is <code>1920x1080</code>, so the total big desktop size is <code>3x1920</code> pixels
wide and 1080 pixels tall</p>
  </li>
  <li>
    <p>Terminals/Browsers run on the left-hand monitor</p>
  </li>
  <li>
    <p>Slides are full-screen on the center monitor</p>
  </li>
  <li>
    <p>Webcam lives on top of the left monitor so we&#8217;re looking roughly towards the
camera when going through a detailed example</p>
  </li>
  <li>
    <p>Sound is coming from a lavalier mic plugged into a USB audio interface made
available via standard Linux alsa devices</p>
  </li>
  <li>
    <p>I use the right-hand monitor to hold terminal windows to start/stop these scripts,
but nothing from there is recorded</p>
  </li>
</ul>

<p>Below, we&#8217;ll go through each of the different capture channels used 
and then wrap it all up with a bow into a single script that follows
the <code>screencasts -&gt; shots -&gt; takes</code> file organization that we used to keep
track of all of this.</p>

<h2 id="slides">Slides</h2>

<p>To capture a stream of slides, we&#8217;re using the <code>x11grab</code> ffmpeg interface. This
is designed to just sample what the X server sees every so often (<code>$framerate</code>)
and then encode and save that as a video stream.</p>

<p>The tricky part is creating a command to record the correct monitor for slides.
Since the middle monitor is running slides, we tell <code>ffmpeg</code> to capture a
single monitor&#8217;s <code>1920x1080</code> worth of screen but <em>start</em> that from the geometry
offset <code>+1920,0</code>&#8230; the top of the middle monitor.</p>

<p>The command </p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">ffmpeg <span class="se">\</span>
</span><span class="line">  -hide_banner -nostats -loglevel warning <span class="se">\</span>
</span><span class="line">  -f x11grab -r <span class="nv">$framerate</span> -s hd1080 -i :0.0+1920,0 <span class="se">\</span>
</span><span class="line">  -vcodec libx264 <span class="se">\</span>
</span><span class="line">  -preset ultrafast <span class="se">\</span>
</span><span class="line">  <span class="nv">$output_dir</span>/slides.mkv &gt; <span class="nv">$output_dir</span>/slides.log 2&gt;&amp;1
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>gets wrapped in a bash function to capture slides:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">capture_slides<span class="o">()</span> <span class="o">{</span>
</span><span class="line">  <span class="nb">local </span><span class="nv">output_dir</span><span class="o">=</span><span class="nv">$1</span>
</span><span class="line">  ~/bin/ffmpeg <span class="se">\</span>
</span><span class="line">    -hide_banner -nostats -loglevel warning <span class="se">\</span>
</span><span class="line">    -f x11grab -r <span class="nv">$framerate</span> -s hd1080 -i :0.0+1920,0 <span class="se">\</span>
</span><span class="line">    -vcodec libx264 <span class="se">\</span>
</span><span class="line">    -preset ultrafast <span class="se">\</span>
</span><span class="line">    <span class="nv">$output_dir</span>/slides.mkv &gt; <span class="nv">$output_dir</span>/slides.log 2&gt;&amp;1
</span><span class="line">  <span class="nb">echo</span> <span class="s2">&quot;slides done&quot;</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>This saves to the files <code>slides.mkv</code> and <code>slides.log</code>.</p>

<h2 id="terminal">Terminal</h2>

<p>We&#8217;ll use <code>x11grab</code> to record the left-hand monitor as well. The offset here is
just the top of the left-hand monitor, so <code>+0,0</code> in X geometry speak:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">capture_terminal<span class="o">()</span> <span class="o">{</span>
</span><span class="line">  <span class="nb">local </span><span class="nv">output_dir</span><span class="o">=</span><span class="nv">$1</span>
</span><span class="line">  ~/bin/ffmpeg <span class="se">\</span>
</span><span class="line">    -hide_banner -nostats -loglevel warning <span class="se">\</span>
</span><span class="line">    -f x11grab -r <span class="nv">$framerate</span> -s hd1080 -i :0.0+0,0 <span class="se">\</span>
</span><span class="line">    -vcodec libx264 <span class="se">\</span>
</span><span class="line">    -preset ultrafast <span class="se">\</span>
</span><span class="line">    <span class="nv">$output_dir</span>/terminal.mkv &gt; <span class="nv">$output_dir</span>/terminal.log 2&gt;&amp;1
</span><span class="line">  <span class="nb">echo</span> <span class="s2">&quot;terminal done&quot;</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>This saves to the files <code>terminal.mkv</code> and <code>terminal.log</code>.</p>

<h2 id="camera">Camera</h2>

<p>To capture the stream from the webcam, we&#8217;re relying heavily on the fact that
the 
<a href="https://www.amazon.com/gp/product/B006JH8T3S/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&amp;psc=1">Logitech HD Pro Webcam C920</a>
does hardware h264 encoding on the fly and we&#8217;re just tapping into that using
ffmpeg&#8217;s <code>v4l2</code> interface to simply <code>copy</code> the video stream out to a file.</p>

<p>I also had some problems understanding the timestamps that the camera&#8217;s
hardware encoder used, so I include the set of <code>ffmpeg</code> args that fixed that.
YMMV depending on your camera.</p>

<p>Probably the most important thing to recognize is that the capture relied on
the hardware encoding.  If we were getting raw video and having to encode on
the fly, then the desktop&#8217;s computational capabilities my come more into play.
This usually results is limiting the framerate you can actually record.</p>

<p>Here&#8217;s the function to capture the camera footage:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">capture_webcam<span class="o">()</span> <span class="o">{</span>
</span><span class="line">  <span class="nb">local </span><span class="nv">output_dir</span><span class="o">=</span><span class="nv">$1</span>
</span><span class="line">  ~/bin/ffmpeg <span class="se">\</span>
</span><span class="line">    -hide_banner -nostats -loglevel warning <span class="se">\</span>
</span><span class="line">    -f v4l2 -framerate <span class="nv">$framerate</span> -input_format h264 -video_size hd1080 -ts mono2abs -i /dev/video0 <span class="se">\</span>
</span><span class="line">    -c copy -copyts -start_at_zero <span class="se">\</span>
</span><span class="line">    <span class="nv">$output_dir</span>/webcam.mkv &gt; <span class="nv">$output_dir</span>/webcam.log 2&gt;&amp;1
</span><span class="line">  <span class="nb">echo</span> <span class="s2">&quot;webcam done&quot;</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>This saves to <code>webcam.mkv</code> and <code>webcam.log</code>.</p>

<h2 id="audio">Audio</h2>

<p>Audio is coming in through a 
<a href="https://www.amazon.com/gp/product/B00MIXF2RS/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&amp;psc=1">TASCAM US-2x2</a>
USB-audio interface, where I have a 
<a href="https://www.amazon.com/gp/product/B01GSVQN6Y/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&amp;psc=1">lavalier mic</a>
plugged in.  This &#8220;just worked&#8221; through the <code>alsa</code> interface for <code>ffmpeg</code> so we
just need to copy the raw audio stream from the device:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">capture_audio<span class="o">()</span> <span class="o">{</span>
</span><span class="line">  <span class="nb">local </span><span class="nv">output_dir</span><span class="o">=</span><span class="nv">$1</span>
</span><span class="line">  ~/bin/ffmpeg <span class="se">\</span>
</span><span class="line">    -hide_banner -nostats -loglevel warning <span class="se">\</span>
</span><span class="line">    -f alsa -i default <span class="se">\</span>
</span><span class="line">    -c copy -copyts -start_at_zero <span class="se">\</span>
</span><span class="line">    <span class="nv">$output_dir</span>/audio.wav &gt; <span class="nv">$output_dir</span>/audio.log 2&gt;&amp;1
</span><span class="line">  <span class="nb">echo</span> <span class="s2">&quot;audio done&quot;</span>
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>which saves <code>audio.wav</code> and <code>audio.log</code>.</p>

<h2 id="putting-this-all-together">Putting this all together</h2>

<p>So all of the above functions get rolled up into a single script named
<code>capture</code>.</p>

<p>This script kicks off the <code>ffmpeg</code> recordings at roughly the same
time and saves all the output to</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line"><span class="nv">output_dir</span><span class="o">=</span><span class="s2">&quot;screencasts/${scene_name}/shot-${shot_number}/take-${timestamp}&quot;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>where the variables in there either are defaults (like the shot number) or are
specified as arguments to the script.  I typically use it like</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line"><span class="nb">cd</span> /opt/screencasts/introducing-spark-streaming
</span><span class="line">capture
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>which kicks off the recording and streams outputs to files such as</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">- shot-010
</span><span class="line">  - take-2018-03-12-165010
</span><span class="line">    - audio.log
</span><span class="line">    - audio.wav
</span><span class="line">    - slides.log
</span><span class="line">    - slides.mkv
</span><span class="line">    - terminal.log
</span><span class="line">    - terminal.mkv
</span><span class="line">    - webcam.log
</span><span class="line">    - webcam.mkv
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>This folder structure lets us keep things nice and tidy for editing.</p>

<p>So here&#8217;s the final script:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
<span class="line-number">61</span>
<span class="line-number">62</span>
<span class="line-number">63</span>
<span class="line-number">64</span>
<span class="line-number">65</span>
<span class="line-number">66</span>
<span class="line-number">67</span>
<span class="line-number">68</span>
<span class="line-number">69</span>
<span class="line-number">70</span>
<span class="line-number">71</span>
<span class="line-number">72</span>
<span class="line-number">73</span>
<span class="line-number">74</span>
<span class="line-number">75</span>
<span class="line-number">76</span>
<span class="line-number">77</span>
<span class="line-number">78</span>
<span class="line-number">79</span>
<span class="line-number">80</span>
<span class="line-number">81</span>
<span class="line-number">82</span>
<span class="line-number">83</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line"><span class="c">#!/bin/bash</span>
</span><span class="line">
</span><span class="line"><span class="nb">set</span> -o errexit -o nounset -o pipefail
</span><span class="line">
</span><span class="line">usage<span class="o">()</span> <span class="o">{</span>
</span><span class="line">	<span class="nb">echo</span> <span class="s2">&quot;Usage: $0 &lt;scene_name&gt; [&lt;shot_number&gt;]</span>
</span><span class="line"><span class="s2">    where:</span>
</span><span class="line"><span class="s2">      &lt;scene_name&gt; is something like intro-what-is-data-eng</span>
</span><span class="line"><span class="s2">      &lt;shot_number&gt; optional, default is 010&quot;</span>
</span><span class="line"><span class="o">}</span>
</span><span class="line"><span class="o">((</span> <span class="nv">$# </span>&lt; 1 <span class="o">))</span> <span class="o">&amp;&amp;</span> usage <span class="o">&amp;&amp;</span> <span class="nb">exit </span>1
</span><span class="line"><span class="nv">scene_name</span><span class="o">=</span><span class="nv">$1</span>
</span><span class="line"><span class="nv">shot_number</span><span class="o">=</span><span class="k">${</span><span class="nv">2</span><span class="k">:-</span><span class="nv">010</span><span class="k">}</span>
</span><span class="line">
</span><span class="line"><span class="nv">timestamp</span><span class="o">=</span><span class="sb">`</span>date +%Y-%m-%d-%H%M%S<span class="sb">`</span>
</span><span class="line"><span class="nv">output_dir</span><span class="o">=</span><span class="s2">&quot;screencasts/${scene_name}/shot-${shot_number}/take-${timestamp}&quot;</span>
</span><span class="line"><span class="nv">framerate</span><span class="o">=</span>30
</span><span class="line">
</span><span class="line">capture_slides<span class="o">()</span> <span class="o">{</span>
</span><span class="line">  <span class="nb">local </span><span class="nv">output_dir</span><span class="o">=</span><span class="nv">$1</span>
</span><span class="line">  ~/bin/ffmpeg <span class="se">\</span>
</span><span class="line">    -hide_banner -nostats -loglevel warning <span class="se">\</span>
</span><span class="line">    -f x11grab -r <span class="nv">$framerate</span> -s hd1080 -i :0.0+1920,0 <span class="se">\</span>
</span><span class="line">    -vcodec libx264 <span class="se">\</span>
</span><span class="line">    -preset ultrafast <span class="se">\</span>
</span><span class="line">    <span class="nv">$output_dir</span>/slides.mkv &gt; <span class="nv">$output_dir</span>/slides.log 2&gt;&amp;1
</span><span class="line">  <span class="nb">echo</span> <span class="s2">&quot;slides done&quot;</span>
</span><span class="line"><span class="o">}</span>
</span><span class="line">
</span><span class="line">capture_terminal<span class="o">()</span> <span class="o">{</span>
</span><span class="line">  <span class="nb">local </span><span class="nv">output_dir</span><span class="o">=</span><span class="nv">$1</span>
</span><span class="line">  ~/bin/ffmpeg <span class="se">\</span>
</span><span class="line">    -hide_banner -nostats -loglevel warning <span class="se">\</span>
</span><span class="line">    -f x11grab -r <span class="nv">$framerate</span> -s hd1080 -i :0.0+0,0 <span class="se">\</span>
</span><span class="line">    -vcodec libx264 <span class="se">\</span>
</span><span class="line">    -preset ultrafast <span class="se">\</span>
</span><span class="line">    <span class="nv">$output_dir</span>/terminal.mkv &gt; <span class="nv">$output_dir</span>/terminal.log 2&gt;&amp;1
</span><span class="line">  <span class="nb">echo</span> <span class="s2">&quot;terminal done&quot;</span>
</span><span class="line"><span class="o">}</span>
</span><span class="line">
</span><span class="line">capture_audio<span class="o">()</span> <span class="o">{</span>
</span><span class="line">  <span class="nb">local </span><span class="nv">output_dir</span><span class="o">=</span><span class="nv">$1</span>
</span><span class="line">  ~/bin/ffmpeg <span class="se">\</span>
</span><span class="line">    -hide_banner -nostats -loglevel warning <span class="se">\</span>
</span><span class="line">    -f alsa -i default <span class="se">\</span>
</span><span class="line">    -c copy -copyts -start_at_zero <span class="se">\</span>
</span><span class="line">    <span class="nv">$output_dir</span>/audio.wav &gt; <span class="nv">$output_dir</span>/audio.log 2&gt;&amp;1
</span><span class="line">  <span class="nb">echo</span> <span class="s2">&quot;audio done&quot;</span>
</span><span class="line"><span class="o">}</span>
</span><span class="line">
</span><span class="line">capture_webcam<span class="o">()</span> <span class="o">{</span>
</span><span class="line">  <span class="nb">local </span><span class="nv">output_dir</span><span class="o">=</span><span class="nv">$1</span>
</span><span class="line">  ~/bin/ffmpeg <span class="se">\</span>
</span><span class="line">    -hide_banner -nostats -loglevel warning <span class="se">\</span>
</span><span class="line">    -f v4l2 -framerate <span class="nv">$framerate</span> -input_format h264 -video_size hd1080 -ts mono2abs -i /dev/video0 <span class="se">\</span>
</span><span class="line">    -c copy -copyts -start_at_zero <span class="se">\</span>
</span><span class="line">    <span class="nv">$output_dir</span>/webcam.mkv &gt; <span class="nv">$output_dir</span>/webcam.log 2&gt;&amp;1
</span><span class="line">  <span class="nb">echo</span> <span class="s2">&quot;webcam done&quot;</span>
</span><span class="line"><span class="o">}</span>
</span><span class="line">
</span><span class="line"><span class="c">##################</span>
</span><span class="line">
</span><span class="line"><span class="nb">echo</span> <span class="s2">&quot;starting ${scene_name}/shot-${shot_number}/take-${timestamp}&quot;</span>
</span><span class="line">
</span><span class="line">mkdir -p <span class="nv">$output_dir</span>
</span><span class="line">
</span><span class="line"><span class="nb">echo</span> <span class="s2">&quot;capturing slides&quot;</span>
</span><span class="line">capture_slides <span class="nv">$output_dir</span> &amp;
</span><span class="line">
</span><span class="line"><span class="nb">echo</span> <span class="s2">&quot;capturing terminal&quot;</span>
</span><span class="line">capture_terminal <span class="nv">$output_dir</span> &amp;
</span><span class="line">
</span><span class="line"><span class="nb">echo</span> <span class="s2">&quot;capturing webcam&quot;</span>
</span><span class="line">capture_webcam <span class="nv">$output_dir</span> &amp;
</span><span class="line">
</span><span class="line"><span class="nb">echo</span> <span class="s2">&quot;capturing audio&quot;</span>
</span><span class="line">capture_audio <span class="nv">$output_dir</span> &amp;
</span><span class="line">
</span><span class="line"><span class="k">for </span>job in <span class="sb">`</span><span class="nb">jobs</span> -p<span class="sb">`</span>; <span class="k">do</span>
</span><span class="line"><span class="k">  </span><span class="nb">wait</span> <span class="nv">$job</span>
</span><span class="line"><span class="k">done</span>
</span><span class="line">
</span><span class="line"><span class="nb">echo</span> <span class="s2">&quot;done&quot;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Note that each function is run in the background so they&#8217;re effectively kicked
off in parallel.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Projects in GCP using Central Billing Accounts]]></title>
    <link href="http://markmims.com/devops/cloud/gcp/iam/2019/06/05/managed-projects-in-gcp.html"/>
    <updated>2019-06-05T00:00:00+00:00</updated>
    <id>http://markmims.com/devops/cloud/gcp/iam/2019/06/05/managed-projects-in-gcp</id>
    <content type="html"><![CDATA[<p>Many organizations recognize the benefits of empowering their developers. In a
cloud environment, that often means giving developers the ability to create and
manage their own infrastructure.</p>

<p>Of course, developers can easily create their own individual or G-Suite GCP
accounts.  They can take advantage of the free trial that Google Cloud offers.
That&#8217;s great, and everything&#8217;s hunky-dory until the credit runs out. What then?</p>

<p>In this post I describe a really simple way to set up and use centralized
billing on GCP&#8230; even across external development accounts.  Way better than
trying to get me to fill out expense reports for infradev!</p>

<!--more-->

<h2 id="contents">Contents</h2>

<ul>
  <li>Organizations and account setup</li>
  <li>Users and IAM roles</li>
  <li>Terraform templates</li>
  <li>Try it out</li>
</ul>

<h2 id="organizations-and-account-setup">Organizations and account setup</h2>

<p>Let&#8217;s consider a common example with two separate organizations in the mix.</p>

<ol>
  <li>
    <p>A <code>bigcorp.com</code> organization that&#8217;s footing the bill for everything</p>
  </li>
  <li>
    <p>An individual developer&#8217;s G-Suite organization, <code>pinkponies.io</code>, where
we&#8217;ll be doing the development</p>
  </li>
</ol>

<p>In this example, we&#8217;re assuming the developer organization <code>pinkponies.io</code> is a
full G-Suite account and not just an ordinary GCP account created using a
single email.</p>

<p>It&#8217;s easy for an individual developer to create a new G-Suite account and that
turns out to be the more typical situation for this kind of cross billing
example. I also really recommend using developer G-Suite accounts for cloud
development in general since they&#8217;ll have the same IAM capabilities and
concerns as the <code>bigcorp.com</code> account.</p>

<h2 id="users-and-iam-roles">Users and IAM roles</h2>

<p>Each developer will need accounts in both orgs to start with.</p>

<p>Take Sam for example. Sam&#8217;s already an Owner of <code>pinkponies.io</code>&#8230;  with
<code>sam@pinkponies.io</code> as a login.</p>

<p>Sam works for BigCorp and is also <code>sam@bigcorp.com</code> where they live in some
folder within the <code>bigcorp.com</code> organization&#8217;s GCP IAM.</p>

<h3 id="in-your-billing-org-bigcorpcom">In your billing org: <code>bigcorp.com</code></h3>

<p>So the <code>billing_account_user</code> (<code>sam@bigcorp.com</code>) needs to be able to create
billing accounts within the BigCorp org.</p>

<p>Sam will need to be assigned a <code>BillingAccountCreator</code> role within the
<code>bigcorp.com</code> org&#8217;s IAM on GCP.</p>

<h3 id="in-your-gsuite-org-pinkponiesio">In your gsuite org: <code>pinkponies.io</code></h3>

<p>It&#8217;s no surprise, the <code>gsuite_user</code> (<code>sam@pinkponies.io</code>) needs to be an
<code>OrganizationAdministrator</code> on that org.</p>

<p>The <code>billing_account_user</code> (<code>sam@bigcorp.com</code>) needs permissions on the
<code>pinkponies.io</code> org too. They need to be:</p>

<ul>
  <li>a <code>BillingAccountAdministrator</code> for the <code>pinkponies.io</code> org</li>
  <li>a <code>ProjectCreator</code> on the <code>pinkponies.io</code> org</li>
  <li>and I added them as an <code>OrganizationAdministrator</code> on <code>pinkponies.org</code> for
good measure</li>
</ul>

<h2 id="terraform-templates">Terraform templates</h2>

<p>I like to manage infrastructure using <a href="terraform.io">Terraform</a> and keep
all my templates and modules checked into GitHub.</p>

<p>The Terraform templates to create these projects are super simple. There&#8217;s a
provider, a resource for the managed project we want to create, and then a
couple of role binding resources</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
</pre></td><td class="code"><pre><code class=""><span class="line">provider "google" {
</span><span class="line">  region      = "${var.region}"
</span><span class="line">}
</span><span class="line">
</span><span class="line">resource "google_project" "gsuite_project" {
</span><span class="line">  name       = "gsuite-project-0"
</span><span class="line">  project_id = "gsuite-project-0"
</span><span class="line">
</span><span class="line">  org_id = "${var.gsuite_org_id}"
</span><span class="line">  billing_account = "${var.billing_account_id}"
</span><span class="line">}
</span><span class="line">
</span><span class="line">resource "google_project_iam_binding" "gsuite_project_owner" {
</span><span class="line">  project = "${google_project.gsuite_project.project_id}"
</span><span class="line">  role    = "roles/owner"
</span><span class="line">
</span><span class="line">  members = [
</span><span class="line">    "user:${var.gsuite_user}",
</span><span class="line">    "user:${var.billing_account_user}",
</span><span class="line">  ]
</span><span class="line">}</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>There&#8217;s no need to get Terraform to slurp in data sources for the GCP orgs,
folders, billing accounts, etc. In this example, we&#8217;ll just create variables
for them</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class=""><span class="line">variable "region" {
</span><span class="line">  default = "us-central1"
</span><span class="line">}
</span><span class="line">
</span><span class="line">variable "billing_account_user" {}
</span><span class="line">variable "billing_folder_id" {}
</span><span class="line">variable "billing_account_id" {}
</span><span class="line">
</span><span class="line">variable "gsuite_user" {}
</span><span class="line">variable "gsuite_org_id" {}</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>and look up the values from the cloud consoles for both our <code>bigcorp.com</code> and
<code>pinkponies.io</code> accounts.  We&#8217;ll add these to <code>terraform.tfvars</code></p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class=""><span class="line">billing_account_user = "sam@bigcorp.com"
</span><span class="line">billing_folder_id = "234567890123" # my-billing-folder
</span><span class="line">billing_account_id = "aaaaaa-bbbbbb-cccccc" # my-billing-account
</span><span class="line">
</span><span class="line">gsuite_user = "sam@pinkponies.io"
</span><span class="line">gsuite_org_id = "345678901234" # pinkponies.io</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Note that there&#8217;s a <code>terraform.tfvars.template</code> included in the example repo
but the actual <code>*.tfvars</code> files, with sensitive account details, are ignored by
revision control so you&#8217;ll have to copy the template and create your own
<code>terraform.tfvars</code>.</p>

<h2 id="try-it-out">Try it out</h2>

<h3 id="example-repo">Example repo</h3>

<p>You can clone and configure the example templates</p>

<ul>
  <li>clone <a href="https://github.com/mmm/gcp-managed-projects">https://github.com/mmm/gcp-managed-projects</a></li>
  <li>copy the tfvars template over to <code>terraform.tfvars</code> and edit it with your info</li>
</ul>

<h3 id="gcloud"><code>gcloud</code></h3>

<p>Terraform&#8217;s provider for GCP needs GCP credentials for your account.  The
easiest thing to do to get that working before trying to run Terraform is to
make sure gcloud is working correctly.</p>

<p>You can do that by installing gcloud and running <code>gcloud init</code> to go through
the oauth dance&#8230; that works.  You&#8217;d need to export your
<code>GOOGLE_APPLICATION_CREDENTIALS</code> as well&#8230; usual stuff.</p>

<p>However, as an easier alternative, use the cloud shell in the cloud console for
your <code>bigcorp.com</code> equivalent account.  The gcloud config and applcation
credentials are all already set up for you.</p>

<p>Side note: The cloud shell is <em>really</em> useful&#8230; check it out if you haven&#8217;t!</p>

<p>Make sure you&#8217;re driving terraform using credentials (your <code>gcloud</code> config)
from the equivalent of your <code>bigcorp.com</code> account and <em>not</em> your
<code>pinkponies.io</code> G-Suite org account.</p>

<h3 id="terraform">Terraform</h3>

<p>Download Terraform from <a href="https://terraform.io/">https://terraform.io/</a>.  Terraform is a standalone
binary so it&#8217;s simple to install&#8230; even in your GCP Cloud Shell.</p>

<p>Init terraform&#8217;s providers and state management</p>

<pre><code>terraform init
</code></pre>

<p>Then check out what changes we&#8217;re _plan_ning to make</p>

<pre><code>terraform plan
</code></pre>

<p>If all looks good from there, then <em>apply</em> that plan to actually create our
project</p>

<pre><code>terraform apply
</code></pre>

<p>Check out the project we just created</p>

<pre><code>gcloud beta billing projects list --billing-account=&lt;billing_account_id&gt;
</code></pre>

<p>Check out the same project from the Cloud Console for your <code>pinkponies.io</code>
G-Suite account.</p>

<p>Now you can use that account within your <code>pinkponies.io</code> G-Suite account and
any charges go straight to your BigCorp billing account.</p>

<h3 id="cleanup">Cleanup</h3>

<p>When you&#8217;re all done, you can clean up after yourself by removing the project
and role bindings we created</p>

<pre><code>terraform destroy
</code></pre>

<p>then deleting the billing account through the Cloud Console.  You could (and
should) totally manage the billing accounts themselves in the bigcorp.org using
Terraform templates as well, but that&#8217;s another story.</p>

<h2 id="disclaimer">Disclaimer</h2>

<p>No big corps or pink ponies were harmed in the production of this post.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Identifying User Activity from Streams of Raw Events]]></title>
    <link href="http://markmims.com/data-engineering/spark/hadoop/devops/2016/03/17/hwtb-sessions.html"/>
    <updated>2016-03-17T12:08:00+00:00</updated>
    <id>http://markmims.com/data-engineering/spark/hadoop/devops/2016/03/17/hwtb-sessions</id>
    <content type="html"><![CDATA[<p>I had a chance to speak at an online conference last weekend,
<a href="http://hadoop.withthebest.com/">Hadoop With The Best</a>.
I had fun sharing one of my total passions&#8230; data pipelines!
In particular, some techniques for catching raw user events,
acting on those events and understanding user activity from 
the sessionization of such events.</p>

<!--more-->

<div class="ratio-4-3 embed-video-container" onclick="var myAnchor = document.getElementById('7geRGzrIXRI');var tmpDiv = document.createElement('div');tmpDiv.innerHTML = '&lt;iframe style=&quot;vertical-align:top;width:100%;height:100%;position:absolute;&quot; src=&quot;http://www.youtube.com/embed/7geRGzrIXRI?autoplay=1&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;';myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;" title="click here to play">
<a class="youtube-lazy-link" style="width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/7geRGzrIXRI/0.jpg) center center no-repeat;background-size:contain;position:absolute" href="http://www.youtube.com/watch?v=7geRGzrIXRI" id="7geRGzrIXRI" onclick="return false;">
<div class="youtube-lazy-link-div"></div>
<div class="youtube-lazy-link-info">embedded youtube video 7geRGzrIXRI</div>
</a>
<div class="video-info">embedded youtube video 7geRGzrIXRI</div>
</div>

<p><a href="svds.com">SVDS</a> is a boutique data science consulting firm.  We help folks
with their hardest Data Strategy, Data Science, and/or Data Engineering
problems.  In this role, we&#8217;re in a unique position to solve different
kinds of problems across various industries&#8230; and start to recognize
the patterns of solution that emerge.  That&#8217;s what I&#8217;d like to share.</p>

<p>This talk is about some common data pipeline patterns used
across various kinds of systems across various industries.
Key Takeaways include:</p>

<ul>
  <li>what&#8217;s needed to understand user activity</li>
  <li>pipeline architectures that support this analysis</li>
</ul>

<p>Along the way, I point out commonalities across business verticals and we see
how volume and latency requirements, unsurprisingly, turn out to be the biggest
differentiators in solution.</p>

<h2 id="agenda">Agenda</h2>

<ul>
  <li>Ingest Events</li>
  <li>Take Action</li>
  <li>Recognize Activity</li>
</ul>

<h2 id="ingest-events">Ingest Events</h2>

<p>The primary goal of an ingestion pipeline is to&#8230; ingest events.  All other
considerations are secondary.  We walk through an example pipeline and discuss
how that architecture changes as we adjust scaling up to handle billions of
events a day.  We&#8217;ll note along the way how general concepts of immutability
and lazy evaluation can have large ramifications on data ingestion pipeline
architecture.</p>

<p>I start out covering typical classes of and types of events, some common event
fields, and various ways that events are represented.  These vary <em>greatly</em>
across current and legacy systems, and you should always expect that munging
will be involved as you&#8217;re working to ingest events from various data sources
over time.</p>

<p>For our sessionization examples, we&#8217;re interested in user events such as
<code>login</code>, <code>checkout</code>, <code>add friend</code>, etc.</p>

<p>These user events can be &#8220;flat&#8221;</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class=""><span class="line">{
</span><span class="line">  "time_utc": "1457741907.959400112",
</span><span class="line">  "user_id": "688b60d1-c361-445b-b2f6-27f2eecfc217",
</span><span class="line">  "event_type": "button_pressed",
</span><span class="line">  "button_type": "one-click purchase",
</span><span class="line">  "item_sku": "1 23456 78999 9",
</span><span class="line">  "item_description": "Tony's Run-flat Tire",
</span><span class="line">  "item_unit_price": ...
</span><span class="line">  ...
</span><span class="line">}</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>or have some structure</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
</pre></td><td class="code"><pre><code class=""><span class="line">{
</span><span class="line">  "time_utc": "1457741907.959400112",
</span><span class="line">  "user_id": "688b60d1-c361-445b-b2f6-27f2eecfc217",
</span><span class="line">  "event_type": "button_pressed",
</span><span class="line">  "event_details": {
</span><span class="line">    "button_type": "one-click purchase",
</span><span class="line">    "puchased_items": [
</span><span class="line">      {
</span><span class="line">        "sku": "1 23456 78999 9",
</span><span class="line">        "description": "Tony's Run-flat Tire",
</span><span class="line">        "unit_price": ...
</span><span class="line">        ...
</span><span class="line">      },
</span><span class="line">    ],
</span><span class="line">  },
</span><span class="line">  ...
</span><span class="line">}</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>and often both formats get used in the same systems in the wild so
you have to intelligently detect or classify events rather than 
just making blatant assumptions about them.  And yes, that <em>is</em> 
expensive&#8230; but it&#8217;s surprisingly common.</p>

<h3 id="ingestion-pipelines">Ingestion Pipelines</h3>

<p>So what do basic ingestion pipelines usually look like?</p>

<p>Tenants to keep in mind here&#8230; build a pipeline that&#8217;s immutable, lazy,
simple/composable, and testable.  I come back to these often throughout
the talk.</p>

<p>With our stated goal
of ingesting events, it should look pretty simple right?  Something along the
lines of
<a href="http://markmims.com/images/event-ingestion-without-streaming-with-filename.svg">
<img src="http://markmims.com/images/event-ingestion-without-streaming-with-filename.svg" width="720px" />
</a></p>

<p>I introduce the &#8220;Power of the Query Side&#8221;&#8230; query-side tools are fast
nowadays.  Tools such as Impala have really won me over.  The Ingest pipeline
needs to get the events as raw as possible as far back as possible in a format
that&#8217;s amenable to fast queries.  Let&#8217;s state that again&#8230; it&#8217;s important.
The pipeline&#8217;s core job is to get events that are as raw as possible (immutable
processing pipeline) as far back into the system as possible (lazily evaluated
analysis) before any expensive computation is done.  Modern query-side tools
support these paradigms quite well.  Better performance is obtained when events
land in query-optimized formats and are grouped into query-optimized files and
partitions where possible
<a href="http://markmims.com/images/event-ingestion-without-streaming-with-filename.svg">
<img src="http://markmims.com/images/event-ingestion-without-streaming-with-filename.svg" width="720px" />
</a></p>

<p>That&#8217;s simple enough and seems pretty straightforward in theory.  In practice
you can ingest events straight into files in hdfs only up to a certain scale
and degree of event complexity.</p>

<p>As scale increases, an ingestion pipeline has to become effectively a dynamic
impedance matching network.  It&#8217;s the funnel that&#8217;s catching events from what
can be a highly distributed, large number of data sources and trying to slam
all these events into a relatively small number of filesystem datanodes.</p>

<p>What can we we do to match those separate source sizes from target sizes?
<a href="http://markmims.com/images/events-without-streaming-question.svg">
<img src="http://markmims.com/images/events-without-streaming-question.svg" width="720px" />
</a>
use Spark!  :-)</p>

<p>No, but seriously, add a streaming solution in-between (I do like Spark
Streaming here)
<a href="http://markmims.com/images/streaming-bare.svg">
<img src="http://markmims.com/images/streaming-bare.svg" width="720px" />
</a>
and use Kafka to decouple all the bits
<a href="http://markmims.com/images/streaming-events-at-scale.svg">
<img src="http://markmims.com/images/streaming-events-at-scale.svg" width="720px" />
</a>
in such a way that your datasources on the left, and your datanodes on the
right can scale independently!  And independently from any stream computation
infrastructure you might need for in-stream decisions in the future.  I go
through that in a little more detail in the talk itself.</p>

<p>Impedance or size mismatches between data sources and data storage are really
only one half of the story.  Note that another culprit, <em>event complexity</em>, can
limit ingest throughput for a number of reasons.  A common example of where
this happens is when event &#8220;types&#8221; are either poorly defined or are changing so
much they&#8217;re hard to identify.  As event complexity increases, so does the
logic you use to group or partition the events so they&#8217;re fast to query.  In
practice this quickly grows from simple logic to full-blown event
classification algorithms.  Often those classification algorithms have to learn
from the body of events that&#8217;ve already landed.  You&#8217;re making decisions on
events in front of you based on all the events you&#8217;ve ever seen.  I&#8217;ll bump any
further discussion of that until we talk more about state in the &#8220;Recognize
Activity&#8221; section later.</p>

<p>Ingest pipelines can get complicated as you try to scale in size and
complexity&#8230; expect it!&#8230; plan for it!  The best way is to do this is to
build or use a toolchain that can let you add a streaming and queueing solution
without a lot of rearchitecture or downtime.  Folks often don&#8217;t try to solve
this problem until it&#8217;s already painful in production!  There&#8217;re great ways
to solve this in general.  My current fav atm uses a hybrid combination of
Terraform, Consul, Ansible, and ClouderaManager/Ambari.</p>

<p>Note also that we haven&#8217;t talked about any real-time processing or low-latency
business requirements here at all.  The need for a stream processing solution
arises when we&#8217;re <em>just</em> trying to catch events at scale.</p>

<hr />

<h2 id="take-action">Take Action</h2>

<p>Catching events within the system is an interesting challenge all by itself.
However, just efficiently and faithfully capturing events isn&#8217;t the end of the
story.</p>

<p><a href="http://markmims.com/images/streaming-bare.svg">
<img src="http://markmims.com/images/streaming-bare.svg" width="720px" />
</a></p>

<p>That&#8217;s sorta boring if we&#8217;re not taking <em>action</em> on events as we catch
them.</p>

<p>Actions such as </p>

<ul>
  <li>Notifications</li>
  <li>Decorations</li>
  <li>Routing / Gating</li>
  <li>Counting</li>
  <li>&#8230;</li>
</ul>

<p>can be taken in either &#8220;batch&#8221; or &#8220;real-time&#8221; modes.</p>

<p><a href="http://markmims.com/images/streaming-simple.svg">
<img src="http://markmims.com/images/streaming-simple.svg" width="720px" />
</a></p>

<p>Unfortunately, folks have all sorts of meanings for these terms.  Let&#8217;s clear
that up and be a little more precise&#8230;</p>

<p>For every action you intend to take, and really every data product of your
pipeline, you need to determine the latency requirements.  What is the
timeliness of that resulting action?  So how soon after either a.) an event was
generated, or b.) an event was seen within the system will that resulting
action be valid?  The answers might surprise you.</p>

<p>Latency requirements let you make a first-pass attempt at specifying the
<em>execution context</em> of each action.  There are two separate execution contexts we
talk about here&#8230; <em>batch</em> and <em>stream</em>.</p>

<ul>
  <li>
    <p>batch.  Asynchronous jobs that are potentially run against the entire body of
events and event histories.  These can be highly complex, computationally
expensive tasks that might involve a large amount of data from various
sources.  The implementations of these jobs can involve Spark or Hadoop
map-reduce code, Cascading-style frameworks, or even sql-based analysis via
Impala, Hive, or SparkSQL.</p>
  </li>
  <li>
    <p>stream.  Jobs that are run against either an individual event or a small
window of events.  These are typically simple, low-computation jobs that
don&#8217;t require context or information from other events.  These are typically
implemented using Spark-streaming or Storm code.</p>
  </li>
</ul>

<p>When I say &#8220;real-time&#8221; in this talk, I mean that the action will be taken from
within the stream execution context.</p>

<p>It&#8217;s important to realize that not all actions require &#8220;real-time&#8221; latency.
There are plenty of actions that are perfectly valid even if they&#8217;re operating
on &#8220;stale&#8221; day-old, hour-old, 15min-old data.  Of course, this sensitivity to
latency varies greatly by action, domain, and industry.  Also, how stale stream
-vs- batch events are depend of the actual performance characteristics of your
ingestion pipeline under load.  Measure all the things!</p>

<p>An approach I particularly like is to initially act from a batch context.
There&#8217;s generally less development effort, more computational resources, more
robustness, more flexibility, and more forgiveness involved when you&#8217;re working
in a batch execution context.  You&#8217;re less likely to interrupt or congest your
ingestion pipeline.</p>

<p>Once you have basic actions working from the batch layer, then do some
profiling and identify which of the actions you&#8217;re working with really require
less stale data.  <em>Selectively</em> bring those actions or analyses forward.  Tools
such as Spark can help tremendously with this.  It&#8217;s not all fully baked yet,
but there are ways to write spark code where the same business logic code can
be optionally bound in either stream or batch execution contexts.  You can move
code around based on pipeline requirements and performance!</p>

<p>In practice, a good deal of architecting such a pipeline is all about
preserving or protecting your stream ingestion and decision-making capabilities
for when you really need them.</p>

<p>A real system often involves additionally protecting and decoupling your stream
processing from making any service API calls (sending emails for example) by
adding kafka queues for things like outbound notifications <em>downstream</em> of
ingestion
<a href="http://markmims.com/images/streaming-with-notify-queues.svg">
<img src="http://markmims.com/images/streaming-with-notify-queues.svg" width="720px" />
</a>
as well as isolating your streaming system from writes to hdfs using 
the same trick (as we saw above)
<a href="http://markmims.com/images/streaming-two-layers.svg">
<img src="http://markmims.com/images/streaming-two-layers.svg" width="720px" />
</a></p>

<hr />

<h2 id="recognize-activity">Recognize Activity</h2>

<p>What&#8217;s user activity?  Usually it&#8217;s a <em>Sequence of one or more events</em>
associated with a user.  From an infrastructure standpoint, the key distinction
is that activity is constructed from a sequence of user events&#8230; <em>that don&#8217;t
all fit within a single window of stream processing</em>.  This can either be
because there are too many of them or because they&#8217;re spread out over too long
a period of time.</p>

<p>Another way to think of this is that event context matters.  In order to
recognize activity as such, you often need to capture or create user context
(let&#8217;s call it &#8220;state&#8221;) in such a way that it&#8217;s easily read by (and possibly
updated from) processing in-stream.</p>

<p>We add hbase to our standard stack, and use it to store state
<a href="http://markmims.com/images/classifying-with-state.svg">
<img src="http://markmims.com/images/classifying-with-state.svg" width="720px" />
</a></p>

<p>which is then accessible from either stream or batch processing.  HBase is
attractive as a fast key-value store.  Several other key-value stores could
work here&#8230; I&#8217;ll often start using one simply because it&#8217;s easier to
deploy/manage at first.  Then refine the choice of tool once more precise
performance requirements of the state store have emerged from use.</p>

<p>It&#8217;s important to note that you want fast key-based reads and writes.
Full-table scans of columns are pretty much verboten in this setup.  They&#8217;re
simply too slow for value from stream.</p>

<p>The usual approach is to update state in batch.  My favorite example when first
talking to folks about this approach is to consider a user&#8217;s credit score.
Events coming into the system are routed in stream based on the associated
user&#8217;s credit score.</p>

<p>The stream system can simply (hopefully quickly) look that up in HBase keyed
on a user id of some sort
<a href="http://markmims.com/images/hbase-state-credit-score.svg">
<img src="http://markmims.com/images/hbase-state-credit-score.svg" width="720px" />
</a>
The credit score is some number calculated by scanning across all a user&#8217;s
events over the years.  It&#8217;s a big, long-running, expensive computation.  Do
that continuously in batch&#8230; just update HBase as you go.  If you do that,
then you make that information available for decisions in stream.</p>

<p>Note that this is effectively a way to base fast-path decisions on
information learned from slow-path computation.  A way for the system to
quite literally <em>learn from the past</em>  :-)</p>

<p>Another example of this is tracking a package.  The events involved are the
various independent scans the package undergoes throughout its journey.</p>

<p>For &#8220;state&#8221; you might just want to keep an abbreviated version of the raw
history of each package
<a href="http://markmims.com/images/hbase-state-tracking-package.svg">
<img src="http://markmims.com/images/hbase-state-tracking-package.svg" width="720px" />
</a>
or just some derived notion of its state
<a href="http://markmims.com/images/hbase-state-tracking-package-derived.svg">
<img src="http://markmims.com/images/hbase-state-tracking-package-derived.svg" width="720px" />
</a>
those derived notions of state are tough to define from a single scan in a
warehouse somewhere&#8230; but make perfect sense when viewed in the context of the
entire package history.</p>

<hr />

<h2 id="wrap-up">Wrap-up</h2>

<p>I eventually come back to our agenda:</p>

<ul>
  <li>Ingest Events</li>
  <li>Take Action</li>
  <li>Recognize Activity</li>
</ul>

<p>Along the way we&#8217;ve done a nod to some data-plumbing best practices&#8230; such as</p>

<h4 id="the-power-of-the-query-side">The Power of the Query Side</h4>
<p>Query-side tools are fast &#8211; use them effectively!</p>

<h4 id="infrastructure-aspirations">Infrastructure Aspirations</h4>
<p>A datascience pipeline is</p>

<ul>
  <li>immutable</li>
  <li>lazy</li>
  <li>atomic
    <ul>
      <li>simple</li>
      <li>composable</li>
      <li>testable</li>
    </ul>
  </li>
</ul>

<p>When building datascience pipelines, these paradigms 
help you stay flexible and scalable</p>

<h4 id="automate-all-of-the-things">Automate All of the Things</h4>
<p>DevOps is your friend.  We&#8217;re using an interesting pushbutton stack that&#8217;ll be
the topic of another blog post :-)</p>

<h4 id="test-all-of-the-things">Test All of the Things</h4>
<p>TDD/BDD is your friend.  Again, I&#8217;ll add another post on &#8220;Sanity-Driven Data
Science&#8221; which is my take on TDD/BDD as applied to datascience pipelines.</p>

<h4 id="failure-is-a-first-class-citizen">Failure is a First Class Citizen</h4>
<p>Fail fast, early, often&#8230; along with the obligatory reference to the Netflix
Simian Army.</p>

<hr />

<h2 id="the-talk-itself">The Talk Itself</h2>

<p>It was a somewhat challenging presentation format.  I presented a live video
feed solo while the audience was watching live and had the ability to send
questions in via chat&#8230; no audio from the audience.  Somewhat reminiscent of
IRC-based presentations we used to do in Ubuntu community events&#8230; but with
video.</p>

<p>The moderator asked the audience to queue questions up until the end, but as
anyone who&#8217;s been in a classroom with me knows, I welcome / live for
interruptions :-) In this case, I could easily see the chat window as I
presented so asking-questions-along-the-way is supported on that presentation
platform.  I&#8217;d definitely ask for that in the future.</p>

<p>I do prefer the fireside chat nature of adding one or two more folks into the
feed&#8230; kinda like on-the-air hangouts&#8230; where the speaker can get audible
feedback from some folks.  Overall though this was a great experience and folks
asked interesting questions at the end.  I&#8217;m not sure how it&#8217;ll be published,
but questions had to be done in a second section as I dropped connectivity
right at the end of the speaking session.</p>

<p>Slides are available
<a href="http://archive.markmims.com/box/talks/2016-03-12-hwtb-sessions/slides.html">here</a>,
and you can get the video straight from the <a href="hadoop.with-the-best.com">hadoop with the
best</a> site.  Note that the slides are
<a href="https://github.com/hakimel/reveal.js/">reveal.js</a> and I make heavy use of
two-dimensional navigation.  Slides advance downwards, topics advance to the
right.</p>

<p>Update: this post has be perdied-up (thanks Meg!) and reposted as part of our
<a href="http://www.svds.com/building-pipelines-understand-user-behavior/">svds blog</a>.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Develop Spark Apps on Yarn using Docker]]></title>
    <link href="http://markmims.com/docker/data-engineering/spark/devops/2015/10/13/docker-cdh.html"/>
    <updated>2015-10-13T15:07:00+00:00</updated>
    <id>http://markmims.com/docker/data-engineering/spark/devops/2015/10/13/docker-cdh</id>
    <content type="html"><![CDATA[<p>At svds, we&#8217;ll often run spark on yarn in production.  Add some artful tuning
and this works pretty well.  However, developers typically build and test spark
application in <em>standalone</em> mode&#8230; not on yarn.</p>

<p>Rather than get bitten by the ideosyncracies involved in running spark on yarn
-vs- standalone when you go to deploy, here&#8217;s a way to set up a development
environment for spark that more closely mimics how it&#8217;s used in the wild.</p>

<!--more-->

<h2 id="a-simple-yarn-cluster-on-your-laptop">A simple yarn &#8220;cluster&#8221; on your laptop</h2>

<p>Run a docker image for a cdh standalone instance</p>

<pre><code>docker run -d --name=mycdh svds/cdh
</code></pre>

<p>when the logs</p>

<pre><code>docker logs -f mycdh
</code></pre>

<p>stop going wild, you can run the usual hadoop-isms to set up a workspace</p>

<pre><code>docker exec -it mycdh hadoop fs -ls /
docker exec -it mycdh hadoop fs -mkdir -p /tmp/blah
</code></pre>

<h2 id="run-spark">Run spark</h2>

<p>Then, it&#8217;s pretty straightforward to run spark against yarn</p>

<pre><code>docker exec -it mycdh \
  spark-submit \
    --master yarn-cluster \
    --class org.apache.spark.examples.SparkPi \
    /usr/lib/spark/examples/lib/spark-examples-1.3.0-cdh5.4.3-hadoop2.6.0-cdh5.4.3.jar \
    1000
</code></pre>

<p>Note that you can <em>submit</em> a spark job to run in either &#8220;yarn-client&#8221; or &#8220;yarn-cluster&#8221; modes.</p>

<p>In &#8220;yarn-client&#8221; mode, the spark driver runs outside of yarn and logs to
console and all spark executors run as yarn containers.</p>

<p>In &#8220;yarn-cluster&#8221; mode, all spark executors run as yarn containers, but then
the spark driver also runs as a yarn container.  Yarn manages all the logs.</p>

<p>You can also run the spark shell so that any workers spawned run in yarn</p>

<pre><code>docker exec -it mycdh spark-shell --master yarn-client
</code></pre>

<p>or</p>

<pre><code>docker exec -it mycdh pyspark --master yarn-client
</code></pre>

<h2 id="your-application">Your Application</h2>

<p>Ok, so <code>SparkPi</code> is all fine and dandy, but how do I run a real application?</p>

<p>Let&#8217;s make up an example.  Say you build your spark project on your laptop in the
<code>/Users/myname/mysparkproject/</code> directory.</p>

<p>When you build with maven or sbt, it typically builds and leaves jars under a
<code>/Users/myname/mysparkproject/target/</code> directory&#8230; for sbt, it&#8217;ll look like
<code>/Users/myname/mysparkproject/target/scala-2.10/</code>.</p>

<p>The idea here is to make these jars directly accessible from both your laptop&#8217;s
build process as well as from inside the cdh container.</p>

<p>When you start up the <code>cdh</code> container, map this local host directory up and
into the container</p>

<pre><code>docker run -d -v ~/mysparkproject/target:/target --name=mycdh svds/cdh 
</code></pre>

<p>where the <code>-v</code> option will make <code>~/mysparkproject/target</code>
available as <code>/target</code> within the container.</p>

<p>So,</p>

<pre><code>sbt clean assembly
</code></pre>

<p>leaves a jar under <code>~/mysparkproject/target</code>, which the container sees as
<code>/target</code> and you can run jobs using something like</p>

<pre><code>docker exec -it mycdh \
  spark-submit \
    --master yarn-cluster \
    --name MyFancySparkJob-name \
    --class org.markmims.MyFancySparkJob \
    /target/scala-2.10/My-assembly-1.0.1.20151013T155727Z.c3c961a51c.jar \
    myarg
</code></pre>

<p>The <code>--name</code> arg makes it easier to find in the midst of multiple yarn jobs.</p>

<h2 id="logs">Logs</h2>

<p>While a spark job is running, you can get its yarn &#8220;applicationId&#8221; from</p>

<pre><code>docker exec -it mycdh yarn application -list
</code></pre>

<p>or if it finished already just list things out with more conditions</p>

<pre><code>docker exec -it mycdh yarn application -list -appStates FINISHED
</code></pre>

<p>You can dig through the yarn-consolidated logs after the job is done
by using</p>

<pre><code>docker exec -it mycdh yarn logs -applicationId &lt;applicationId&gt;
</code></pre>

<h2 id="consoles">Consoles</h2>

<p>Web consoles are critical for application development.  Spend time up front
getting ports open or forwarded correctly for all environments.  Don&#8217;t wait
until you&#8217;re actually trying to debug something critical to figure out how to
forward ports to see the staging UI in all environments.</p>

<h3 id="yarn-resourcemanager-ui">Yarn ResourceManager UI</h3>

<p>Yarn gives you quite a bit of info about the system right from the
ResourceManager on its ip address and webgui port (usually 8088)</p>

<pre><code>open http://&lt;resource-manager-ip&gt;:&lt;resource-manager-port&gt;/
</code></pre>

<h3 id="spark-staging-ui">Spark Staging UI</h3>

<p>Yarn also conveniently proxies access to the spark staging UI for a given
application.  This looks like</p>

<pre><code>open http://&lt;resource-manager-ip&gt;:&lt;resource-manager-port&gt;/proxy/&lt;applicationId&gt;
</code></pre>

<p>for example,</p>

<pre><code>open http://localhost:8088/proxy/application_1444330488724_0005/
</code></pre>

<h3 id="ports-and-docker">Ports and Docker</h3>

<p>There are a few ways to deal with accessing port <code>8088</code> of the yarn resource
manager from outside of the docker container.  I typically use ssh for
everything and just forward ports out to <code>localhost</code> on the host.  However,
most people will expect to access ports directly on the <code>docker-machine ip</code>
address.  To do that, you have to map each port when you first spin up the
<code>cdh</code> container using the <code>-p 8088</code> option</p>

<pre><code>docker run -d -v target -p 8088 --name=mycdh svds/cdh 
</code></pre>

<p>Then you should be good to go with something like</p>

<pre><code>open http://`docker-machine ip`:8088/
</code></pre>

<p>to access the yarn console.</p>

<hr />

<h2 id="tips-and-gotchas">Tips and Gotchas</h2>

<ul>
  <li>
    <p>The docker image <code>svds/cdh</code> is quite large (2GB).  I like to do a separate
<code>docker pull</code> from any <code>docker run</code> commands just to isolate the download.
In fact, I recommend pinning the cdh version for the same reason&#8230; so
<code>docker pull svds/cdh:5.4.0</code> for instance, then refer to it that way
throughout <code>docker run -d --name=mycdh svds/cdh:5.4.0</code> and that&#8217;ll insure
you&#8217;re not littering your laptop&#8217;s filesystem with docker layers from
multiple cdh versions.  The bare <code>svds/cdh</code> (equiv to <code>svds/cdh:latest</code>)
floats with the most recent cloudera versions</p>
  </li>
  <li>
    <p>I&#8217;m using a CDH container here&#8230; but there&#8217;s an HDP one on the way as well.
Keep an eye out for it on <a href="`https://hub.docker.com/u/svds`">svds&#8217;s dockerhub page</a></p>
  </li>
  <li>
    <p>web consoles and forwarding ports through SSH</p>
  </li>
</ul>

<h2 id="bonus">Bonus</h2>

<p>Ok, so the downside here is that the image is fat.  The upside is that it lets
you play with the full suite of CDH-based tools.  I&#8217;ve tested out (besides the
spark variations above)</p>

<h3 id="impala-shell">Impala shell</h3>

<pre><code>docker exec mycdh impala-shell
</code></pre>

<h3 id="hbase-shell">HBase shell</h3>

<pre><code>docker exec mycdh hbase shell
</code></pre>

<h3 id="hive">Hive</h3>

<pre><code>echo "show tables;" | docker exec mycdh beeline -u jdbc:hive2://localhost:10000 -n username -p password -d org.apache.hive.jdbc.HiveDriver
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SSH Tips and Tricks]]></title>
    <link href="http://markmims.com/2015/06/01/ssh-tips.html"/>
    <updated>2015-06-01T16:00:00+00:00</updated>
    <id>http://markmims.com/2015/06/01/ssh-tips</id>
    <content type="html"><![CDATA[<p>Notes from a lunch-and-learn talk.  It&#8217;s a little weak without the narrative,
but I&#8217;ll post it here for reference anyway.</p>

<p>Agenda:</p>

<ul>
  <li>config</li>
  <li>tunnels
    <ul>
      <li>forward</li>
      <li>reverse</li>
    </ul>
  </li>
  <li>proxies</li>
  <li>ssh + tmux = &lt;3
    <ul>
      <li>ssh, then tmux</li>
      <li>tmux, then ssh</li>
    </ul>
  </li>
</ul>

<!-- more -->

<h2 id="why-ssh">Why SSH?</h2>

<p>It&#8217;s common practice to secure a cluster of servers using a <code>bastion</code> host.
This might be a cluster of servers in a colocation facility, containers on a
single host, or instances in an <code>EC2</code> region&#8230; the pattern can still be applied.</p>

<p>The way this works is that the servers in the cluster are all locked down and
not accessible to the outside world except where necessary for the production
network design of the pipeline or application.</p>

<p>That&#8217;s all great for production network traffic.  However, there&#8217;s often a need
for adhoc access: testing, debugging, monitoring, etc of the cluster.  This is
usually access to information that&#8217;s required <em>in addition</em> to the existing
monitoring and logging for the production pipeline.  Until automated management
solutions involving immutable infrastructure components are widely adopted,
you&#8217;ll almost always need the ability for an engineer to directly log into
cluster instances to do things like clear <code>/tmp</code> directories, run jobs, etc.  </p>

<p>You&#8217;ve also gotta routinely access various web consoles (ClouderaManager,
spark, hdfs, etc) to debug functional or performance problems, to change
config, or even just to do sanity checks on overall cluster health.</p>

<p>How do you access all of this?  You can&#8217;t just expose them to the outside
world.  None of these consoles were ever designed for that.  They&#8217;re rife with
holes&#8230;  with often <em>huge</em> ramifications for any incursions!  On the other
hand, it&#8217;s often quite difficult (and dangerous!) to add adhoc network access
into production network planning.</p>

<p>Two practices are common:</p>

<ul>
  <li>VPN access</li>
  <li>SSH proxies and tunnels</li>
</ul>

<p>They each have pros/cons, tradeoffs between security, ease-of-use, flexibility,
and capability.  VPN access is often ineffective due to its static nature and
sensitivity to all manner of bad security practices.  It&#8217;s particularly
pointless due to the random way different web consoles choose which interfaces
they like to bind to.  That&#8217;s a whole other discussion&#8230; for this talk,
suffice it to say that I highly recommend and infinitely prefer an SSH-based
solution.  It&#8217;s worth traversing the learning curve of SSH for the sheer power
and flexiblity it gives you without compromising security.</p>

<hr />

<h2 id="config-files">Config Files</h2>

<p>In your home directory, there&#8217;s an optional <code>~/.ssh/config</code> file 
where you can customize your local SSH client behavior.</p>

<p>You can use this for simple aliases&#8230;</p>

<pre><code>#################
# MyBastions
#################
Host customerXbastion
    Hostname ec2-xxx-xxx-xxx-xxx.compute-1.amazonaws.com
Host customerYbastion
    Hostname ec2-yyy-yyy-yyy-yyy.compute-1.amazonaws.com
Host customerZbastion
    Hostname ec2-zzz-zzz-zzz-zzz.compute-1.amazonaws.com 
</code></pre>

<p>or adding extra stuff that&#8217;s a pain to type <em>every</em> time</p>

<pre><code>############
# CustomerX
############
Host dev-control-*.customerX.com
    User ubuntu
    IdentityFile ~/projects/customerX/creds/dev_control.pem
Host dev-es-*.customerX.com
    User ubuntu
    IdentityFile ~/projects/customerX/creds/dev_es.pem
Host dev-hdp-*.customerX.com
    User ubuntu
    IdentityFile ~/projects/customerX/creds/dev_hdp.pem
(etc)
</code></pre>

<p>notice the pattern entries?</p>

<p>You can include tunnels (discussed below)</p>

<pre><code>Host myserver
    Hostname 10.2.3.4
    LocalForward 7080 localhost:7080
    LocalForward 8080 localhost:8080
</code></pre>

<p>or proxies (also discussed below)</p>

<pre><code>#############
# CustomerY
#############
Host customerYbastion
    Hostname ec2-yyy-yyy-yyy-yyy.compute-1.amazonaws.com
    User ubuntu
    ProxyCommand none
Host *.inside.customerY.com
    User ubuntu
    ProxyCommand ssh customerYbastion nc -q0 %h %p
</code></pre>

<p>Once you add multiple cluster configs and different customer environments,
these SSH config files can get quite complex.  Here&#8217;re a couple of ways I&#8217;ve
seen people manage that:</p>

<ul>
  <li>
    <p>just manage one big <code>~/.ssh/config</code> file by hand and use <code>Host</code> names and
comments to keep track of everything</p>
  </li>
  <li>
    <p>explictly specify config files at the command line a la <code>ssh -F
~/.ssh/customerX-config &lt;server&gt;</code>&#8230; maybe even use a shell alias to shorten
this if you do it a lot</p>
  </li>
  <li>
    <p>[what I currently do] scripts to glue multiple config snippets from
<code>~/.ssh/config.d/customerX.conf</code> into a single big read-only <code>~/.ssh/config</code>.
It&#8217;d be nice to eventually change the ssh client to optionally read from
these kind of <code>~/.ssh/config.d/</code> and <code>~/.ssh/authorized_keys.d/</code> snippet
directories</p>
  </li>
  <li>
    <p>customer-specific containers&#8230; I actually work a lot from <em>inside</em> of
containers on an ec2 instance.  I usually have them just bind-mount the
underlying hosts home directory, but you could easily keep them isolated with
separate config and spin them up only when you need overlay specific to a
customer.  This also works even with gui apps on a laptop btw, but that&#8217;s a
longer story :)</p>
  </li>
</ul>

<p>It&#8217;s also pretty common for folks to write scripts using config management
(juju, knife, or ClouderaManager-like APIs) to generate ssh config snippets
from a running infrastructure.  This can be quite useful, but is still a static
picture of a cluster that changes.  Depending on the lifetime or stability of
the cluster, you&#8217;re often better off using a more dynamic approach like <code>knife
ssh</code>.  It&#8217;s a no-win tradeoff of sharing static SSH config snippets -vs-
configuring chef environments for everyone who needs to access the cluster.</p>

<p>I&#8217;d love to hear other solutions folks have come up with to deal with this.  I
have no clue what puppet offers here, and I bet there are great examples of
ansible&#8217;s ec2 plugin that&#8217;ll be a dead-simple way to interact with a dynamic
host inventory.  Perhaps that&#8217;s where I&#8217;ll head next&#8230; we&#8217;ll see.  Totally
depends on customer environments.</p>

<hr />

<h2 id="proxies">Proxies</h2>

<p>One server, a <code>bastion</code> host, accepts SSH traffic from the outside world.
Remaining <code>target</code> hosts in the cluster are configured internal access only.</p>

<p>Consider the following scenario using a <code>ProxyCommand</code>.</p>

<p>Take an externally accessible <code>bastion</code> and an internally accessible <code>target</code>.
Set up your SSH config so you can ssh directly to the <code>bastion</code> host</p>

<pre><code>     +--------------------+         +-------------------+
     |                    |         |                   |
     |                    |         |                   |
     |                    |         |                   |
     |                    |         |                   |
     |                    |         |                   |
     |    laptop          |         |      bastion      |
     |                    |  ssh    |                   |
     |                    +---------&gt;                   +
     |                    |         |                   |
     |                    |         |                   |
     |                    |         |                   |
     |                    |         |                   |
     |                    |         |                   |
     |                    |         |                   |
     |                    |         |                   |
     +--------------------+         +-------------------+
</code></pre>

<p>with a command like </p>

<pre><code>     `ssh bastion`
</code></pre>

<p>Then you can ssh from there to a <code>target</code> host</p>

<pre><code>                                    +-------------------+          +-------------------+
                                    |                   |          |                   |
                                    |                   |          |                   |
                                    |                   |          |                   |
                                    |                   |          |                   |
                                    |                   |          |                   |
                                    |      bastion      |          |     target        |
                                    |                   |  ssh     |                   |
                                    |                   +----------&gt;                   |
                                    |                   |          |                   |
                                    |                   |          |                   |
                                    |                   |          |                   |
                                    |                   |          |                   |
                                    |                   |          |                   |
                                    |                   |          |                   |
                                    |                   |          |                   |
                                    +-------------------+          +-------------------+


                                    `ssh target`
</code></pre>

<p>The key bit here is that we can compress this to one step for the user.</p>

<pre><code>     +--------------------+         +-------------------+       +-------------------+
     |                    |         |                   |       |                   |
     |                    |         |                   |       |                   |
     |                    |         |                   |       |                   |
     |                    |         |                   |       |                   |
     |                    |         |                   |       |                   |
     |    laptop          |         |      bastion      |       |      target       |
     |                    |  ssh    |                   |  ssh  |                   |
     |                    +---------&gt;                   +-------&gt;                   |
     |                    |         |                   |       |                   |
     |                    |         |                   |       |                   |
     |                    |         |                   |       |                   |
     |                    |         |                   |       |                   |
     |                    |         |                   |       |                   |
     |                    |         |                   |       |                   |
     |                    |         |                   |       |                   |
     +--------------------+         +-------------------+       +-------------------+
</code></pre>

<p>From laptop&#8217;s <code>~/.ssh/config</code> file:</p>

<pre><code>Host bastion
    Hostname ec2-xxx.xxx.....amazon.com
Host target
    Hostname ip-10-xx-xx-xx.internal....amazon.com
    ProxyCommand ssh bastion nc -q1 %h %p
</code></pre>

<p>then you can just <code>ssh target</code> directly from your laptop.  It automatically
traverses the proxy <code>bastion</code> on your behalf.</p>

<p>Note, that from an administrative perspective, it&#8217;s easy to control user access
at the single <code>bastion</code>&#8230; if you can&#8217;t establish an ssh connection to the
bastion, you can&#8217;t &#8220;jump through it&#8221; to internal hosts.</p>

<hr />

<h2 id="tunnels">Tunnels</h2>

<p>SSH in general <em>is</em> a tunnel</p>

<pre><code>        +-----------------------+                             +------------------------+
        |                       |                             |                        |
        |                       |                             |                        |
        |                       |            Inet             |                        |
        |                       |                             |                        |
        |                       +-----------------------------&gt;                        |
        |                       +         &lt;-- text --&gt;        |           fred         |
        |                       +                             |                        |
        |        laptop         +-----------------------------&gt;      (ec2 instance)    |
        |                       |                             |                        |
        |                       |                             |    (any remote server) |
        |                       |                             |                        |
        |                       |                             |                        |
        |                       |                             |                        |
        |                       |                             |                        |
        |                       |                             |                        |
        |                       |                             |                        |
        |                       |                             +------------------------+
        +-----------------------+



`ssh fred`
</code></pre>

<h3 id="forward-tunnels">forward tunnels</h3>

<p>aka, &#8220;port forwarding&#8221;</p>

<h4 id="forwarding-web-traffic">forwarding web traffic</h4>

<pre><code>          +----------------+                  +------------------------+
          |                |                  |                        |
          |                |                  |                        |
          |                |                  |                        |
-- 8888 -&gt;|                |                  |                        |
          |                +------------------&gt;                        |
          |                +   &lt;-- text --&gt;   |                        |
          |                +   &lt;--  web --&gt;   |                        | -----&gt;  http://nfl.com/
          |     laptop     +------------------&gt;      ec2 instance      |
          |                |                  |                        |
          |                |                  |    (any remote server) |
          |                |                  |                        |
          |                |                  |                        |
          |                |                  |                        |
          |                |                  |                        |
          |                |                  |                        |
          |                |                  |                        |
          |                |                  +------------------------+
          +----------------+


`ssh fred -L8888:www.nfl.com:80`

`open http://localhost:8888/`
</code></pre>

<h4 id="forwarding-localhost">forwarding localhost</h4>

<pre><code>          +------------+                +------------------------+
          |            |                |                        |
          |            |                |                        |
-- 50070-&gt;|            |                |                        |
          |            |                |                        | http://...  &lt;---+
          |            |                |                        |    (50070)      |
          |            +----------------&gt;                        |                 |
          |            +    &lt;-----&gt;     |                        |                 |
          |            +    &lt;-----&gt;     |                        | ----------------+
          |  laptop    +----------------&gt;      ec2 instance      |
          |            |                |                        |
          |            |                |    (any remote server) |
          |            |                |                        |
          |            |                |                        |
          |            |                |                        |
          |            |                |                        |
          |            |                |                        |
          |            |                |                        |
          |            |                +------------------------+
          +------------+


`ssh fred -L8888:localhost:80`
</code></pre>

<p>or, perhaps more useful&#8230;</p>

<pre><code>`ssh fred -L50070:localhost:50070`
`open http://localhost:50070/`
</code></pre>

<p>or </p>

<pre><code>`ssh fred -L50070:localhost:50070 -L50030:localhost:50030`
</code></pre>

<h3 id="reverse-tunnels">reverse tunnels</h3>

<pre><code>        +-----------------------+                             +------------------------+
        |                       |                             |                        |
        |                       |                   -- 2222 -&gt;|                        |
        |                       |                             |                        |
        |                       |                             |                        |
        |                       |                             |                        |
        |                       +-----------------------------&gt;                        |
        |                       +            &lt;-----&gt;          |                        |
        |                       +            &lt;-----&gt;          |                        |
        |        laptop         +-----------------------------&gt;      ec2 instance      |
        |                       |                             |                        |
        |                       |                             |    (any remote server) |
        |                       |                             |                        |
        |                       | 22 &lt;---+                    |                        |
        |                       |        |                    |                        |
        |                       |        |                    |                        |
        |                       |--------+                    |                        |
        |                       |                             |                        |
        |                       |                             +------------------------+
        +-----------------------+



`ssh fred -R2222:localhost:22`
</code></pre>

<p>or maybe something like&#8230;</p>

<pre><code>`ssh fred -R8888:localhost:80`
</code></pre>

<p>or even <code>ssh root@fred -R80:localhost:80</code></p>

<h3 id="add-tunnels-to-your-ssh-config">add tunnels to your ssh config</h3>

<pre><code>Host myhost
    Hostname 10.1.2.3
    LocalForward 7080 localhost:7080
    LocalForward 8080 localhost:8080
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CharmSchool Hangout - Juju Local Provider]]></title>
    <link href="http://markmims.com/cloud/2013/08/23/charmschool-local-provider.html"/>
    <updated>2013-08-23T13:02:00+00:00</updated>
    <id>http://markmims.com/cloud/2013/08/23/charmschool-local-provider</id>
    <content type="html"><![CDATA[<p>Continuing the series of regular CharmSchool Hangouts.  This week&#8217;s video
walks through using the new version of the juju local provider.  It&#8217;s cool!</p>

<div class="ratio-4-3 embed-video-container" onclick="var myAnchor = document.getElementById('3AiQhHIBQJk');var tmpDiv = document.createElement('div');tmpDiv.innerHTML = '&lt;iframe style=&quot;vertical-align:top;width:100%;height:100%;position:absolute;&quot; src=&quot;http://www.youtube.com/embed/3AiQhHIBQJk?autoplay=1&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;';myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;" title="click here to play">
<a class="youtube-lazy-link" style="width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/3AiQhHIBQJk/0.jpg) center center no-repeat;background-size:contain;position:absolute" href="http://www.youtube.com/watch?v=3AiQhHIBQJk" id="3AiQhHIBQJk" onclick="return false;">
<div class="youtube-lazy-link-div"></div>
<div class="youtube-lazy-link-info">embedded youtube video 3AiQhHIBQJk</div>
</a>
<div class="video-info">embedded youtube video 3AiQhHIBQJk</div>
</div>

<!--more-->

<p>As before, there are links to the whole series of charmschool hangouts in the juju
<a href="https://juju.ubuntu.com/resources/videos/">video archive</a>
where we also have videos and screencasts of demos, talks,  and any other charm
schools we&#8217;ve been able to capture on video.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CharmSchool Hangout - Juju Events in Depth]]></title>
    <link href="http://markmims.com/cloud/2013/07/12/charmschool-events-in-depth.html"/>
    <updated>2013-07-12T13:02:00+00:00</updated>
    <id>http://markmims.com/cloud/2013/07/12/charmschool-events-in-depth</id>
    <content type="html"><![CDATA[<p>Continuing the series of regular CharmSchool Hangouts.  This week&#8217;s video
is a little more detail on juju events&#8230;</p>

<div class="ratio-4-3 embed-video-container" onclick="var myAnchor = document.getElementById('vPBrpMcXHN0');var tmpDiv = document.createElement('div');tmpDiv.innerHTML = '&lt;iframe style=&quot;vertical-align:top;width:100%;height:100%;position:absolute;&quot; src=&quot;http://www.youtube.com/embed/vPBrpMcXHN0?autoplay=1&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;';myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;" title="click here to play">
<a class="youtube-lazy-link" style="width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/vPBrpMcXHN0/0.jpg) center center no-repeat;background-size:contain;position:absolute" href="http://www.youtube.com/watch?v=vPBrpMcXHN0" id="vPBrpMcXHN0" onclick="return false;">
<div class="youtube-lazy-link-div"></div>
<div class="youtube-lazy-link-info">embedded youtube video vPBrpMcXHN0</div>
</a>
<div class="video-info">embedded youtube video vPBrpMcXHN0</div>
</div>

<!--more-->

<p>As before, there are links to the whole series of charmschool hangouts in the juju
<a href="https://juju.ubuntu.com/resources/videos/">video archive</a>
where we also have videos and screencasts of demos, talks,  and any other charm
schools we&#8217;ve been able to capture on video.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CharmSchool Hangout - Charming Best Practices]]></title>
    <link href="http://markmims.com/cloud/2013/06/28/charmschool-charm-best-practices.html"/>
    <updated>2013-06-28T13:02:00+00:00</updated>
    <id>http://markmims.com/cloud/2013/06/28/charmschool-charm-best-practices</id>
    <content type="html"><![CDATA[<p>Continuing the series of regular CharmSchool Hangouts.  In this week&#8217;s charmschool
we just sort of sat around and discussed best practices&#8230;</p>

<div class="ratio-4-3 embed-video-container" onclick="var myAnchor = document.getElementById('08dOs3eO04M');var tmpDiv = document.createElement('div');tmpDiv.innerHTML = '&lt;iframe style=&quot;vertical-align:top;width:100%;height:100%;position:absolute;&quot; src=&quot;http://www.youtube.com/embed/08dOs3eO04M?autoplay=1&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;';myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;" title="click here to play">
<a class="youtube-lazy-link" style="width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/08dOs3eO04M/0.jpg) center center no-repeat;background-size:contain;position:absolute" href="http://www.youtube.com/watch?v=08dOs3eO04M" id="08dOs3eO04M" onclick="return false;">
<div class="youtube-lazy-link-div"></div>
<div class="youtube-lazy-link-info">embedded youtube video 08dOs3eO04M</div>
</a>
<div class="video-info">embedded youtube video 08dOs3eO04M</div>
</div>

<!--more-->

<p>As before, there are links to the whole series of charmschool hangouts in the juju
<a href="https://juju.ubuntu.com/resources/videos/">video archive</a>
where we also have videos and screencasts of demos, talks,  and any other charm
schools we&#8217;ve been able to capture on video.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Juju Demo Videos]]></title>
    <link href="http://markmims.com/cloud/2013/06/15/juju-demo-videos.html"/>
    <updated>2013-06-15T13:02:00+00:00</updated>
    <id>http://markmims.com/cloud/2013/06/15/juju-demo-videos</id>
    <content type="html"><![CDATA[<p>I Put together a series of demo videos using juju-0.7 for oscon.</p>

<p>These are really interesting in that they involve migrating environments
between providers.  This works slightly differently on the newer juju-1.x
series, but the idea&#8217;s still sound.</p>

<p>There&#8217;s no sound on these&#8230; they&#8217;re raw video backups for demoing juju (in
case we lost networking during the demo).</p>

<p>migrate local to hp</p>

<div class="ratio-4-3 embed-video-container" onclick="var myAnchor = document.getElementById('Jfnxl1Kh9SY');var tmpDiv = document.createElement('div');tmpDiv.innerHTML = '&lt;iframe style=&quot;vertical-align:top;width:100%;height:100%;position:absolute;&quot; src=&quot;http://www.youtube.com/embed/Jfnxl1Kh9SY?autoplay=1&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;';myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;" title="click here to play">
<a class="youtube-lazy-link" style="width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/Jfnxl1Kh9SY/0.jpg) center center no-repeat;background-size:contain;position:absolute" href="http://www.youtube.com/watch?v=Jfnxl1Kh9SY" id="Jfnxl1Kh9SY" onclick="return false;">
<div class="youtube-lazy-link-div"></div>
<div class="youtube-lazy-link-info">embedded youtube video Jfnxl1Kh9SY</div>
</a>
<div class="video-info">embedded youtube video Jfnxl1Kh9SY</div>
</div>

<!--more-->

<p>migrate ec2 to hpcloud</p>

<div class="ratio-4-3 embed-video-container" onclick="var myAnchor = document.getElementById('HUtR3_YlKXU');var tmpDiv = document.createElement('div');tmpDiv.innerHTML = '&lt;iframe style=&quot;vertical-align:top;width:100%;height:100%;position:absolute;&quot; src=&quot;http://www.youtube.com/embed/HUtR3_YlKXU?autoplay=1&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;';myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;" title="click here to play">
<a class="youtube-lazy-link" style="width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/HUtR3_YlKXU/0.jpg) center center no-repeat;background-size:contain;position:absolute" href="http://www.youtube.com/watch?v=HUtR3_YlKXU" id="HUtR3_YlKXU" onclick="return false;">
<div class="youtube-lazy-link-div"></div>
<div class="youtube-lazy-link-info">embedded youtube video HUtR3_YlKXU</div>
</a>
<div class="video-info">embedded youtube video HUtR3_YlKXU</div>
</div>

<p>Local provider</p>

<div class="ratio-4-3 embed-video-container" onclick="var myAnchor = document.getElementById('EpIP4ly4E0E');var tmpDiv = document.createElement('div');tmpDiv.innerHTML = '&lt;iframe style=&quot;vertical-align:top;width:100%;height:100%;position:absolute;&quot; src=&quot;http://www.youtube.com/embed/EpIP4ly4E0E?autoplay=1&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;';myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;" title="click here to play">
<a class="youtube-lazy-link" style="width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/EpIP4ly4E0E/0.jpg) center center no-repeat;background-size:contain;position:absolute" href="http://www.youtube.com/watch?v=EpIP4ly4E0E" id="EpIP4ly4E0E" onclick="return false;">
<div class="youtube-lazy-link-div"></div>
<div class="youtube-lazy-link-info">embedded youtube video EpIP4ly4E0E</div>
</a>
<div class="video-info">embedded youtube video EpIP4ly4E0E</div>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CharmSchool Hangout - Charming from scratch]]></title>
    <link href="http://markmims.com/cloud/2013/06/05/charmschool-hangout-charming-from-scratch.html"/>
    <updated>2013-06-05T13:02:00+00:00</updated>
    <id>http://markmims.com/cloud/2013/06/05/charmschool-hangout-charming-from-scratch</id>
    <content type="html"><![CDATA[<p>Continuing the series of regular CharmSchool Hangouts.  In last week&#8217;s video
we wrote a charm from scratch&#8230;</p>

<div class="ratio-4-3 embed-video-container" onclick="var myAnchor = document.getElementById('NQmxuzdc4Zg');var tmpDiv = document.createElement('div');tmpDiv.innerHTML = '&lt;iframe style=&quot;vertical-align:top;width:100%;height:100%;position:absolute;&quot; src=&quot;http://www.youtube.com/embed/NQmxuzdc4Zg?autoplay=1&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;';myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;" title="click here to play">
<a class="youtube-lazy-link" style="width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/NQmxuzdc4Zg/0.jpg) center center no-repeat;background-size:contain;position:absolute" href="http://www.youtube.com/watch?v=NQmxuzdc4Zg" id="NQmxuzdc4Zg" onclick="return false;">
<div class="youtube-lazy-link-div"></div>
<div class="youtube-lazy-link-info">embedded youtube video NQmxuzdc4Zg</div>
</a>
<div class="video-info">embedded youtube video NQmxuzdc4Zg</div>
</div>

<p>Starting from a simple node.js application, we put together &#8220;just enough&#8221; charm
to get things working.  Watch for future episodes where we&#8217;ll refactor and
refine both the application and the charm.</p>

<!--more-->

<p>As before, there are links to the whole series of charmschool hangouts in the juju
<a href="https://juju.ubuntu.com/resources/videos/">video archive</a>
where we also have videos and screencasts of demos, talks,  and any other charm
schools we&#8217;ve been able to capture on video.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CharmSchool Hangouts]]></title>
    <link href="http://markmims.com/cloud/2013/05/20/charmschool-hangouts.html"/>
    <updated>2013-05-20T12:49:00+00:00</updated>
    <id>http://markmims.com/cloud/2013/05/20/charmschool-hangouts</id>
    <content type="html"><![CDATA[<p>We&#8217;re doing a series of regular CharmSchools on G+ hangouts.
Last week we did an intro to juju and charming&#8230;</p>

<div class="ratio-4-3 embed-video-container" onclick="var myAnchor = document.getElementById('yRcqSjOGweo');var tmpDiv = document.createElement('div');tmpDiv.innerHTML = '&lt;iframe style=&quot;vertical-align:top;width:100%;height:100%;position:absolute;&quot; src=&quot;http://www.youtube.com/embed/yRcqSjOGweo?autoplay=1&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;';myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;" title="click here to play">
<a class="youtube-lazy-link" style="width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/yRcqSjOGweo/0.jpg) center center no-repeat;background-size:contain;position:absolute" href="http://www.youtube.com/watch?v=yRcqSjOGweo" id="yRcqSjOGweo" onclick="return false;">
<div class="youtube-lazy-link-div"></div>
<div class="youtube-lazy-link-info">embedded youtube video yRcqSjOGweo</div>
</a>
<div class="video-info">embedded youtube video yRcqSjOGweo</div>
</div>

<p>There&#8217;re links to the whole series of charmschool hangouts in the juju
<a href="https://juju.ubuntu.com/resources/videos/">video archive</a>
where we also have videos and screencasts of demos, talks,  and any other charm
schools we&#8217;ve been able to capture on video.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CharmSchool Video from the Openstack Summit]]></title>
    <link href="http://markmims.com/cloud/2013/04/25/charmschool-video-from-the-openstack-summit.html"/>
    <updated>2013-04-25T12:40:00+00:00</updated>
    <id>http://markmims.com/cloud/2013/04/25/charmschool-video-from-the-openstack-summit</id>
    <content type="html"><![CDATA[<p>Jorge and I gave a charmschool at the ODS summit.
The room was packed with 300+&#8230; and they pretty much stayed the whole time&#8230; whoohoo!</p>

<p>Watch it 
<a href="http://www.openstack.org/summit/portland-2013/session-videos/presentation/juju-with-openstack-workshop">here</a>
or </p>

<div class="ratio-4-3 embed-video-container" onclick="var myAnchor = document.getElementById('YenD4oxfEa4');var tmpDiv = document.createElement('div');tmpDiv.innerHTML = '&lt;iframe style=&quot;vertical-align:top;width:100%;height:100%;position:absolute;&quot; src=&quot;http://www.youtube.com/embed/YenD4oxfEa4?autoplay=1&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;';myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;" title="click here to play">
<a class="youtube-lazy-link" style="width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/YenD4oxfEa4/0.jpg) center center no-repeat;background-size:contain;position:absolute" href="http://www.youtube.com/watch?v=YenD4oxfEa4" id="YenD4oxfEa4" onclick="return false;">
<div class="youtube-lazy-link-div"></div>
<div class="youtube-lazy-link-info">embedded youtube video YenD4oxfEa4</div>
</a>
<div class="video-info">embedded youtube video YenD4oxfEa4</div>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running the LinuxPlumbers Conference Schedule with Juju]]></title>
    <link href="http://markmims.com/cloud/2012/09/25/linuxplumbers-juju.html"/>
    <updated>2012-09-25T00:00:00+00:00</updated>
    <id>http://markmims.com/cloud/2012/09/25/linuxplumbers-juju</id>
    <content type="html"><![CDATA[<p class="meta">
Written by Mark Mims and Chris Johnston
</p>

<p>Hey, so last month we ran scheduling for the 
<a href="http://linuxplumbersconf.org">Linux Plumbers Conference</a>
entirely on juju!</p>

<p>Here&#8217;s a little background on the experience.</p>

<p>Along the way, we&#8217;ll go into a little more detail about running juju in
production than the particular problem at hand might warrant.  It&#8217;s a basic
stack of services that&#8217;s only alive for 6-months or so&#8230;  but this discussion
applies to bigger longer-running production infrastructures too, so it&#8217;s worth
going over here.</p>

<!--more-->

<h2 id="the-app">The App</h2>

<p>So <a href="https://launchpad.net/summit">summit</a> is this great django app built for
scheduling conferences.  It&#8217;s evolved over time to handle
<a href="uds.ubuntu.com">UDS</a>-level traffic and is currently maintained by a
<a href="https://launchpad.net/~summit-hackers">Summit Hackers</a> team that includes
<a href="http://www.chrisjohnston.org/">Chris Johnston</a>
and <a href="http://mhall119.com/">Michael Hall</a>.</p>

<p>Chris contacted me to help him use juju to manage summit for this year&#8217;s
Plumbers conference.  At the time we started this, the 11.10 version of juju
wasn&#8217;t exactly blessed for production environments, but we decided it&#8217;d be a
great opportunity to work things out.</p>

<h2 id="the-stack">The Stack</h2>

<p>A typical summit stack&#8217;s got postgresql, the django app itself, and a memcached server.</p>

<p><a href="http://markmims.com/images/django-stack.png">
<img src="http://markmims.com/images/django-stack.png" width="198px" />
</a></p>

<p>We additionally talked about putting this all behind some sort of a head like haproxy.</p>

<p><a href="http://markmims.com/images/bigger-django-stack.png">
<img src="http://markmims.com/images/bigger-django-stack.png" width="334px" />
</a></p>

<p>This&#8217;d let the app scale horizontally as well as give us a stable point to
attach an elastic-ip.  We decided to <em>not</em> do this at the time b/c we could
most likely handle the peak conference load with a single django service-unit
provided we slam select snippets of the site into memcached.</p>

<p>This turned out to be true load-wise, but it really would&#8217;ve been a whole lot
easier to have a nice constant haproxy node out there to tack the elastic-ip
to.
During development (charm, app, and theme) you want the freedom to destroy
a service and respawn it without having to use external tools to go around and
attach public IP addresses to the right places.  That&#8217;s a pain.  Also, if
there&#8217;s a sensitive part of this infrastructure in production, it wouldn&#8217;t be
postgresql, memcached, or haproxy&#8230; the app itself would be the most likely
point of instability, so it was a mistake to attach the elastic-ip there.</p>

<h2 id="the-environment">The Environment</h2>

<h3 id="choice-of-cloud">choice of cloud</h3>

<p>We chose to use ec2 to host the summit stack&#8230; mostly a matter of
convenience.  The juju openstack-native provider wasn&#8217;t completed when we spun
up the production environment for linuxplumbers and we didn&#8217;t have access to a
stable private ubuntu cloud running the openstack-ec2-api at the time.
All of this has subsequently landed, so we&#8217;d have more options today.</p>

<h3 id="the-charms">the charms</h3>

<p>We forked <a href="https://twitter.com/michaelanelson">Michael Nelson</a>&#8217;s excellent
<a href="lp:~michael.nelson/charms/oneiric/apache-django-wsgi/trunk">django charm</a>
to create a
<a href="https://code.launchpad.net/~mark-mims/charms/oneiric/summit/trunk">summit-charm</a>
and freely specialized it for summit.</p>

<p>Note that we&#8217;re updating this charm for 12.04
<a href="https://code.launchpad.net/~mark-mims/charms/precise/summit/trunk">here</a>, but
this will probably go away in the near future and we&#8217;ll just use a generic
django charm.  It turns out we didn&#8217;t do too much here that won&#8217;t apply to
django apps in general, but more on that another time.</p>

<p>There was nothing special about our tuning of postgresql or memcached.  We just
used the services provided by the canned charms.  These sort of peripheral
services aren&#8217;t the kind of charms you&#8217;re likely to be making changes to or
tweaking outside of their exposed config parameters.  I know <em>jack</em> about
memcached, so I&#8217;ll defer to the experts in this regard.  Similarly for
postgresql&#8230; and haproxy if we used it in this stack.</p>

<p>The summit charm is a little different.  It&#8217;s something we were continuing to
tweak during development.  Perhaps with future more generic django charm
versions, we won&#8217;t need to tweak the charm itself&#8230; just configure it.</p>

<p>We used a &#8220;local&#8221; repository for <em>all</em> charms because the charm store hadn&#8217;t
landed when we were setting this up.  Well, now that the charm store is live,
you can just deploy the canned charms straight from the store</p>

<pre><code>`juju deploy -e summit memcached`
</code></pre>

<p>and keep the ones you want to tweak in a local repository&#8230; </p>

<pre><code>`juju deploy -e summit --repository ~/charms local:summit`
</code></pre>

<p>all within the same environment.  It works out nicely.</p>

<h3 id="control-environment">control environment</h3>

<p>We had multiple people to manage the production summit environment.  What&#8217;s the
best way to do that?  It turns out juju supports this pretty well right out of
the box.  There&#8217;s an environment config for the set of ssh public keys to
inject into everything in the environment as it starts up&#8230; you can read more
about that on 
<a href="http://askubuntu.com/questions/179230/how-can-i-manage-multiple-administrators-with-juju">askubuntu</a>.</p>

<p>Note that this is only useful to configure at the beginning of the stack.  Once
you&#8217;re up, adding keys is problematic.  I don&#8217;t even recommend trying b/c of
the risk of getting undetermined state for the environment.  i.e., different
nodes with different sets of keys depending on when you changed the keys relative
to what actions you&#8217;ve performed on the environment.  It&#8217;s a problem.</p>

<p>What I recommend now is actually to use <em>another</em> juju environment&#8230;  (and no,
we&#8217;re not paid to promote cloud providers by the instance :) I wish! ) a dedicated
&#8220;control&#8221; environment.  You bootstrap it, then set up a juju client that controls
the main production environment.  Then set up a shared tmux session that any of
the admins for the production environment can use:</p>

<p><a href="http://markmims.com/images/summit-control.png">
<img src="http://markmims.com/images/summit-control.png" width="720px" />
</a></p>

<p>Adding/changing the set of
admin keys is then done in a single place.  This technique isn&#8217;t strictly
necessary, but it was certainly worth it here with different admins having
various different levels of familiarity with the tools.  I started it as a
teaching tool, left it up because it was an easy control dashboard, and now
recommend it because it works so well.</p>

<h3 id="its-chilly-in-here">it&#8217;s chilly in here</h3>

<p>Yeah, so during development you break things.  There were a couple of times
using 11.10 juju that changes to juju core prevented a client from talking to
an existing stack.  Aargh!  This wasn&#8217;t going to fly for production use.</p>

<p>The juju team has subsequently done a <em>bunch</em> to prevent this from happening,
but hey we needed production summit working and stable at the time.  The
answer&#8230; freeze the code.</p>

<p>Juju has an environment config option <code>juju-origin</code> to specify where to
get the juju installed on all instances in the environment.  I branched juju
core to <code>lp:~mark-mims/juju/running-summit</code> and just worked straight from there
for the lifetime of the environment (still up atm).  Easy enough.</p>

<p>Now the tricky part is to make sure that you&#8217;re always using the
<code>lp:~mark-mims/juju/running-summit</code> version of the juju cli when talking to the
production summit environment.</p>

<p>I set up</p>

<pre><code>#!/bin/bash
export JUJU_BRANCH=$HOME/src/juju/running-summit
export PATH=$JUJU_BRANCH/bin:$PATH
export PYTHONPATH=$JUJU_BRANCH
</code></pre>

<p>which my tmuxinator config sources into every pane in my <code>summit</code> tmux session.</p>

<p>This was also done on the <code>summit-control</code> instance so it&#8217;s easy to make sure
we&#8217;re all using the right version of the juju cli to talk to the production
environment.</p>

<h3 id="backups">backups</h3>

<p>The <code>juju ssh</code> subcommand to the rescue.  You can do all your standard ssh
tricks&#8230;</p>

<pre><code>juju ssh postgresql/0 'su postgres pg_dump summit' &gt; summit.dump
</code></pre>

<p>&#8230; on a cronjob.  Juju just stays out of the way and just helps out a bit with
the addressing.  Real version pipes through bzip2 and adds timestamps of course.</p>

<p>Of course snapshots are easy enough too via euca2ools, but the pgsql dumps
themselves turned out to be more useful and easy to get to in case of a
failover.</p>

<h3 id="debugging">debugging</h3>

<p>The biggest debugging activity during development was cleaning up the app&#8217;s
theming.  The summit charm is configured to get the django app itself from
one <a href="https://code.launchpad.net/summit">application branch</a> and the theme from a separate
<a href="https://code.launchpad.net/~lpc-organizers/ubuntu-community-webthemes/light-django-plumbers-theme">theme branch</a>.</p>

<p>So&#8230; ahem&#8230; &#8220;best practice&#8221; for theme development would&#8217;ve been to
develop/tweak the theme locally, then push to the branch.  A simple</p>

<pre><code>juju set --config=summit.yaml summit/0
</code></pre>

<p>would update config for the live instances.</p>

<p>Well&#8230;  some of the menus from the base template used absolute paths so it was
simpler to cheat a bit early in the process to test it all in-place with actual
dns names.  Had we been doing this the &#8220;right&#8221; way from the beginning we
would&#8217;ve had much more confidence in the stack when practicing recovery and
failover later in the cycle&#8230; we would&#8217;ve been doing it all since day one.</p>

<p>Another thing we had to do was manually test memcached.  To test out caching
we&#8217;d ssh to the memcached instance, stop the service, run memcached verbosely
in the foreground.  Once we determined everything was working the way we
expected, we&#8217;d kill it and restart the upstart job.</p>

<p>This is a bug in the memcached charm imo&#8230; the option to temporarily run
verbosely for debugging should totally be a config option for that service.
It&#8217;d then be a simple matter of</p>

<pre><code>juju set memcached/0 debug=true
</code></pre>

<p>and then</p>

<pre><code>juju ssh memcached/0
</code></pre>

<p>to watch some logs.  Once we&#8217;re convinced it&#8217;s working the way it should</p>

<pre><code>juju set memcached/0 debug=false
</code></pre>

<p>should make it performant again.</p>

<p>Next time around, we should take more advantage of <code>juju set</code> config to
update/reconfigure the app as we made changes&#8230; and generally implement a
better set of development practices.</p>

<h3 id="monitoring">monitoring</h3>

<p>Sorely lacking.  &#8220;What? curl doesn&#8217;t cut it?&#8221;&#8230; um&#8230; no.</p>

<h3 id="planning-for-failures">planning for failures</h3>

<p>Our notion of failover for this app was just a spare set of cloud credentials
and a tested recovery plan.</p>

<p>The plan we practiced was&#8230;</p>

<ul>
  <li>bootstrap a new environment (using spare credentials if necessary)</li>
  <li>spin up the summit stack</li>
  <li>ssh to the new <code>postgresql/0</code> and drop the db  (Note: the postgresql charm
should be extended to accept a config parameter of a storage url, S3 in this
case, to slurp the db backups from)</li>
  <li>
    <p>restore from offsite backups&#8230; something along the lines of</p>

    <table>
      <tbody>
        <tr>
          <td>cat summit-$timestamp.dump.bz2</td>
          <td>juju ssh -e failover postgresql/0 &#8216;bunzip2 -c</td>
          <td>su - postgres pgsql summit&#8217;</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>In practice, that took about 10-15minutes to recover once we started acting.
Given the additional delay between notification and action, that could spell an
hour or two of outtage.  That&#8217;s not so great.</p>

<p>Juju makes other failover scenarios cheaper and easier to implement than they
used to be, so why not put those into place just to be safe?  Perhaps the
additional instance costs for hot-spares wouldn&#8217;t&#8217;ve been necessary for the
entire 6-months of lead-time for scheduling and planning this conference, but
they&#8217;d certainly be worth the spend during the few days of the event itself.
Juju sort of makes it a no-brainer.  We should do more posts on this one
issue&#8230; the game has changed here.</p>

<h2 id="lessons-learned">Lessons Learned</h2>

<p>What would we do differently next time?  Well, there&#8217;s a list :).</p>

<ul>
  <li>use the stable ppa&#8230; instead of freezing the code</li>
  <li>sit the app behind haproxy</li>
  <li>use s3fs or equivalent subordinate charm to manage backups instead of just
sshing them off the box</li>
  <li>better monitoring&#8230; we&#8217;ve gotten a great set of monitoring charms
recently&#8230; thanks <a href="http://fewbar.com/">Clint</a>!</li>
  <li>log aggregation would&#8217;ve been a little bit of overkill for this app, but next
time might warrant it</li>
  <li>it&#8217;s cheap to add failover with juju&#8230; just do it</li>
  <li>maybe follow a development process a little more carefully next time around :)</li>
  <li>we&#8217;ll soon have access to a production-stable private ubuntu cloud for these sorts of apps/projects</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scaling a 2000-node Hadoop cluster on EC2/Ubuntu with Juju]]></title>
    <link href="http://markmims.com/cloud/2012/06/04/juju-at-scale.html"/>
    <updated>2012-06-04T00:00:00+00:00</updated>
    <id>http://markmims.com/cloud/2012/06/04/juju-at-scale</id>
    <content type="html"><![CDATA[<p class="meta">
Written by Mark Mims and James Page
</p>

<p>Lately we&#8217;ve been fleshing out our testing frameworks for Juju and Juju Charms.  There&#8217;s
lots of great stuff going on here, so we figured it&#8217;s time to start posting about it.</p>

<p>First off, the coolest thing we did during last month&#8217;s Ubuntu Developer Summit (UDS)
was get the go-ahead to spend more time/effort/money scale-testing Juju.</p>

<!--more-->

<h2 id="the-plan">The Plan</h2>

<ul>
  <li>pick a service that scales</li>
  <li>spin up a cluster of units for this service</li>
  <li>try to run it in a way that actively engages all units of the cluster</li>
  <li>repeat:
    <ul>
      <li>instrument</li>
      <li>profile</li>
      <li>optimize</li>
      <li>grow</li>
    </ul>
  </li>
</ul>

<p><a href="https://plus.google.com/u/0/109495998940814132432/posts">James</a>,
<a href="https://plus.google.com/u/0/108276830347560657704/posts">Kapil</a>,
<a href="https://plus.google.com/102506066601287922723/posts">Juan</a>,
<a href="https://plus.google.com/u/0/100536568598074282388/posts">Ben</a>,
and <a href="http://markmims.com/about">Mark</a>
sat down over the course of
a couple of nights at UDS to take a crack at it.
We chose Hadoop.
We started with 40 nodes and iterated up 100, 500, 1000 and 2000.
Here&#8217;re some notes on the process.</p>

<h2 id="hadoop">Hadoop</h2>

<p>Hadoop was a pretty obvious choice here.
It&#8217;s a great actively-maintained
<a href="http://hadoop.apache.org/">project</a>
with a large community of users.
It scales in a somewhat known manner, and the
<a href="http://jujucharms.com/charms/precise/hadoop">hadoop charm</a>
makes it super-simple to manage.
There are also several known benchmarks that are pretty straightforward to get going,
and distribute load throughout the cluster.</p>

<p>There&#8217;s an entire science/art to tuning hadoop jobs to run optimally given the 
characteristics of a particular cluster.  Our sole goal in tuning hadoop benchmarks
was to <em>engage</em> the entire cluster and profile juju during various activities throughout
an actual run.  For our purposes, we&#8217;re in no hurry&#8230; a slower/longer run gives us a
good profiling picture for managing the nodes themselves under load (with a sufficient
mix of i/o -vs- cpu load).</p>

<h2 id="ec2">EC2</h2>

<p>Surprisingly enough, we don&#8217;t really have that many servers just lying around&#8230; so EC2 to the rescue.</p>

<p>Disclaimer&#8230; we&#8217;re testing our infrastructure tools here, not benchmarking hadoop in EC2.
Some folks advocate running hadoop in a cloudy virtualized environment&#8230; while some
folks are die-hard server huggers.  That&#8217;s actually a really interesting discussion.
It comes down to the actual jobs/problems you&#8217;re
trying to solve and how those jobs fit in your data pipeline.
Please note that we&#8217;re not
trying to solve that problem here or even provide realistic benchmarking data to contribute
to the discussion&#8230; we&#8217;re simply testing how our infrastructure tools perform at scale.</p>

<p>If you <em>do</em> run hadoop in EC2, Amazon&#8217;s Elastic Map Reduce service is likely to perform
better at scale in EC2 than just running hadoop itself on general purpose instances.
Amazon can do all sorts of stuff internally to show hadoop lots of love.
We chose not to use EMR because we&#8217;re interested in testing how juju performs
with <em>generic</em> Ubuntu Server images, not EMR&#8230; at least for now.</p>

<p>Note that stock EC2 accounts limit you to something like 20 instances.  To grow beyond that, you have to
ask AWS to bump up your limits.</p>

<h2 id="juju">Juju</h2>

<p>We started scale testing from a fresh branch of juju trunk&#8230; what gets deployed to
the PPA nightly&#8230; this freed us up to experiment with live changes to add instrumentation,
profiling information, and randomly mess with code as necessary.  This also locks in 
the branch of juju that the scale testing environment uses.</p>

<p>As usual, juju will keep track of the state of our infrastructure going forward and
we can make changes as necessary via juju commands.  To bootstrap and spin up the
initial environment we&#8217;ll just use shell scripts wrapping juju commands.</p>

<h3 id="spinning-up-a-cluster">Spinning up a cluster</h3>

<p>These scripts are really just
hadoop versions of some standard juju demo scripts such as those used for 
a simple <a href="https://gist.github.com/2050525">rails stack</a>
or a more realistic HA <a href="https://gist.github.com/1406018">wiki stack</a>.</p>

<p>The hadoop scripts for EC2 will get a little more complex as we grow simply because
we don&#8217;t want AWS to think we&#8217;re a DoS attack&#8230; we&#8217;ll pace ourselves during spinup.</p>

<p>From the hadoop charm&#8217;s readme, the basic steps to spinning up a simple combined
hdfs and mapreduce cluster are:</p>

<pre><code>juju bootstrap

juju deploy hadoop hadoop-master
juju deploy -n3 hadoop hadoop-slavecluster

juju add-relation hadoop-master:namenode hadoop-slavecluster:datanode
juju add-relation hadoop-master:jobtracker hadoop-slavecluster:tasktracker
</code></pre>

<p>which we expand on a bit to start with a base startup script that looks like:</p>

<pre><code>#!/bin/bash

juju_root="/home/ubuntu/scale"
juju_env=${1:-"-escale"}

###

echo "deploying stack"

juju bootstrap $juju_env

deploy_cluster() {
  local cluster_name=$1

  juju deploy $juju_env --repository "$juju_root/charms" --constraints="instance-type=m1.large" --config "$juju_root/etc/hadoop-master.yaml" local:hadoop ${cluster_name}-master

  juju deploy $juju_env --repository "$juju_root/charms" --constraints="instance-type=m1.medium" --config "$juju_root/etc/hadoop-slave.yaml" -n 37 local:hadoop ${cluster_name}-slave

  juju add-relation $juju_env ${cluster_name}-master:namenode ${cluster_name}-slave:datanode
  juju add-relation $juju_env ${cluster_name}-master:jobtracker ${cluster_name}-slave:tasktracker

  juju expose $juju_env ${cluster_name}-master

}

deploy_cluster hadoop

echo "done"
</code></pre>

<p>and then manually adjust this for cluster size.</p>

<h3 id="configuring-hadoop">Configuring Hadoop</h3>

<p>Note that we&#8217;re specifying constraints to tell juju to use different sized ec2 instances for
different juju services.  We&#8217;d like an m1.large for the
hadoop master</p>

<pre><code>juju deploy ... --constraints "instance-type=m1.large" ... hadoop-master
</code></pre>

<p>and m1.mediums for the slaves</p>

<pre><code>juju deploy ... --constraints "instance-type=m1.medium" ... hadoop-slave
</code></pre>

<p>Note that we&#8217;ll also pass config files to specify different heap sizes for the different memory footprints</p>

<pre><code>juju deploy ... --config "hadoop-master.yaml" ... hadoop-master
</code></pre>

<p>where <code>hadoop-master.yaml</code> looks like</p>

<pre><code># m1.large
hadoop-master:
  heap: 2048
  dfs.block.size: 134217728
  dfs.namenode.handler.count: 20
  mapred.reduce.parallel.copies: 50
  mapred.child.java.opts: -Xmx512m
  mapred.job.tracker.handler.count: 60
#  fs.inmemory.size.mb: 200
  io.sort.factor: 100
  io.sort.mb: 200
  io.file.buffer.size: 131072
  tasktracker.http.threads: 50
  hadoop.dir.base: /mnt/hadoop
</code></pre>

<p>and</p>

<pre><code>juju deploy ... --config "hadoop-slave.yaml" ... hadoop-slave
</code></pre>

<p>where <code>hadoop-slave.yaml</code> looks like</p>

<pre><code># m1.medium
hadoop-slave:
  heap: 1024
  dfs.block.size: 134217728
  dfs.namenode.handler.count: 20
  mapred.reduce.parallel.copies: 50
  mapred.child.java.opts: -Xmx512m
  mapred.job.tracker.handler.count: 60
#  fs.inmemory.size.mb: 200
  io.sort.factor: 100
  io.sort.mb: 200
  io.file.buffer.size: 131072
  tasktracker.http.threads: 50
  hadoop.dir.base: /mnt/hadoop
</code></pre>

<p>Note also that we also have our juju environment configured to use
instance-store images&#8230; juju defaults to ebs-rooted images, but that&#8217;s
not a great idea with hdfs.  You specify this by adding a <code>default-image-id</code>
into your <code>~/.juju/environments.yaml</code> file.
This gave each of our instances an extra ~400G local drive
on <code>/mnt</code>&#8230; hence the <code>hadoop.dir.base</code> of <code>/mnt/hadoop</code>
in the config above.</p>

<h2 id="nodes-and-100-nodes">40 nodes and 100 nodes</h2>

<p>Both the 40-node and 100-node runs went as smooth as silk.
The only thing to note was that it took a while to get AWS to increase
our account limits to allow for 100+ nodes.</p>

<h2 id="nodes">500 nodes</h2>

<p>Once we had permission from Amazon to spin up 500 nodes on our account,
we initially just naively spun
up 500 instances&#8230; and quickly got throttled.</p>

<p>No particular surprise, we&#8217;re not specifying multiplicity in the ec2 api,
nor are we using an auto scaling group&#8230; we must look like a DoS attack.</p>

<p>The order was eventually fulfilled, and juju waited around for it.
Everything ran as expected, it just took about an hour and 15 minutes
to spin up the stack.  This gave us a nice little cluster with HDFS
storage of almost 200TB</p>

<p><a href="http://markmims.com/images/scale-500-50070.png">
<img src="http://markmims.com/images/scale-500-50070.png" width="720px" />
</a></p>

<p>The hadoop terasort job was run from the following script</p>

<pre><code>#!/bin/bash

SIZE=10000000000
NUM_MAPS=1500
NUM_REDUCES=1500
IN_DIR=in_dir
OUT_DIR=out_dir

hadoop jar /usr/lib/hadoop/hadoop-examples*.jar teragen -Dmapred.map.tasks=${NUM_MAPS} ${SIZE} ${IN_DIR}

sleep 10

hadoop jar /usr/lib/hadoop/hadoop-examples*.jar terasort -Dmapred.reduce.tasks=${NUM_REDUCES} ${IN_DIR} ${OUT_DIR}
</code></pre>

<p>which, with a replfactor of 3, engaged the entire cluster just fine, 
and ran terasort with no problems</p>

<p><a href="http://markmims.com/images/scale-500-50030.png">
<img src="http://markmims.com/images/scale-500-50030.png" width="720px" />
</a></p>

<p>Juju itself seemed to work great in this run, but this brought up a couple of basic optimizations against the EC2 api:</p>

<pre><code>- pass the '-n' options directly to the provisioning agent... don't expand `juju deploy -n &lt;num_units&gt;` and `juju add-unit -n &lt;num_units&gt;` in the client
- pass these along all the way to the ec2 api... don't expand these into multiple api calls
</code></pre>

<p>We&#8217;ll add those to the list of things to do.</p>

<h2 id="nodes-1">1000 nodes</h2>

<p>Onward, upward!</p>

<p>To get around the api throttling, we start up
batches of 99 slaves at a time with a 2-minute wait
between each batch</p>

<pre><code>#!/bin/bash

juju_env=${1:-"-escale"}
juju_root="/home/ubuntu/scale"
juju_repo="$juju_root/charms"

############################################

timestamp() {
  date +"%G-%m-%d-%H%M%S"
}

add_more_units() {
  local num_units=$1
  local service_name=$2

  echo "sleeping"
  sleep 120

  echo "adding another $num_units units at $(timestamp)"
  juju add-unit $juju_env -n $num_units $service_name
}

deploy_slaves() {
  local cluster_name=$1
  local slave_config="$juju_root/etc/hadoop-slave.yaml"
  local slave_size="instance-type=m1.medium"
  local slaves_at_a_time=99
  #local num_slave_batches=10

  juju deploy $juju_env --repository $juju_repo --constraints $slave_size --config $slave_config -n $slaves_at_a_time local:hadoop ${cluster_name}-slave
  echo "deployed $slaves_at_a_time slaves"

  juju add-relation $juju_env ${cluster_name}-master:namenode ${cluster_name}-slave:datanode
  juju add-relation $juju_env ${cluster_name}-master:jobtracker ${cluster_name}-slave:tasktracker

  for i in {1..9}; do
    add_more_units $slaves_at_a_time ${cluster_name}-slave
    echo "deployed $slaves_at_a_time slaves at $(timestamp)"
  done
}

deploy_cluster() {
  local cluster_name=$1
  local master_config="$juju_root/etc/hadoop-master.yaml"
  local master_size="instance-type=m1.large"

  juju deploy $juju_env --repository $juju_repo --constraints $master_size --config $master_config local:hadoop ${cluster_name}-master

  deploy_slaves ${cluster_name}

  juju expose $juju_env ${cluster_name}-master
}

main() {
  echo "deploying stack at $(timestamp)"

  juju bootstrap $juju_env --constraints="instance-type=m1.xlarge"

  sleep 120
  deploy_cluster hadoop

  echo "done at $(timestamp)"
}
main $*
exit 0
</code></pre>

<p>We experimented with more clever ways of doing the spinup
(too little coffee at this point of the night)&#8230;
but the real fix is to get juju to take
advantage of multiplicity in api calls.
Until then, timed batches work just fine.</p>

<p>Juju spun the cluster up in about 2 and a half hours.
It had about 380TB of HDFS storage</p>

<p><a href="http://markmims.com/images/scale-1000-50070.png">
<img src="http://markmims.com/images/scale-1000-50070.png" width="720px" />
</a></p>

<p>The terasort job that was run from the script above with</p>

<pre><code>SIZE=10000000000
NUM_MAPS=3000
NUM_REDUCES=3000
</code></pre>

<p><a href="http://markmims.com/images/scale-1000-50030.png">
<img src="http://markmims.com/images/scale-1000-50030.png" width="720px" />
</a></p>

<p>eventually completed.</p>

<h2 id="nodes-2">2000 nodes</h2>

<p>After the 1000-node run, we chose to clean up from the
previous job and just add more nodes to that same cluster.</p>

<p>Again, to get around the api throttling, we added
batches of 99 slaves at a time with a 2-minute wait
between each batch until we got near 2000 slaves.</p>

<p>This gave us almost 760TB of HDFS storage</p>

<p><a href="http://markmims.com/images/scale-2000-50070.png">
<img src="http://markmims.com/images/scale-2000-50070.png" width="720px" />
</a></p>

<p>and was running fine</p>

<p><a href="http://markmims.com/images/scale-2000-50030.png">
<img src="http://markmims.com/images/scale-2000-50030.png" width="720px" />
</a></p>

<p>but was stopped early b/c waiting for the job to complete
would&#8217;ve just been silly at this point.  With our naive job
config, we&#8217;re considerably past the point of diminishing
returns for adding nodes to the actual terasort, and we&#8217;d
captured the profiling info we needed at this point.</p>

<p>Juju spun up 1972 slaves in just over seven hours total.
Profiling showed that juju was spending a <em>lot</em> of time
serializing stuff into zookeeper nodes using yaml.  It
looks like python&#8217;s yaml implementation is python, and
not just wrapping libyaml.  We tested a smaller run replacing
the internal yaml serialization with json.. 
Wham!  two orders of magnitude faster.  No particular surprise.</p>

<h2 id="lessons-learned">Lessons Learned</h2>

<p>Ok, so at the end of the day, what did we learn here?</p>

<p>What we did here is the way developing for performance at scale
should be done&#8230; start with a naive, flexible approach
and then spend time and effort obtaining real profiling
information.  Follow that with optimization decisions that actually
make a difference.  Otherwise it&#8217;s all just a crapshoot
based on where developers think the bottlenecks might be.</p>

<p>Things to do to juju as a result of these tests:</p>

<ul>
  <li>streamline our implementation of &#8216;-n&#8217; options
    <ul>
      <li>the client should pass the multiplicity to the provisioning agent</li>
      <li>the provisioning agent should pass the multiplicity to the EC2 api</li>
    </ul>
  </li>
  <li>don&#8217;t use yaml to marshall data in and out of zookeeper</li>
  <li>replace per-instance security groups with per-instance firewalls</li>
</ul>

<h2 id="whats-next">What&#8217;s Next?</h2>

<p>So that&#8217;s a big enough bite for one round of scale testing.</p>

<p>Next up:</p>

<ul>
  <li>land a few of the changes outlined above into trunk.
Then, spin up another round of scale tests to look at the numbers.</li>
  <li>more providers (other clouds as well as a MaaS lab too)</li>
  <li>regular scale testing?
    <ul>
      <li>can this coincide with upstream scale testing for projects like hadoop?</li>
    </ul>
  </li>
  <li>test scaling for various services?  What does this look like for other stacks
of services?</li>
</ul>

<h2 id="wishlist">Wishlist</h2>

<ul>
  <li>
    <p>find some better test jobs!  benchmarks are boring&#8230; perhaps we can use
this compute time to mine educational data or cure cancer or something?</p>
  </li>
  <li>
    <p>perhaps push juju topology information further into zk leaf nodes?
Are there transactional features in more recent versions of zk that we can use?</p>
  </li>
  <li>
    <p>use spot instances on ec2.  This is harder because you&#8217;ve gotta incorporate price monitoring.</p>
  </li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Charm School!]]></title>
    <link href="http://markmims.com/cloud/2011/11/22/charm-school.html"/>
    <updated>2011-11-22T00:00:00+00:00</updated>
    <id>http://markmims.com/cloud/2011/11/22/charm-school</id>
    <content type="html"><![CDATA[<p>Wanna learn more about <a href="http://juju.ubuntu.com">juju</a>?</p>

<p>Drop by <a href="https://juju.ubuntu.com/CharmSchool/2December11">Charm School</a>:</p>

<p><a href="https://juju.ubuntu.com/CharmSchool/2December11">
  <img src="http://markmims.com/images/charmschool.png" width="220px" />
</a></p>

<!--more-->

<p>Details from <a href="http://tumblr.com/ZKG8NyCGhHZR">Jorge&#8217;s post</a>:</p>

<pre><code>We're holding a Charm School on IRC.

juju Charm School is a virtual event where a juju expert
is available to answer questions about writing your own
juju charms. The intended audience are people who deploy
software and want to contribute charms to the wider devops
community to make deploying in the public and private
cloud easy.

Attendees are more than welcome to:

Ask questions about juju and charms
Ask for help modifying existing scripts and make charms out of them
Ask for peer review on existing charms you might be working on

Though not required, we recommend that you have juju installed
and configured if you want to get deep into the event.
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ensemble renamed to juju]]></title>
    <link href="http://markmims.com/cloud/2011/10/12/project-rename.html"/>
    <updated>2011-10-12T00:00:00+00:00</updated>
    <id>http://markmims.com/cloud/2011/10/12/project-rename</id>
    <content type="html"><![CDATA[<p>Just a note that the Ubuntu Ensemble suite of DevOps tools for
Ubuntu Server has been renamed to <code>juju</code>.</p>

<p>I&#8217;ll be updating previous posts to reflect the name changes so they&#8217;ll be up to date.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Where did the 'close-lid' action go in gnome3?]]></title>
    <link href="http://markmims.com/howtos/2011/09/13/power-lid-closed-actions.html"/>
    <updated>2011-09-13T00:00:00+00:00</updated>
    <id>http://markmims.com/howtos/2011/09/13/power-lid-closed-actions</id>
    <content type="html"><![CDATA[<p>Here&#8217;s a workaround&#8230; (thanks slangasek!)</p>

<pre><code>gsettings set org.gnome.settings-daemon.plugins.power lid-close-ac-action 'nothing'
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Stir that memory of autotools]]></title>
    <link href="http://markmims.com/howtos/2011/09/08/autotools.html"/>
    <updated>2011-09-08T00:00:00+00:00</updated>
    <id>http://markmims.com/howtos/2011/09/08/autotools</id>
    <content type="html"><![CDATA[<p>Ok, never going to forget these again, dammit!</p>

<pre><code>$ aclocal
$ autoconf --force
$ automake --add-missing --copy --force-missing
$ ./configure
$ OS_ARCH=amd64 make
</code></pre>

<p>or sometimes you can use</p>

<pre><code>$ autoreconf --force --install
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Node.js and MongoDB on Ubuntu]]></title>
    <link href="http://markmims.com/cloud/2011/09/07/node-mongo-ubuntu.html"/>
    <updated>2011-09-07T00:00:00+00:00</updated>
    <id>http://markmims.com/cloud/2011/09/07/node-mongo-ubuntu</id>
    <content type="html"><![CDATA[<p>I gave my first talk on IRC the other day on deploying Node.js &amp; Mongo in Ubuntu&#8230; 
it was quite a new experience.
Figured I&#8217;d post details of the talk here.</p>

<h2 id="an-example-stack">An example stack</h2>

<p>We&#8217;ll use <a href="http://juju.ubuntu.com/">juju</a> to deploy a basic
<a href="http://nodejs.org">node.js</a>
app along with a couple of typical surrounding services..
- <a href="http://haproxy.1wt.eu/">haproxy</a> to catch inbound web traffic and route it to our node.js app cluster
- <a href="http://mongodb.org/">mongodb</a> for app storage</p>

<p>Along the way, we&#8217;ll see what it takes to connect and scale this particular stack
of services.  I&#8217;ll err on the side of too much detail over simplicity in this
example, but I&#8217;ll try to make it clear when there&#8217;s a sidebar topic.</p>

<!--more-->

<p>At the end of the day, the deployment for our application
would look like the usual juju deployment</p>

<pre><code>$ juju bootstrap
</code></pre>

<p>(with a pregnant pause to allow EC2 to catch up)</p>

<pre><code>$ juju deploy --repository ~/charms local:mongodb
$ juju deploy --repository ~/charms local:node-app myapp
$ juju add-relation mongodb myapp

$ juju deploy --repository ~/charms local:haproxy
$ juju add-relation myapp haproxy
$ juju expose haproxy
</code></pre>

<p>(with another pregnant pause to allow EC2 to catch up)</p>

<p>We can get the service URLs from</p>

<pre><code>$ juju status
</code></pre>

<p>and hit the head of the haproxy service to see the app in action.</p>

<p>We can scale it out with</p>

<pre><code>$ for i in {1..4}; do
$   juju add-unit myapp
$ done
</code></pre>

<p>and we&#8217;ll soon have a cluster of one haproxy node balancing
between five application nodes all talking to a single mongo
node in the backend.  Of course, we can scale mongo too,
but that&#8217;s another post.</p>

<h2 id="juju-application-charms">juju &#8220;Application&#8221; charms</h2>

<p>There are two types of juju charms used in this example:</p>

<p>&#8220;Canned Charms&#8221;, like the
<a href="http://github.com/mmm/juju-haproxy">haproxy charm</a>
and the
<a href="http://github.com/mmm/juju-mongodb">mongodb charm</a>,
and &#8220;Application Charms&#8221;, like
the <a href="http://github.com/mmm/juju-node-app">node.js app charm</a>.</p>

<p>Canned charms can be used as-is right off the shelf.</p>

<p>Application charms are used to manage your custom application
as an juju service.  We haven&#8217;t nailed down the language on
this, but these charms create a contained environment,
&#8220;framework&#8221;, or &#8220;wrapper&#8221; around your custom application and
help it to play nicely with other services.</p>

<p>The 
<a href="http://github.com/mmm/juju-node-app">node-app charm</a>
we use here
is meant to be an example that you can fork/adapt and use
to maintain custom components of your infrastructure.</p>

<h2 id="the-node-app-charm">The <code>node-app</code> charm</h2>

<p>The <code>node-app</code> charm is the key feature we want to look at.
It&#8217;s a charm that will pull your app from revision control
and config/deploy/maintain it as a service within your
infrastructure.</p>

<p>Setup and clone this charm</p>

<pre><code>$ mkdir ~/charms
$ cd ~/charms
~/charms$ git clone http://github.com/charms/node-app
</code></pre>

<p>and we&#8217;ll walk through it.</p>

<pre><code>README.markdown
config.yaml
copyright
metadata.yaml
revision
hooks/
  install
  mongodb-relation-changed
  mongodb-relation-departed
  mongodb-relation-joined
  start
  stop
  website-relation-joined
</code></pre>

<p>We can see the usual <code>install</code>, <code>start</code>, and <code>stop</code> hooks for the
node.js service, along with a couple of other hooks for relating to
other services.</p>

<p>Before we go into this in detail, let&#8217;s take a little sidebar on
the Node.js app we&#8217;ll be deploying&#8230;</p>

<h3 id="example-nodejs-app">Example node.js app</h3>

<p>The example app I&#8217;m using for this</p>

<pre><code>http://github.com/mmm/testnode
</code></pre>

<p>just logs page hits in mongo and reports results.</p>

<p>As usual, I have absolutely no graphic design gifts so things look a little
bare-bones.  Don&#8217;t let that fool you&#8230; it&#8217;s quite easy to dress this up
with some svg maps and some client-side js a la topfunky&#8217;s (peepcode.com)
node examples.</p>

<p>This is a really basic node app that&#8230;</p>

<p>Reads config info</p>

<pre><code>var config = require('./config/config'),
    mongo = require('mongodb'),
    http = require('http');
</code></pre>

<p>from a file <code>config/config.js</code></p>

<pre><code>module.exports = config = {
   "name" : "mynodeapp"
  ,"listen_port" : 8000
  ,"mongo_host" : "localhost"
  ,"mongo_port" : 27017
}
</code></pre>

<p>attaches to the mongo instance specified in the config file</p>

<pre><code>var db = new mongo.Db('mynodeapp', new mongo.Server(config.mongo_host, config.mongo_port, {}), {});
</code></pre>

<p>spins up a webservice</p>

<pre><code>var server = http.createServer(function (request, response) {

  var url = require('url').parse(request.url);

  if(url.pathname === '/hits') {
    show_log(request, response);
  } else {
    track_hit(request, response);
  }

});
server.listen(config.listen_port);
</code></pre>

<p>and handles requests.</p>

<p>The entire app would look something like</p>

<pre><code>//require.paths.unshift(__dirname + '/lib');
//require.paths.unshift(__dirname);

var config = require('./config/config'),
    mongo = require('mongodb'),
    http = require('http');

var show_log = function(request, response){
  var db = new mongo.Db('mynodeapp', new mongo.Server(config.mongo_host, config.mongo_port, {}), {});
  db.addListener("error", function(error) { console.log("Error connecting to mongo"); });
  db.open(function(err, db){
    db.collection('addresses', function(err, collection){
      collection.find({}, {limit:10, sort:[['_id','desc']]}, function(err, cursor){
        cursor.toArray(function(err, items){
          response.writeHead(200, {'Content-Type': 'text/plain'});
          for(i=0; i&lt;items.length;i++){
            response.write(JSON.stringify(items[i]) + "\n");
          }
          response.end();
        });
      });
    });
  });
}

var track_hit = function(request, response){
  var db = new mongo.Db('mynodeapp', new mongo.Server(config.mongo_host, config.mongo_port, {}), {});
  db.addListener("error", function(error) { console.log("Error connecting to mongo"); });
  db.open(function(err, db){
    db.collection('addresses', function(err, collection){
      var address = request.headers['x-forwarded-for'] || request.connection.remoteAddress;

      hit_record = { 'client': address,'ts': new Date() };

      collection.insert( hit_record, {safe:true}, function(err){
        if(err) { 
          console.log(err.stack);
        }
        response.writeHead(200, {'Content-Type': 'text/plain'});
        response.write(JSON.stringify(hit_record));
        response.end("Tracked hit from " + address + "\n");
      });
    });
  });
}

var server = http.createServer(function (request, response) {

  var url = require('url').parse(request.url);

  if(url.pathname === '/hits') {
    show_log(request, response);
  } else {
    track_hit(request, response);
  }

});
server.listen(config.listen_port);

console.log("Server running at http://0.0.0.0:" + config.listen_port + "/");
</code></pre>

<p>We won&#8217;t get into my node.js skillz at the moment&#8230; 
it&#8217;s a deployment example.</p>

<p>I&#8217;ve also got a <code>package.json</code> in there to let <code>npm</code> resolve
some example dependencies upon install.</p>

<p>Now, there&#8217;s no standard way to handle configuration in node
apps, so it&#8217;s quite likely your app&#8217;s config looks a bit 
different.  No problem, it&#8217;s pretty straightforward to adapt
this example charm to handle the way your app works&#8230;
and use your own config file paths, and config parameter names.</p>

<p>End-of-sidebar&#8230; Back to the <code>node-app</code> charm.</p>

<h3 id="hooks">Hooks</h3>

<p>Let&#8217;s go through the hooks as they would be executing during
deployment and service relation.</p>

<p>The <code>install</code> hook is kicked off upon deployment,
reads its config from <code>config.yaml</code> and then will</p>

<ul>
  <li>install <code>node</code>/<code>npm</code></li>
  <li>clone your node app from the repo specified in <code>app_repo</code></li>
  <li>run <code>npm</code> if your app contains <code>package.json</code></li>
  <li>configure networking if your app contains <code>config/config.js</code></li>
  <li>create a startup service for your app</li>
  <li>wait to startup once we&#8217;re joined to a <code>mongodb</code> service</li>
</ul>

<p><code>start</code> and <code>stop</code> are trivial in this charm because
we want to wait for mongo to join before we actually run
the app.  If your app was simpler and didn&#8217;t depend on
a backing store, then you could use these hooks to
manage the service created during installation.</p>

<h3 id="mongodb">MongoDB</h3>

<p>The key to almost every charm is in the relation hooks.</p>

<p>This particular app is written against mongodb
so the app&#8217;s charm has hooks that get fired when
the &#8220;app&#8221; service is related to the mongo service.</p>

<p>This relation was defined when we did</p>

<pre><code>$ juju add-relation mongodb myapp
</code></pre>

<p>and the <code>relation-joined/changed</code> hooks
get fired after the <code>install</code> and <code>start</code>
hooks have successfully completed for both
ends of the relationship.</p>

<p>The <code>mongodb-relation-changed</code> hook in this charm
will read config from <code>config.yaml</code></p>

<ul>
  <li>get relation info from the mongo service (i.e., hostname)</li>
  <li>configure the app to use that host for mongo connections</li>
  <li>start the node app service we created during <code>install</code></li>
</ul>

<p>That&#8217;s it really&#8230; our app is up and running at this
point.</p>

<p>Note that the example here depends on mongo,
but juju makes it easy to relate to some other backend db.
Just like we have <code>mongodb-relation-changed</code> hooks, we
could just as easily have <code>cassandra-relation-changed</code> hooks
that would look strikingly similar.  Of course, our app would
have to be written in such a way that it could use either,
but that&#8217;s another topic.  The deployment tool supports
the choice being made dynamically when relations are joined.
I&#8217;d say &#8220;at deployment time&#8221; but it&#8217;s even better than that
because I can remove relations and add other ones at 
any time throughout the lifetime of the service&#8230; and the
correct hooks get called.</p>

<h3 id="haproxy">HAProxy</h3>

<p>For this example, I&#8217;d like to use haproxy to load balance</p>

<p>This example stack uses haproxy to handle initial
web requests from outside.  haproxy will load balance
across multiple instances of our app.  That way we
could just attach an elastic ip to haproxy, configure
dns, and we&#8217;re cruising 
(of course we&#8217;re leaving out
plenty of infrastructure aspects like 
monitoring/logging/backups/etc that are
pretty important for a production deployment
in the cloud).</p>

<p>The app charm has
hooks that get fired when
the &#8220;app&#8221; service is related to the haproxy service.
Just as above, this relation was defined when we did</p>

<pre><code>$ juju add-relation haproxy myapp
</code></pre>

<p>and the <code>relation-joined/changed</code> hooks
get fired after the <code>install</code> and <code>start</code>
hooks have successfully completed for both
ends of the relationship.</p>

<p>The <code>website-relation-changed</code> hook in this charm
in its entirety:</p>

<pre><code>#!/bin/sh
 
app_port=`config-get app_port`
relation-set port=$app_port hostname=`hostname -f`
</code></pre>

<p>simply tells the haproxy service which 
address and port our application uses to handle
requests.</p>

<p>We could of course configure our app to listen on port
80, tell the charm to open port 80 in its firewall,
and then expose port 80 for our app service to the
outside world.  That&#8217;d be fine if we never needed to
scale or we were planning to load balance multiple
units of our app using dns, elastic load balancer instances,
or something else external.</p>

<p>Again, note that the example here uses haproxy, but
we could easily swap that out with any other service
that consumed the juju <code>http</code> interface.</p>

<h3 id="charm-configuration">Charm configuration</h3>

<p>Ok, so I lied a little up above when I said that the hooks
read config info from <code>config.yaml</code>.  Yes, they do read config
information from there, but that&#8217;s not the whole story.
The values of the configurable parameters can be set/overidden
in a number of different ways throughout the lifecycle of
the service.</p>

<p>You can pass in dynamic configuration during deployment or later
at runtime using the cli</p>

<pre><code>`juju set &lt;service_name&gt; &lt;config_param&gt;=&lt;value&gt;`
</code></pre>

<p>or configure the charm at deployment time via a <code>yaml</code> file
passed to the <code>juju deploy --config</code> command.</p>

<h3 id="scaling-tips">Scaling tips</h3>

<p>Scaling with juju works really well.  The key
to this lies in the boundaries between
configuration for the service itself, versus configuration for
the service <em>in the context of</em> a relation with another service.</p>

<p>When these two types of configuration are well isolated,
scaling with juju just works.  I&#8217;ve caught myself several
times working on just getting a service charm working, with
no real thought to scalability, and being pleasantly surprised
to find out that the service pretty much scales as written.</p>

<p>The best way to grok this is to walk through the process
of joining your relations as single unit services&#8230;</p>

<p>In our example,</p>

<pre><code>haproxy &lt;-&gt; myapp &lt;-&gt; mongodb
</code></pre>

<p>containers for each service get instantiated, then the <code>install</code>
and <code>start</code> hooks are run for each service.  Once both sides
of relations are <code>started</code> then the relation hooks get called:
<code>joined</code> and then usually several rounds of <code>changed</code> depending
on the relation parameters being set.  Once these are complete,
the services are up, related, and running.</p>

<p>Ok, now comes scaling.  <code>juju add-unit myapp</code> adds a new
<code>myapp</code> service node and goes through the whole cycle above.
The &#8220;services&#8221; are already related, so the relation hooks are
automatically fired as each new unit is <code>started</code>.
Since we divided up
the installation/configuration/setup/startup of the service
into the parts that are specific to the service and parts that
are specific to the relation with another service, then each
new unit runs &#8220;just enough&#8221; configuration to join it to the
cluster.</p>

<p>Not all tools can be configured like that, but that&#8217;s the key
to strive for when writing relation hooks.
Identify the components of your application
configuration that really depend on another service, and 
isolate them as much as possible.
Only configure 
relation-specific things in the relation hooks.
The more minimal the relation hooks,
the more scalable the service.</p>

]]></content>
  </entry>
  
</feed>
