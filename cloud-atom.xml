<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 <title>Mark Mims</title>
 <link href="http://markmims.com/cloud-atom.xml" rel="self"/>
 <link href="http://markmims.com/"/>
 <updated>2024-02-19T17:25:18+00:00</updated>
 <id>http://markmims.com/</id>
 <author>
   <name>Mark Mims</name>
   <email>mark.mims@canonical.com</email>
 </author>
 
 <entry>
   <title>Projects in GCP using Central Billing Accounts</title>
   <link href="http://markmims.com/devops/cloud/gcp/iam/2019/06/05/managed-projects-in-gcp.html"/>
   <updated>2019-06-05T00:00:00+00:00</updated>
   <id>http://markmims.com/devops/cloud/gcp/iam/2019/06/05/managed-projects-in-gcp</id>
   <content type="html">&lt;p&gt;Many organizations recognize the benefits of empowering their developers. In a
cloud environment, that often means giving developers the ability to create and
manage their own infrastructure.&lt;/p&gt;

&lt;p&gt;Of course, developers can easily create their own individual or G-Suite GCP
accounts.  They can take advantage of the free trial that Google Cloud offers.
That&amp;#8217;s great, and everything&amp;#8217;s hunky-dory until the credit runs out. What then?&lt;/p&gt;

&lt;p&gt;In this post I describe a really simple way to set up and use centralized
billing on GCP&amp;#8230; even across external development accounts.  Way better than
trying to get me to fill out expense reports for infradev!&lt;/p&gt;

&lt;!&#8211;more&#8211;&gt;

&lt;h2 id=&quot;contents&quot;&gt;Contents&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Organizations and account setup&lt;/li&gt;
  &lt;li&gt;Users and IAM roles&lt;/li&gt;
  &lt;li&gt;Terraform templates&lt;/li&gt;
  &lt;li&gt;Try it out&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;organizations-and-account-setup&quot;&gt;Organizations and account setup&lt;/h2&gt;

&lt;p&gt;Let&amp;#8217;s consider a common example with two separate organizations in the mix.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;A &lt;code&gt;bigcorp.com&lt;/code&gt; organization that&amp;#8217;s footing the bill for everything&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An individual developer&amp;#8217;s G-Suite organization, &lt;code&gt;pinkponies.io&lt;/code&gt;, where
we&amp;#8217;ll be doing the development&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this example, we&amp;#8217;re assuming the developer organization &lt;code&gt;pinkponies.io&lt;/code&gt; is a
full G-Suite account and not just an ordinary GCP account created using a
single email.&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s easy for an individual developer to create a new G-Suite account and that
turns out to be the more typical situation for this kind of cross billing
example. I also really recommend using developer G-Suite accounts for cloud
development in general since they&amp;#8217;ll have the same IAM capabilities and
concerns as the &lt;code&gt;bigcorp.com&lt;/code&gt; account.&lt;/p&gt;

&lt;h2 id=&quot;users-and-iam-roles&quot;&gt;Users and IAM roles&lt;/h2&gt;

&lt;p&gt;Each developer will need accounts in both orgs to start with.&lt;/p&gt;

&lt;p&gt;Take Sam for example. Sam&amp;#8217;s already an Owner of &lt;code&gt;pinkponies.io&lt;/code&gt;&amp;#8230;  with
&lt;code&gt;sam@pinkponies.io&lt;/code&gt; as a login.&lt;/p&gt;

&lt;p&gt;Sam works for BigCorp and is also &lt;code&gt;sam@bigcorp.com&lt;/code&gt; where they live in some
folder within the &lt;code&gt;bigcorp.com&lt;/code&gt; organization&amp;#8217;s GCP IAM.&lt;/p&gt;

&lt;h3 id=&quot;in-your-billing-org-bigcorpcom&quot;&gt;In your billing org: &lt;code&gt;bigcorp.com&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;So the &lt;code&gt;billing_account_user&lt;/code&gt; (&lt;code&gt;sam@bigcorp.com&lt;/code&gt;) needs to be able to create
billing accounts within the BigCorp org.&lt;/p&gt;

&lt;p&gt;Sam will need to be assigned a &lt;code&gt;BillingAccountCreator&lt;/code&gt; role within the
&lt;code&gt;bigcorp.com&lt;/code&gt; org&amp;#8217;s IAM on GCP.&lt;/p&gt;

&lt;h3 id=&quot;in-your-gsuite-org-pinkponiesio&quot;&gt;In your gsuite org: &lt;code&gt;pinkponies.io&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;It&amp;#8217;s no surprise, the &lt;code&gt;gsuite_user&lt;/code&gt; (&lt;code&gt;sam@pinkponies.io&lt;/code&gt;) needs to be an
&lt;code&gt;OrganizationAdministrator&lt;/code&gt; on that org.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;billing_account_user&lt;/code&gt; (&lt;code&gt;sam@bigcorp.com&lt;/code&gt;) needs permissions on the
&lt;code&gt;pinkponies.io&lt;/code&gt; org too. They need to be:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a &lt;code&gt;BillingAccountAdministrator&lt;/code&gt; for the &lt;code&gt;pinkponies.io&lt;/code&gt; org&lt;/li&gt;
  &lt;li&gt;a &lt;code&gt;ProjectCreator&lt;/code&gt; on the &lt;code&gt;pinkponies.io&lt;/code&gt; org&lt;/li&gt;
  &lt;li&gt;and I added them as an &lt;code&gt;OrganizationAdministrator&lt;/code&gt; on &lt;code&gt;pinkponies.org&lt;/code&gt; for
good measure&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;terraform-templates&quot;&gt;Terraform templates&lt;/h2&gt;

&lt;p&gt;I like to manage infrastructure using &lt;a href=&quot;terraform.io&quot;&gt;Terraform&lt;/a&gt; and keep
all my templates and modules checked into GitHub.&lt;/p&gt;

&lt;p&gt;The Terraform templates to create these projects are super simple. There&amp;#8217;s a
provider, a resource for the managed project we want to create, and then a
couple of role binding resources&lt;/p&gt;

&lt;div class=&quot;bogus-wrapper&quot;&gt;&lt;notextile&gt;&lt;figure class=&quot;code&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre class=&quot;line-numbers&quot;&gt;&lt;span class=&quot;line-number&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;5&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;6&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;7&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;8&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;9&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;11&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;12&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;13&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;14&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;15&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;16&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;17&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;18&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;19&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;20&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;21&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;code class=&quot;&quot;&gt;&lt;span class=&quot;line&quot;&gt;provider &quot;google&quot; {
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;  region      = &quot;${var.region}&quot;
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;}
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;resource &quot;google_project&quot; &quot;gsuite_project&quot; {
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;  name       = &quot;gsuite-project-0&quot;
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;  project_id = &quot;gsuite-project-0&quot;
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;  org_id = &quot;${var.gsuite_org_id}&quot;
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;  billing_account = &quot;${var.billing_account_id}&quot;
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;}
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;resource &quot;google_project_iam_binding&quot; &quot;gsuite_project_owner&quot; {
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;  project = &quot;${google_project.gsuite_project.project_id}&quot;
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;  role    = &quot;roles/owner&quot;
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;  members = [
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;    &quot;user:${var.gsuite_user}&quot;,
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;    &quot;user:${var.billing_account_user}&quot;,
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;  ]
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;&lt;/figure&gt;&lt;/notextile&gt;&lt;/div&gt;

&lt;p&gt;There&amp;#8217;s no need to get Terraform to slurp in data sources for the GCP orgs,
folders, billing accounts, etc. In this example, we&amp;#8217;ll just create variables
for them&lt;/p&gt;

&lt;div class=&quot;bogus-wrapper&quot;&gt;&lt;notextile&gt;&lt;figure class=&quot;code&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre class=&quot;line-numbers&quot;&gt;&lt;span class=&quot;line-number&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;5&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;6&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;7&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;8&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;9&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;10&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;code class=&quot;&quot;&gt;&lt;span class=&quot;line&quot;&gt;variable &quot;region&quot; {
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;  default = &quot;us-central1&quot;
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;}
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;variable &quot;billing_account_user&quot; {}
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;variable &quot;billing_folder_id&quot; {}
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;variable &quot;billing_account_id&quot; {}
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;variable &quot;gsuite_user&quot; {}
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;variable &quot;gsuite_org_id&quot; {}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;&lt;/figure&gt;&lt;/notextile&gt;&lt;/div&gt;

&lt;p&gt;and look up the values from the cloud consoles for both our &lt;code&gt;bigcorp.com&lt;/code&gt; and
&lt;code&gt;pinkponies.io&lt;/code&gt; accounts.  We&amp;#8217;ll add these to &lt;code&gt;terraform.tfvars&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;bogus-wrapper&quot;&gt;&lt;notextile&gt;&lt;figure class=&quot;code&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre class=&quot;line-numbers&quot;&gt;&lt;span class=&quot;line-number&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;5&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;6&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;code class=&quot;&quot;&gt;&lt;span class=&quot;line&quot;&gt;billing_account_user = &quot;sam@bigcorp.com&quot;
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;billing_folder_id = &quot;234567890123&quot; # my-billing-folder
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;billing_account_id = &quot;aaaaaa-bbbbbb-cccccc&quot; # my-billing-account
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;gsuite_user = &quot;sam@pinkponies.io&quot;
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;gsuite_org_id = &quot;345678901234&quot; # pinkponies.io&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;&lt;/figure&gt;&lt;/notextile&gt;&lt;/div&gt;

&lt;p&gt;Note that there&amp;#8217;s a &lt;code&gt;terraform.tfvars.template&lt;/code&gt; included in the example repo
but the actual &lt;code&gt;*.tfvars&lt;/code&gt; files, with sensitive account details, are ignored by
revision control so you&amp;#8217;ll have to copy the template and create your own
&lt;code&gt;terraform.tfvars&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;try-it-out&quot;&gt;Try it out&lt;/h2&gt;

&lt;h3 id=&quot;example-repo&quot;&gt;Example repo&lt;/h3&gt;

&lt;p&gt;You can clone and configure the example templates&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;clone &lt;a href=&quot;https://github.com/mmm/gcp-managed-projects&quot;&gt;https://github.com/mmm/gcp-managed-projects&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;copy the tfvars template over to &lt;code&gt;terraform.tfvars&lt;/code&gt; and edit it with your info&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gcloud&quot;&gt;&lt;code&gt;gcloud&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;Terraform&amp;#8217;s provider for GCP needs GCP credentials for your account.  The
easiest thing to do to get that working before trying to run Terraform is to
make sure gcloud is working correctly.&lt;/p&gt;

&lt;p&gt;You can do that by installing gcloud and running &lt;code&gt;gcloud init&lt;/code&gt; to go through
the oauth dance&amp;#8230; that works.  You&amp;#8217;d need to export your
&lt;code&gt;GOOGLE_APPLICATION_CREDENTIALS&lt;/code&gt; as well&amp;#8230; usual stuff.&lt;/p&gt;

&lt;p&gt;However, as an easier alternative, use the cloud shell in the cloud console for
your &lt;code&gt;bigcorp.com&lt;/code&gt; equivalent account.  The gcloud config and applcation
credentials are all already set up for you.&lt;/p&gt;

&lt;p&gt;Side note: The cloud shell is &lt;em&gt;really&lt;/em&gt; useful&amp;#8230; check it out if you haven&amp;#8217;t!&lt;/p&gt;

&lt;p&gt;Make sure you&amp;#8217;re driving terraform using credentials (your &lt;code&gt;gcloud&lt;/code&gt; config)
from the equivalent of your &lt;code&gt;bigcorp.com&lt;/code&gt; account and &lt;em&gt;not&lt;/em&gt; your
&lt;code&gt;pinkponies.io&lt;/code&gt; G-Suite org account.&lt;/p&gt;

&lt;h3 id=&quot;terraform&quot;&gt;Terraform&lt;/h3&gt;

&lt;p&gt;Download Terraform from &lt;a href=&quot;https://terraform.io/&quot;&gt;https://terraform.io/&lt;/a&gt;.  Terraform is a standalone
binary so it&amp;#8217;s simple to install&amp;#8230; even in your GCP Cloud Shell.&lt;/p&gt;

&lt;p&gt;Init terraform&amp;#8217;s providers and state management&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform init
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then check out what changes we&amp;#8217;re _plan_ning to make&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform plan
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If all looks good from there, then &lt;em&gt;apply&lt;/em&gt; that plan to actually create our
project&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform apply
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check out the project we just created&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gcloud beta billing projects list &#8211;billing-account=&amp;lt;billing_account_id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check out the same project from the Cloud Console for your &lt;code&gt;pinkponies.io&lt;/code&gt;
G-Suite account.&lt;/p&gt;

&lt;p&gt;Now you can use that account within your &lt;code&gt;pinkponies.io&lt;/code&gt; G-Suite account and
any charges go straight to your BigCorp billing account.&lt;/p&gt;

&lt;h3 id=&quot;cleanup&quot;&gt;Cleanup&lt;/h3&gt;

&lt;p&gt;When you&amp;#8217;re all done, you can clean up after yourself by removing the project
and role bindings we created&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform destroy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;then deleting the billing account through the Cloud Console.  You could (and
should) totally manage the billing accounts themselves in the bigcorp.org using
Terraform templates as well, but that&amp;#8217;s another story.&lt;/p&gt;

&lt;h2 id=&quot;disclaimer&quot;&gt;Disclaimer&lt;/h2&gt;

&lt;p&gt;No big corps or pink ponies were harmed in the production of this post.&lt;/p&gt;
</content>
   <author>
     <name>Mark Mims</name>
     <uri>http://markmims.com/</uri>
   </author>
 </entry>
 
 <entry>
   <title>CharmSchool Hangout - Juju Local Provider</title>
   <link href="http://markmims.com/cloud/2013/08/23/charmschool-local-provider.html"/>
   <updated>2013-08-23T13:02:00+00:00</updated>
   <id>http://markmims.com/cloud/2013/08/23/charmschool-local-provider</id>
   <content type="html">&lt;p&gt;Continuing the series of regular CharmSchool Hangouts.  This week&amp;#8217;s video
walks through using the new version of the juju local provider.  It&amp;#8217;s cool!&lt;/p&gt;

&lt;div class=&quot;ratio-4-3 embed-video-container&quot; onclick=&quot;var myAnchor = document.getElementById(&#39;3AiQhHIBQJk&#39;);var tmpDiv = document.createElement(&#39;div&#39;);tmpDiv.innerHTML = &#39;&amp;lt;iframe style=&amp;quot;vertical-align:top;width:100%;height:100%;position:absolute;&amp;quot; src=&amp;quot;http://www.youtube.com/embed/3AiQhHIBQJk?autoplay=1&amp;quot; frameborder=&amp;quot;0&amp;quot; allowfullscreen&amp;gt;&amp;lt;/iframe&amp;gt;&#39;;myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;&quot; title=&quot;click here to play&quot;&gt;
&lt;a class=&quot;youtube-lazy-link&quot; style=&quot;width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/3AiQhHIBQJk/0.jpg) center center no-repeat;background-size:contain;position:absolute&quot; href=&quot;http://www.youtube.com/watch?v=3AiQhHIBQJk&quot; id=&quot;3AiQhHIBQJk&quot; onclick=&quot;return false;&quot;&gt;
&lt;div class=&quot;youtube-lazy-link-div&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;youtube-lazy-link-info&quot;&gt;embedded youtube video 3AiQhHIBQJk&lt;/div&gt;
&lt;/a&gt;
&lt;div class=&quot;video-info&quot;&gt;embedded youtube video 3AiQhHIBQJk&lt;/div&gt;
&lt;/div&gt;

&lt;!&#8211;more&#8211;&gt;

&lt;p&gt;As before, there are links to the whole series of charmschool hangouts in the juju
&lt;a href=&quot;https://juju.ubuntu.com/resources/videos/&quot;&gt;video archive&lt;/a&gt;
where we also have videos and screencasts of demos, talks,  and any other charm
schools we&amp;#8217;ve been able to capture on video.&lt;/p&gt;
</content>
   <author>
     <name>Mark Mims</name>
     <uri>http://markmims.com/</uri>
   </author>
 </entry>
 
 <entry>
   <title>CharmSchool Hangout - Juju Events in Depth</title>
   <link href="http://markmims.com/cloud/2013/07/12/charmschool-events-in-depth.html"/>
   <updated>2013-07-12T13:02:00+00:00</updated>
   <id>http://markmims.com/cloud/2013/07/12/charmschool-events-in-depth</id>
   <content type="html">&lt;p&gt;Continuing the series of regular CharmSchool Hangouts.  This week&amp;#8217;s video
is a little more detail on juju events&amp;#8230;&lt;/p&gt;

&lt;div class=&quot;ratio-4-3 embed-video-container&quot; onclick=&quot;var myAnchor = document.getElementById(&#39;vPBrpMcXHN0&#39;);var tmpDiv = document.createElement(&#39;div&#39;);tmpDiv.innerHTML = &#39;&amp;lt;iframe style=&amp;quot;vertical-align:top;width:100%;height:100%;position:absolute;&amp;quot; src=&amp;quot;http://www.youtube.com/embed/vPBrpMcXHN0?autoplay=1&amp;quot; frameborder=&amp;quot;0&amp;quot; allowfullscreen&amp;gt;&amp;lt;/iframe&amp;gt;&#39;;myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;&quot; title=&quot;click here to play&quot;&gt;
&lt;a class=&quot;youtube-lazy-link&quot; style=&quot;width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/vPBrpMcXHN0/0.jpg) center center no-repeat;background-size:contain;position:absolute&quot; href=&quot;http://www.youtube.com/watch?v=vPBrpMcXHN0&quot; id=&quot;vPBrpMcXHN0&quot; onclick=&quot;return false;&quot;&gt;
&lt;div class=&quot;youtube-lazy-link-div&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;youtube-lazy-link-info&quot;&gt;embedded youtube video vPBrpMcXHN0&lt;/div&gt;
&lt;/a&gt;
&lt;div class=&quot;video-info&quot;&gt;embedded youtube video vPBrpMcXHN0&lt;/div&gt;
&lt;/div&gt;

&lt;!&#8211;more&#8211;&gt;

&lt;p&gt;As before, there are links to the whole series of charmschool hangouts in the juju
&lt;a href=&quot;https://juju.ubuntu.com/resources/videos/&quot;&gt;video archive&lt;/a&gt;
where we also have videos and screencasts of demos, talks,  and any other charm
schools we&amp;#8217;ve been able to capture on video.&lt;/p&gt;
</content>
   <author>
     <name>Mark Mims</name>
     <uri>http://markmims.com/</uri>
   </author>
 </entry>
 
 <entry>
   <title>CharmSchool Hangout - Charming Best Practices</title>
   <link href="http://markmims.com/cloud/2013/06/28/charmschool-charm-best-practices.html"/>
   <updated>2013-06-28T13:02:00+00:00</updated>
   <id>http://markmims.com/cloud/2013/06/28/charmschool-charm-best-practices</id>
   <content type="html">&lt;p&gt;Continuing the series of regular CharmSchool Hangouts.  In this week&amp;#8217;s charmschool
we just sort of sat around and discussed best practices&amp;#8230;&lt;/p&gt;

&lt;div class=&quot;ratio-4-3 embed-video-container&quot; onclick=&quot;var myAnchor = document.getElementById(&#39;08dOs3eO04M&#39;);var tmpDiv = document.createElement(&#39;div&#39;);tmpDiv.innerHTML = &#39;&amp;lt;iframe style=&amp;quot;vertical-align:top;width:100%;height:100%;position:absolute;&amp;quot; src=&amp;quot;http://www.youtube.com/embed/08dOs3eO04M?autoplay=1&amp;quot; frameborder=&amp;quot;0&amp;quot; allowfullscreen&amp;gt;&amp;lt;/iframe&amp;gt;&#39;;myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;&quot; title=&quot;click here to play&quot;&gt;
&lt;a class=&quot;youtube-lazy-link&quot; style=&quot;width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/08dOs3eO04M/0.jpg) center center no-repeat;background-size:contain;position:absolute&quot; href=&quot;http://www.youtube.com/watch?v=08dOs3eO04M&quot; id=&quot;08dOs3eO04M&quot; onclick=&quot;return false;&quot;&gt;
&lt;div class=&quot;youtube-lazy-link-div&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;youtube-lazy-link-info&quot;&gt;embedded youtube video 08dOs3eO04M&lt;/div&gt;
&lt;/a&gt;
&lt;div class=&quot;video-info&quot;&gt;embedded youtube video 08dOs3eO04M&lt;/div&gt;
&lt;/div&gt;

&lt;!&#8211;more&#8211;&gt;

&lt;p&gt;As before, there are links to the whole series of charmschool hangouts in the juju
&lt;a href=&quot;https://juju.ubuntu.com/resources/videos/&quot;&gt;video archive&lt;/a&gt;
where we also have videos and screencasts of demos, talks,  and any other charm
schools we&amp;#8217;ve been able to capture on video.&lt;/p&gt;
</content>
   <author>
     <name>Mark Mims</name>
     <uri>http://markmims.com/</uri>
   </author>
 </entry>
 
 <entry>
   <title>Juju Demo Videos</title>
   <link href="http://markmims.com/cloud/2013/06/15/juju-demo-videos.html"/>
   <updated>2013-06-15T13:02:00+00:00</updated>
   <id>http://markmims.com/cloud/2013/06/15/juju-demo-videos</id>
   <content type="html">&lt;p&gt;I Put together a series of demo videos using juju-0.7 for oscon.&lt;/p&gt;

&lt;p&gt;These are really interesting in that they involve migrating environments
between providers.  This works slightly differently on the newer juju-1.x
series, but the idea&amp;#8217;s still sound.&lt;/p&gt;

&lt;p&gt;There&amp;#8217;s no sound on these&amp;#8230; they&amp;#8217;re raw video backups for demoing juju (in
case we lost networking during the demo).&lt;/p&gt;

&lt;p&gt;migrate local to hp&lt;/p&gt;

&lt;div class=&quot;ratio-4-3 embed-video-container&quot; onclick=&quot;var myAnchor = document.getElementById(&#39;Jfnxl1Kh9SY&#39;);var tmpDiv = document.createElement(&#39;div&#39;);tmpDiv.innerHTML = &#39;&amp;lt;iframe style=&amp;quot;vertical-align:top;width:100%;height:100%;position:absolute;&amp;quot; src=&amp;quot;http://www.youtube.com/embed/Jfnxl1Kh9SY?autoplay=1&amp;quot; frameborder=&amp;quot;0&amp;quot; allowfullscreen&amp;gt;&amp;lt;/iframe&amp;gt;&#39;;myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;&quot; title=&quot;click here to play&quot;&gt;
&lt;a class=&quot;youtube-lazy-link&quot; style=&quot;width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/Jfnxl1Kh9SY/0.jpg) center center no-repeat;background-size:contain;position:absolute&quot; href=&quot;http://www.youtube.com/watch?v=Jfnxl1Kh9SY&quot; id=&quot;Jfnxl1Kh9SY&quot; onclick=&quot;return false;&quot;&gt;
&lt;div class=&quot;youtube-lazy-link-div&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;youtube-lazy-link-info&quot;&gt;embedded youtube video Jfnxl1Kh9SY&lt;/div&gt;
&lt;/a&gt;
&lt;div class=&quot;video-info&quot;&gt;embedded youtube video Jfnxl1Kh9SY&lt;/div&gt;
&lt;/div&gt;

&lt;!&#8211;more&#8211;&gt;

&lt;p&gt;migrate ec2 to hpcloud&lt;/p&gt;

&lt;div class=&quot;ratio-4-3 embed-video-container&quot; onclick=&quot;var myAnchor = document.getElementById(&#39;HUtR3_YlKXU&#39;);var tmpDiv = document.createElement(&#39;div&#39;);tmpDiv.innerHTML = &#39;&amp;lt;iframe style=&amp;quot;vertical-align:top;width:100%;height:100%;position:absolute;&amp;quot; src=&amp;quot;http://www.youtube.com/embed/HUtR3_YlKXU?autoplay=1&amp;quot; frameborder=&amp;quot;0&amp;quot; allowfullscreen&amp;gt;&amp;lt;/iframe&amp;gt;&#39;;myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;&quot; title=&quot;click here to play&quot;&gt;
&lt;a class=&quot;youtube-lazy-link&quot; style=&quot;width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/HUtR3_YlKXU/0.jpg) center center no-repeat;background-size:contain;position:absolute&quot; href=&quot;http://www.youtube.com/watch?v=HUtR3_YlKXU&quot; id=&quot;HUtR3_YlKXU&quot; onclick=&quot;return false;&quot;&gt;
&lt;div class=&quot;youtube-lazy-link-div&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;youtube-lazy-link-info&quot;&gt;embedded youtube video HUtR3_YlKXU&lt;/div&gt;
&lt;/a&gt;
&lt;div class=&quot;video-info&quot;&gt;embedded youtube video HUtR3_YlKXU&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Local provider&lt;/p&gt;

&lt;div class=&quot;ratio-4-3 embed-video-container&quot; onclick=&quot;var myAnchor = document.getElementById(&#39;EpIP4ly4E0E&#39;);var tmpDiv = document.createElement(&#39;div&#39;);tmpDiv.innerHTML = &#39;&amp;lt;iframe style=&amp;quot;vertical-align:top;width:100%;height:100%;position:absolute;&amp;quot; src=&amp;quot;http://www.youtube.com/embed/EpIP4ly4E0E?autoplay=1&amp;quot; frameborder=&amp;quot;0&amp;quot; allowfullscreen&amp;gt;&amp;lt;/iframe&amp;gt;&#39;;myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;&quot; title=&quot;click here to play&quot;&gt;
&lt;a class=&quot;youtube-lazy-link&quot; style=&quot;width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/EpIP4ly4E0E/0.jpg) center center no-repeat;background-size:contain;position:absolute&quot; href=&quot;http://www.youtube.com/watch?v=EpIP4ly4E0E&quot; id=&quot;EpIP4ly4E0E&quot; onclick=&quot;return false;&quot;&gt;
&lt;div class=&quot;youtube-lazy-link-div&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;youtube-lazy-link-info&quot;&gt;embedded youtube video EpIP4ly4E0E&lt;/div&gt;
&lt;/a&gt;
&lt;div class=&quot;video-info&quot;&gt;embedded youtube video EpIP4ly4E0E&lt;/div&gt;
&lt;/div&gt;

</content>
   <author>
     <name>Mark Mims</name>
     <uri>http://markmims.com/</uri>
   </author>
 </entry>
 
 <entry>
   <title>CharmSchool Hangout - Charming from scratch</title>
   <link href="http://markmims.com/cloud/2013/06/05/charmschool-hangout-charming-from-scratch.html"/>
   <updated>2013-06-05T13:02:00+00:00</updated>
   <id>http://markmims.com/cloud/2013/06/05/charmschool-hangout-charming-from-scratch</id>
   <content type="html">&lt;p&gt;Continuing the series of regular CharmSchool Hangouts.  In last week&amp;#8217;s video
we wrote a charm from scratch&amp;#8230;&lt;/p&gt;

&lt;div class=&quot;ratio-4-3 embed-video-container&quot; onclick=&quot;var myAnchor = document.getElementById(&#39;NQmxuzdc4Zg&#39;);var tmpDiv = document.createElement(&#39;div&#39;);tmpDiv.innerHTML = &#39;&amp;lt;iframe style=&amp;quot;vertical-align:top;width:100%;height:100%;position:absolute;&amp;quot; src=&amp;quot;http://www.youtube.com/embed/NQmxuzdc4Zg?autoplay=1&amp;quot; frameborder=&amp;quot;0&amp;quot; allowfullscreen&amp;gt;&amp;lt;/iframe&amp;gt;&#39;;myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;&quot; title=&quot;click here to play&quot;&gt;
&lt;a class=&quot;youtube-lazy-link&quot; style=&quot;width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/NQmxuzdc4Zg/0.jpg) center center no-repeat;background-size:contain;position:absolute&quot; href=&quot;http://www.youtube.com/watch?v=NQmxuzdc4Zg&quot; id=&quot;NQmxuzdc4Zg&quot; onclick=&quot;return false;&quot;&gt;
&lt;div class=&quot;youtube-lazy-link-div&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;youtube-lazy-link-info&quot;&gt;embedded youtube video NQmxuzdc4Zg&lt;/div&gt;
&lt;/a&gt;
&lt;div class=&quot;video-info&quot;&gt;embedded youtube video NQmxuzdc4Zg&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Starting from a simple node.js application, we put together &amp;#8220;just enough&amp;#8221; charm
to get things working.  Watch for future episodes where we&amp;#8217;ll refactor and
refine both the application and the charm.&lt;/p&gt;

&lt;!&#8211;more&#8211;&gt;

&lt;p&gt;As before, there are links to the whole series of charmschool hangouts in the juju
&lt;a href=&quot;https://juju.ubuntu.com/resources/videos/&quot;&gt;video archive&lt;/a&gt;
where we also have videos and screencasts of demos, talks,  and any other charm
schools we&amp;#8217;ve been able to capture on video.&lt;/p&gt;
</content>
   <author>
     <name>Mark Mims</name>
     <uri>http://markmims.com/</uri>
   </author>
 </entry>
 
 <entry>
   <title>CharmSchool Hangouts</title>
   <link href="http://markmims.com/cloud/2013/05/20/charmschool-hangouts.html"/>
   <updated>2013-05-20T12:49:00+00:00</updated>
   <id>http://markmims.com/cloud/2013/05/20/charmschool-hangouts</id>
   <content type="html">&lt;p&gt;We&amp;#8217;re doing a series of regular CharmSchools on G+ hangouts.
Last week we did an intro to juju and charming&amp;#8230;&lt;/p&gt;

&lt;div class=&quot;ratio-4-3 embed-video-container&quot; onclick=&quot;var myAnchor = document.getElementById(&#39;yRcqSjOGweo&#39;);var tmpDiv = document.createElement(&#39;div&#39;);tmpDiv.innerHTML = &#39;&amp;lt;iframe style=&amp;quot;vertical-align:top;width:100%;height:100%;position:absolute;&amp;quot; src=&amp;quot;http://www.youtube.com/embed/yRcqSjOGweo?autoplay=1&amp;quot; frameborder=&amp;quot;0&amp;quot; allowfullscreen&amp;gt;&amp;lt;/iframe&amp;gt;&#39;;myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;&quot; title=&quot;click here to play&quot;&gt;
&lt;a class=&quot;youtube-lazy-link&quot; style=&quot;width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/yRcqSjOGweo/0.jpg) center center no-repeat;background-size:contain;position:absolute&quot; href=&quot;http://www.youtube.com/watch?v=yRcqSjOGweo&quot; id=&quot;yRcqSjOGweo&quot; onclick=&quot;return false;&quot;&gt;
&lt;div class=&quot;youtube-lazy-link-div&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;youtube-lazy-link-info&quot;&gt;embedded youtube video yRcqSjOGweo&lt;/div&gt;
&lt;/a&gt;
&lt;div class=&quot;video-info&quot;&gt;embedded youtube video yRcqSjOGweo&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;There&amp;#8217;re links to the whole series of charmschool hangouts in the juju
&lt;a href=&quot;https://juju.ubuntu.com/resources/videos/&quot;&gt;video archive&lt;/a&gt;
where we also have videos and screencasts of demos, talks,  and any other charm
schools we&amp;#8217;ve been able to capture on video.&lt;/p&gt;

</content>
   <author>
     <name>Mark Mims</name>
     <uri>http://markmims.com/</uri>
   </author>
 </entry>
 
 <entry>
   <title>CharmSchool Video from the Openstack Summit</title>
   <link href="http://markmims.com/cloud/2013/04/25/charmschool-video-from-the-openstack-summit.html"/>
   <updated>2013-04-25T12:40:00+00:00</updated>
   <id>http://markmims.com/cloud/2013/04/25/charmschool-video-from-the-openstack-summit</id>
   <content type="html">&lt;p&gt;Jorge and I gave a charmschool at the ODS summit.
The room was packed with 300+&amp;#8230; and they pretty much stayed the whole time&amp;#8230; whoohoo!&lt;/p&gt;

&lt;p&gt;Watch it 
&lt;a href=&quot;http://www.openstack.org/summit/portland-2013/session-videos/presentation/juju-with-openstack-workshop&quot;&gt;here&lt;/a&gt;
or &lt;/p&gt;

&lt;div class=&quot;ratio-4-3 embed-video-container&quot; onclick=&quot;var myAnchor = document.getElementById(&#39;YenD4oxfEa4&#39;);var tmpDiv = document.createElement(&#39;div&#39;);tmpDiv.innerHTML = &#39;&amp;lt;iframe style=&amp;quot;vertical-align:top;width:100%;height:100%;position:absolute;&amp;quot; src=&amp;quot;http://www.youtube.com/embed/YenD4oxfEa4?autoplay=1&amp;quot; frameborder=&amp;quot;0&amp;quot; allowfullscreen&amp;gt;&amp;lt;/iframe&amp;gt;&#39;;myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;&quot; title=&quot;click here to play&quot;&gt;
&lt;a class=&quot;youtube-lazy-link&quot; style=&quot;width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/YenD4oxfEa4/0.jpg) center center no-repeat;background-size:contain;position:absolute&quot; href=&quot;http://www.youtube.com/watch?v=YenD4oxfEa4&quot; id=&quot;YenD4oxfEa4&quot; onclick=&quot;return false;&quot;&gt;
&lt;div class=&quot;youtube-lazy-link-div&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;youtube-lazy-link-info&quot;&gt;embedded youtube video YenD4oxfEa4&lt;/div&gt;
&lt;/a&gt;
&lt;div class=&quot;video-info&quot;&gt;embedded youtube video YenD4oxfEa4&lt;/div&gt;
&lt;/div&gt;

</content>
   <author>
     <name>Mark Mims</name>
     <uri>http://markmims.com/</uri>
   </author>
 </entry>
 
 <entry>
   <title>Running the LinuxPlumbers Conference Schedule with Juju</title>
   <link href="http://markmims.com/cloud/2012/09/25/linuxplumbers-juju.html"/>
   <updated>2012-09-25T00:00:00+00:00</updated>
   <id>http://markmims.com/cloud/2012/09/25/linuxplumbers-juju</id>
   <content type="html">&lt;p class=&quot;meta&quot;&gt;
Written by Mark Mims and Chris Johnston
&lt;/p&gt;

&lt;p&gt;Hey, so last month we ran scheduling for the 
&lt;a href=&quot;http://linuxplumbersconf.org&quot;&gt;Linux Plumbers Conference&lt;/a&gt;
entirely on juju!&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s a little background on the experience.&lt;/p&gt;

&lt;p&gt;Along the way, we&amp;#8217;ll go into a little more detail about running juju in
production than the particular problem at hand might warrant.  It&amp;#8217;s a basic
stack of services that&amp;#8217;s only alive for 6-months or so&amp;#8230;  but this discussion
applies to bigger longer-running production infrastructures too, so it&amp;#8217;s worth
going over here.&lt;/p&gt;

&lt;!&#8211;more&#8211;&gt;

&lt;h2 id=&quot;the-app&quot;&gt;The App&lt;/h2&gt;

&lt;p&gt;So &lt;a href=&quot;https://launchpad.net/summit&quot;&gt;summit&lt;/a&gt; is this great django app built for
scheduling conferences.  It&amp;#8217;s evolved over time to handle
&lt;a href=&quot;uds.ubuntu.com&quot;&gt;UDS&lt;/a&gt;-level traffic and is currently maintained by a
&lt;a href=&quot;https://launchpad.net/~summit-hackers&quot;&gt;Summit Hackers&lt;/a&gt; team that includes
&lt;a href=&quot;http://www.chrisjohnston.org/&quot;&gt;Chris Johnston&lt;/a&gt;
and &lt;a href=&quot;http://mhall119.com/&quot;&gt;Michael Hall&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Chris contacted me to help him use juju to manage summit for this year&amp;#8217;s
Plumbers conference.  At the time we started this, the 11.10 version of juju
wasn&amp;#8217;t exactly blessed for production environments, but we decided it&amp;#8217;d be a
great opportunity to work things out.&lt;/p&gt;

&lt;h2 id=&quot;the-stack&quot;&gt;The Stack&lt;/h2&gt;

&lt;p&gt;A typical summit stack&amp;#8217;s got postgresql, the django app itself, and a memcached server.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/images/django-stack.png&quot;&gt;
&lt;img src=&quot;/images/django-stack.png&quot; width=&quot;198px&quot; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We additionally talked about putting this all behind some sort of a head like haproxy.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/images/bigger-django-stack.png&quot;&gt;
&lt;img src=&quot;/images/bigger-django-stack.png&quot; width=&quot;334px&quot; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This&amp;#8217;d let the app scale horizontally as well as give us a stable point to
attach an elastic-ip.  We decided to &lt;em&gt;not&lt;/em&gt; do this at the time b/c we could
most likely handle the peak conference load with a single django service-unit
provided we slam select snippets of the site into memcached.&lt;/p&gt;

&lt;p&gt;This turned out to be true load-wise, but it really would&amp;#8217;ve been a whole lot
easier to have a nice constant haproxy node out there to tack the elastic-ip
to.
During development (charm, app, and theme) you want the freedom to destroy
a service and respawn it without having to use external tools to go around and
attach public IP addresses to the right places.  That&amp;#8217;s a pain.  Also, if
there&amp;#8217;s a sensitive part of this infrastructure in production, it wouldn&amp;#8217;t be
postgresql, memcached, or haproxy&amp;#8230; the app itself would be the most likely
point of instability, so it was a mistake to attach the elastic-ip there.&lt;/p&gt;

&lt;h2 id=&quot;the-environment&quot;&gt;The Environment&lt;/h2&gt;

&lt;h3 id=&quot;choice-of-cloud&quot;&gt;choice of cloud&lt;/h3&gt;

&lt;p&gt;We chose to use ec2 to host the summit stack&amp;#8230; mostly a matter of
convenience.  The juju openstack-native provider wasn&amp;#8217;t completed when we spun
up the production environment for linuxplumbers and we didn&amp;#8217;t have access to a
stable private ubuntu cloud running the openstack-ec2-api at the time.
All of this has subsequently landed, so we&amp;#8217;d have more options today.&lt;/p&gt;

&lt;h3 id=&quot;the-charms&quot;&gt;the charms&lt;/h3&gt;

&lt;p&gt;We forked &lt;a href=&quot;https://twitter.com/michaelanelson&quot;&gt;Michael Nelson&lt;/a&gt;&amp;#8217;s excellent
&lt;a href=&quot;lp:~michael.nelson/charms/oneiric/apache-django-wsgi/trunk&quot;&gt;django charm&lt;/a&gt;
to create a
&lt;a href=&quot;https://code.launchpad.net/~mark-mims/charms/oneiric/summit/trunk&quot;&gt;summit-charm&lt;/a&gt;
and freely specialized it for summit.&lt;/p&gt;

&lt;p&gt;Note that we&amp;#8217;re updating this charm for 12.04
&lt;a href=&quot;https://code.launchpad.net/~mark-mims/charms/precise/summit/trunk&quot;&gt;here&lt;/a&gt;, but
this will probably go away in the near future and we&amp;#8217;ll just use a generic
django charm.  It turns out we didn&amp;#8217;t do too much here that won&amp;#8217;t apply to
django apps in general, but more on that another time.&lt;/p&gt;

&lt;p&gt;There was nothing special about our tuning of postgresql or memcached.  We just
used the services provided by the canned charms.  These sort of peripheral
services aren&amp;#8217;t the kind of charms you&amp;#8217;re likely to be making changes to or
tweaking outside of their exposed config parameters.  I know &lt;em&gt;jack&lt;/em&gt; about
memcached, so I&amp;#8217;ll defer to the experts in this regard.  Similarly for
postgresql&amp;#8230; and haproxy if we used it in this stack.&lt;/p&gt;

&lt;p&gt;The summit charm is a little different.  It&amp;#8217;s something we were continuing to
tweak during development.  Perhaps with future more generic django charm
versions, we won&amp;#8217;t need to tweak the charm itself&amp;#8230; just configure it.&lt;/p&gt;

&lt;p&gt;We used a &amp;#8220;local&amp;#8221; repository for &lt;em&gt;all&lt;/em&gt; charms because the charm store hadn&amp;#8217;t
landed when we were setting this up.  Well, now that the charm store is live,
you can just deploy the canned charms straight from the store&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`juju deploy -e summit memcached`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and keep the ones you want to tweak in a local repository&amp;#8230; &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`juju deploy -e summit &#8211;repository ~/charms local:summit`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;all within the same environment.  It works out nicely.&lt;/p&gt;

&lt;h3 id=&quot;control-environment&quot;&gt;control environment&lt;/h3&gt;

&lt;p&gt;We had multiple people to manage the production summit environment.  What&amp;#8217;s the
best way to do that?  It turns out juju supports this pretty well right out of
the box.  There&amp;#8217;s an environment config for the set of ssh public keys to
inject into everything in the environment as it starts up&amp;#8230; you can read more
about that on 
&lt;a href=&quot;http://askubuntu.com/questions/179230/how-can-i-manage-multiple-administrators-with-juju&quot;&gt;askubuntu&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Note that this is only useful to configure at the beginning of the stack.  Once
you&amp;#8217;re up, adding keys is problematic.  I don&amp;#8217;t even recommend trying b/c of
the risk of getting undetermined state for the environment.  i.e., different
nodes with different sets of keys depending on when you changed the keys relative
to what actions you&amp;#8217;ve performed on the environment.  It&amp;#8217;s a problem.&lt;/p&gt;

&lt;p&gt;What I recommend now is actually to use &lt;em&gt;another&lt;/em&gt; juju environment&amp;#8230;  (and no,
we&amp;#8217;re not paid to promote cloud providers by the instance :) I wish! ) a dedicated
&amp;#8220;control&amp;#8221; environment.  You bootstrap it, then set up a juju client that controls
the main production environment.  Then set up a shared tmux session that any of
the admins for the production environment can use:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/images/summit-control.png&quot;&gt;
&lt;img src=&quot;/images/summit-control.png&quot; width=&quot;720px&quot; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Adding/changing the set of
admin keys is then done in a single place.  This technique isn&amp;#8217;t strictly
necessary, but it was certainly worth it here with different admins having
various different levels of familiarity with the tools.  I started it as a
teaching tool, left it up because it was an easy control dashboard, and now
recommend it because it works so well.&lt;/p&gt;

&lt;h3 id=&quot;its-chilly-in-here&quot;&gt;it&amp;#8217;s chilly in here&lt;/h3&gt;

&lt;p&gt;Yeah, so during development you break things.  There were a couple of times
using 11.10 juju that changes to juju core prevented a client from talking to
an existing stack.  Aargh!  This wasn&amp;#8217;t going to fly for production use.&lt;/p&gt;

&lt;p&gt;The juju team has subsequently done a &lt;em&gt;bunch&lt;/em&gt; to prevent this from happening,
but hey we needed production summit working and stable at the time.  The
answer&amp;#8230; freeze the code.&lt;/p&gt;

&lt;p&gt;Juju has an environment config option &lt;code&gt;juju-origin&lt;/code&gt; to specify where to
get the juju installed on all instances in the environment.  I branched juju
core to &lt;code&gt;lp:~mark-mims/juju/running-summit&lt;/code&gt; and just worked straight from there
for the lifetime of the environment (still up atm).  Easy enough.&lt;/p&gt;

&lt;p&gt;Now the tricky part is to make sure that you&amp;#8217;re always using the
&lt;code&gt;lp:~mark-mims/juju/running-summit&lt;/code&gt; version of the juju cli when talking to the
production summit environment.&lt;/p&gt;

&lt;p&gt;I set up&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
export JUJU_BRANCH=$HOME/src/juju/running-summit
export PATH=$JUJU_BRANCH/bin:$PATH
export PYTHONPATH=$JUJU_BRANCH
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which my tmuxinator config sources into every pane in my &lt;code&gt;summit&lt;/code&gt; tmux session.&lt;/p&gt;

&lt;p&gt;This was also done on the &lt;code&gt;summit-control&lt;/code&gt; instance so it&amp;#8217;s easy to make sure
we&amp;#8217;re all using the right version of the juju cli to talk to the production
environment.&lt;/p&gt;

&lt;h3 id=&quot;backups&quot;&gt;backups&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;juju ssh&lt;/code&gt; subcommand to the rescue.  You can do all your standard ssh
tricks&amp;#8230;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;juju ssh postgresql/0 &#39;su postgres pg_dump summit&#39; &amp;gt; summit.dump
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#8230; on a cronjob.  Juju just stays out of the way and just helps out a bit with
the addressing.  Real version pipes through bzip2 and adds timestamps of course.&lt;/p&gt;

&lt;p&gt;Of course snapshots are easy enough too via euca2ools, but the pgsql dumps
themselves turned out to be more useful and easy to get to in case of a
failover.&lt;/p&gt;

&lt;h3 id=&quot;debugging&quot;&gt;debugging&lt;/h3&gt;

&lt;p&gt;The biggest debugging activity during development was cleaning up the app&amp;#8217;s
theming.  The summit charm is configured to get the django app itself from
one &lt;a href=&quot;https://code.launchpad.net/summit&quot;&gt;application branch&lt;/a&gt; and the theme from a separate
&lt;a href=&quot;https://code.launchpad.net/~lpc-organizers/ubuntu-community-webthemes/light-django-plumbers-theme&quot;&gt;theme branch&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So&amp;#8230; ahem&amp;#8230; &amp;#8220;best practice&amp;#8221; for theme development would&amp;#8217;ve been to
develop/tweak the theme locally, then push to the branch.  A simple&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;juju set &#8211;config=summit.yaml summit/0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;would update config for the live instances.&lt;/p&gt;

&lt;p&gt;Well&amp;#8230;  some of the menus from the base template used absolute paths so it was
simpler to cheat a bit early in the process to test it all in-place with actual
dns names.  Had we been doing this the &amp;#8220;right&amp;#8221; way from the beginning we
would&amp;#8217;ve had much more confidence in the stack when practicing recovery and
failover later in the cycle&amp;#8230; we would&amp;#8217;ve been doing it all since day one.&lt;/p&gt;

&lt;p&gt;Another thing we had to do was manually test memcached.  To test out caching
we&amp;#8217;d ssh to the memcached instance, stop the service, run memcached verbosely
in the foreground.  Once we determined everything was working the way we
expected, we&amp;#8217;d kill it and restart the upstart job.&lt;/p&gt;

&lt;p&gt;This is a bug in the memcached charm imo&amp;#8230; the option to temporarily run
verbosely for debugging should totally be a config option for that service.
It&amp;#8217;d then be a simple matter of&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;juju set memcached/0 debug=true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and then&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;juju ssh memcached/0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to watch some logs.  Once we&amp;#8217;re convinced it&amp;#8217;s working the way it should&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;juju set memcached/0 debug=false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;should make it performant again.&lt;/p&gt;

&lt;p&gt;Next time around, we should take more advantage of &lt;code&gt;juju set&lt;/code&gt; config to
update/reconfigure the app as we made changes&amp;#8230; and generally implement a
better set of development practices.&lt;/p&gt;

&lt;h3 id=&quot;monitoring&quot;&gt;monitoring&lt;/h3&gt;

&lt;p&gt;Sorely lacking.  &amp;#8220;What? curl doesn&amp;#8217;t cut it?&amp;#8221;&amp;#8230; um&amp;#8230; no.&lt;/p&gt;

&lt;h3 id=&quot;planning-for-failures&quot;&gt;planning for failures&lt;/h3&gt;

&lt;p&gt;Our notion of failover for this app was just a spare set of cloud credentials
and a tested recovery plan.&lt;/p&gt;

&lt;p&gt;The plan we practiced was&amp;#8230;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;bootstrap a new environment (using spare credentials if necessary)&lt;/li&gt;
  &lt;li&gt;spin up the summit stack&lt;/li&gt;
  &lt;li&gt;ssh to the new &lt;code&gt;postgresql/0&lt;/code&gt; and drop the db  (Note: the postgresql charm
should be extended to accept a config parameter of a storage url, S3 in this
case, to slurp the db backups from)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;restore from offsite backups&amp;#8230; something along the lines of&lt;/p&gt;

    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;cat summit-$timestamp.dump.bz2&lt;/td&gt;
          &lt;td&gt;juju ssh -e failover postgresql/0 &amp;#8216;bunzip2 -c&lt;/td&gt;
          &lt;td&gt;su - postgres pgsql summit&amp;#8217;&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In practice, that took about 10-15minutes to recover once we started acting.
Given the additional delay between notification and action, that could spell an
hour or two of outtage.  That&amp;#8217;s not so great.&lt;/p&gt;

&lt;p&gt;Juju makes other failover scenarios cheaper and easier to implement than they
used to be, so why not put those into place just to be safe?  Perhaps the
additional instance costs for hot-spares wouldn&amp;#8217;t&amp;#8217;ve been necessary for the
entire 6-months of lead-time for scheduling and planning this conference, but
they&amp;#8217;d certainly be worth the spend during the few days of the event itself.
Juju sort of makes it a no-brainer.  We should do more posts on this one
issue&amp;#8230; the game has changed here.&lt;/p&gt;

&lt;h2 id=&quot;lessons-learned&quot;&gt;Lessons Learned&lt;/h2&gt;

&lt;p&gt;What would we do differently next time?  Well, there&amp;#8217;s a list :).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;use the stable ppa&amp;#8230; instead of freezing the code&lt;/li&gt;
  &lt;li&gt;sit the app behind haproxy&lt;/li&gt;
  &lt;li&gt;use s3fs or equivalent subordinate charm to manage backups instead of just
sshing them off the box&lt;/li&gt;
  &lt;li&gt;better monitoring&amp;#8230; we&amp;#8217;ve gotten a great set of monitoring charms
recently&amp;#8230; thanks &lt;a href=&quot;http://fewbar.com/&quot;&gt;Clint&lt;/a&gt;!&lt;/li&gt;
  &lt;li&gt;log aggregation would&amp;#8217;ve been a little bit of overkill for this app, but next
time might warrant it&lt;/li&gt;
  &lt;li&gt;it&amp;#8217;s cheap to add failover with juju&amp;#8230; just do it&lt;/li&gt;
  &lt;li&gt;maybe follow a development process a little more carefully next time around :)&lt;/li&gt;
  &lt;li&gt;we&amp;#8217;ll soon have access to a production-stable private ubuntu cloud for these sorts of apps/projects&lt;/li&gt;
&lt;/ul&gt;

</content>
   <author>
     <name>Mark Mims</name>
     <uri>http://markmims.com/</uri>
   </author>
 </entry>
 
 <entry>
   <title>Scaling a 2000-node Hadoop cluster on EC2/Ubuntu with Juju</title>
   <link href="http://markmims.com/cloud/2012/06/04/juju-at-scale.html"/>
   <updated>2012-06-04T00:00:00+00:00</updated>
   <id>http://markmims.com/cloud/2012/06/04/juju-at-scale</id>
   <content type="html">&lt;p class=&quot;meta&quot;&gt;
Written by Mark Mims and James Page
&lt;/p&gt;

&lt;p&gt;Lately we&amp;#8217;ve been fleshing out our testing frameworks for Juju and Juju Charms.  There&amp;#8217;s
lots of great stuff going on here, so we figured it&amp;#8217;s time to start posting about it.&lt;/p&gt;

&lt;p&gt;First off, the coolest thing we did during last month&amp;#8217;s Ubuntu Developer Summit (UDS)
was get the go-ahead to spend more time/effort/money scale-testing Juju.&lt;/p&gt;

&lt;!&#8211;more&#8211;&gt;

&lt;h2 id=&quot;the-plan&quot;&gt;The Plan&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;pick a service that scales&lt;/li&gt;
  &lt;li&gt;spin up a cluster of units for this service&lt;/li&gt;
  &lt;li&gt;try to run it in a way that actively engages all units of the cluster&lt;/li&gt;
  &lt;li&gt;repeat:
    &lt;ul&gt;
      &lt;li&gt;instrument&lt;/li&gt;
      &lt;li&gt;profile&lt;/li&gt;
      &lt;li&gt;optimize&lt;/li&gt;
      &lt;li&gt;grow&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://plus.google.com/u/0/109495998940814132432/posts&quot;&gt;James&lt;/a&gt;,
&lt;a href=&quot;https://plus.google.com/u/0/108276830347560657704/posts&quot;&gt;Kapil&lt;/a&gt;,
&lt;a href=&quot;https://plus.google.com/102506066601287922723/posts&quot;&gt;Juan&lt;/a&gt;,
&lt;a href=&quot;https://plus.google.com/u/0/100536568598074282388/posts&quot;&gt;Ben&lt;/a&gt;,
and &lt;a href=&quot;http://markmims.com/about&quot;&gt;Mark&lt;/a&gt;
sat down over the course of
a couple of nights at UDS to take a crack at it.
We chose Hadoop.
We started with 40 nodes and iterated up 100, 500, 1000 and 2000.
Here&amp;#8217;re some notes on the process.&lt;/p&gt;

&lt;h2 id=&quot;hadoop&quot;&gt;Hadoop&lt;/h2&gt;

&lt;p&gt;Hadoop was a pretty obvious choice here.
It&amp;#8217;s a great actively-maintained
&lt;a href=&quot;http://hadoop.apache.org/&quot;&gt;project&lt;/a&gt;
with a large community of users.
It scales in a somewhat known manner, and the
&lt;a href=&quot;http://jujucharms.com/charms/precise/hadoop&quot;&gt;hadoop charm&lt;/a&gt;
makes it super-simple to manage.
There are also several known benchmarks that are pretty straightforward to get going,
and distribute load throughout the cluster.&lt;/p&gt;

&lt;p&gt;There&amp;#8217;s an entire science/art to tuning hadoop jobs to run optimally given the 
characteristics of a particular cluster.  Our sole goal in tuning hadoop benchmarks
was to &lt;em&gt;engage&lt;/em&gt; the entire cluster and profile juju during various activities throughout
an actual run.  For our purposes, we&amp;#8217;re in no hurry&amp;#8230; a slower/longer run gives us a
good profiling picture for managing the nodes themselves under load (with a sufficient
mix of i/o -vs- cpu load).&lt;/p&gt;

&lt;h2 id=&quot;ec2&quot;&gt;EC2&lt;/h2&gt;

&lt;p&gt;Surprisingly enough, we don&amp;#8217;t really have that many servers just lying around&amp;#8230; so EC2 to the rescue.&lt;/p&gt;

&lt;p&gt;Disclaimer&amp;#8230; we&amp;#8217;re testing our infrastructure tools here, not benchmarking hadoop in EC2.
Some folks advocate running hadoop in a cloudy virtualized environment&amp;#8230; while some
folks are die-hard server huggers.  That&amp;#8217;s actually a really interesting discussion.
It comes down to the actual jobs/problems you&amp;#8217;re
trying to solve and how those jobs fit in your data pipeline.
Please note that we&amp;#8217;re not
trying to solve that problem here or even provide realistic benchmarking data to contribute
to the discussion&amp;#8230; we&amp;#8217;re simply testing how our infrastructure tools perform at scale.&lt;/p&gt;

&lt;p&gt;If you &lt;em&gt;do&lt;/em&gt; run hadoop in EC2, Amazon&amp;#8217;s Elastic Map Reduce service is likely to perform
better at scale in EC2 than just running hadoop itself on general purpose instances.
Amazon can do all sorts of stuff internally to show hadoop lots of love.
We chose not to use EMR because we&amp;#8217;re interested in testing how juju performs
with &lt;em&gt;generic&lt;/em&gt; Ubuntu Server images, not EMR&amp;#8230; at least for now.&lt;/p&gt;

&lt;p&gt;Note that stock EC2 accounts limit you to something like 20 instances.  To grow beyond that, you have to
ask AWS to bump up your limits.&lt;/p&gt;

&lt;h2 id=&quot;juju&quot;&gt;Juju&lt;/h2&gt;

&lt;p&gt;We started scale testing from a fresh branch of juju trunk&amp;#8230; what gets deployed to
the PPA nightly&amp;#8230; this freed us up to experiment with live changes to add instrumentation,
profiling information, and randomly mess with code as necessary.  This also locks in 
the branch of juju that the scale testing environment uses.&lt;/p&gt;

&lt;p&gt;As usual, juju will keep track of the state of our infrastructure going forward and
we can make changes as necessary via juju commands.  To bootstrap and spin up the
initial environment we&amp;#8217;ll just use shell scripts wrapping juju commands.&lt;/p&gt;

&lt;h3 id=&quot;spinning-up-a-cluster&quot;&gt;Spinning up a cluster&lt;/h3&gt;

&lt;p&gt;These scripts are really just
hadoop versions of some standard juju demo scripts such as those used for 
a simple &lt;a href=&quot;https://gist.github.com/2050525&quot;&gt;rails stack&lt;/a&gt;
or a more realistic HA &lt;a href=&quot;https://gist.github.com/1406018&quot;&gt;wiki stack&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The hadoop scripts for EC2 will get a little more complex as we grow simply because
we don&amp;#8217;t want AWS to think we&amp;#8217;re a DoS attack&amp;#8230; we&amp;#8217;ll pace ourselves during spinup.&lt;/p&gt;

&lt;p&gt;From the hadoop charm&amp;#8217;s readme, the basic steps to spinning up a simple combined
hdfs and mapreduce cluster are:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;juju bootstrap

juju deploy hadoop hadoop-master
juju deploy -n3 hadoop hadoop-slavecluster

juju add-relation hadoop-master:namenode hadoop-slavecluster:datanode
juju add-relation hadoop-master:jobtracker hadoop-slavecluster:tasktracker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which we expand on a bit to start with a base startup script that looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash

juju_root=&quot;/home/ubuntu/scale&quot;
juju_env=${1:-&quot;-escale&quot;}

###

echo &quot;deploying stack&quot;

juju bootstrap $juju_env

deploy_cluster() {
  local cluster_name=$1

  juju deploy $juju_env &#8211;repository &quot;$juju_root/charms&quot; &#8211;constraints=&quot;instance-type=m1.large&quot; &#8211;config &quot;$juju_root/etc/hadoop-master.yaml&quot; local:hadoop ${cluster_name}-master

  juju deploy $juju_env &#8211;repository &quot;$juju_root/charms&quot; &#8211;constraints=&quot;instance-type=m1.medium&quot; &#8211;config &quot;$juju_root/etc/hadoop-slave.yaml&quot; -n 37 local:hadoop ${cluster_name}-slave

  juju add-relation $juju_env ${cluster_name}-master:namenode ${cluster_name}-slave:datanode
  juju add-relation $juju_env ${cluster_name}-master:jobtracker ${cluster_name}-slave:tasktracker

  juju expose $juju_env ${cluster_name}-master

}

deploy_cluster hadoop

echo &quot;done&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and then manually adjust this for cluster size.&lt;/p&gt;

&lt;h3 id=&quot;configuring-hadoop&quot;&gt;Configuring Hadoop&lt;/h3&gt;

&lt;p&gt;Note that we&amp;#8217;re specifying constraints to tell juju to use different sized ec2 instances for
different juju services.  We&amp;#8217;d like an m1.large for the
hadoop master&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;juju deploy &#8230; &#8211;constraints &quot;instance-type=m1.large&quot; &#8230; hadoop-master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and m1.mediums for the slaves&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;juju deploy &#8230; &#8211;constraints &quot;instance-type=m1.medium&quot; &#8230; hadoop-slave
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that we&amp;#8217;ll also pass config files to specify different heap sizes for the different memory footprints&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;juju deploy &#8230; &#8211;config &quot;hadoop-master.yaml&quot; &#8230; hadoop-master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;hadoop-master.yaml&lt;/code&gt; looks like&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# m1.large
hadoop-master:
  heap: 2048
  dfs.block.size: 134217728
  dfs.namenode.handler.count: 20
  mapred.reduce.parallel.copies: 50
  mapred.child.java.opts: -Xmx512m
  mapred.job.tracker.handler.count: 60
#  fs.inmemory.size.mb: 200
  io.sort.factor: 100
  io.sort.mb: 200
  io.file.buffer.size: 131072
  tasktracker.http.threads: 50
  hadoop.dir.base: /mnt/hadoop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;juju deploy &#8230; &#8211;config &quot;hadoop-slave.yaml&quot; &#8230; hadoop-slave
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;hadoop-slave.yaml&lt;/code&gt; looks like&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# m1.medium
hadoop-slave:
  heap: 1024
  dfs.block.size: 134217728
  dfs.namenode.handler.count: 20
  mapred.reduce.parallel.copies: 50
  mapred.child.java.opts: -Xmx512m
  mapred.job.tracker.handler.count: 60
#  fs.inmemory.size.mb: 200
  io.sort.factor: 100
  io.sort.mb: 200
  io.file.buffer.size: 131072
  tasktracker.http.threads: 50
  hadoop.dir.base: /mnt/hadoop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note also that we also have our juju environment configured to use
instance-store images&amp;#8230; juju defaults to ebs-rooted images, but that&amp;#8217;s
not a great idea with hdfs.  You specify this by adding a &lt;code&gt;default-image-id&lt;/code&gt;
into your &lt;code&gt;~/.juju/environments.yaml&lt;/code&gt; file.
This gave each of our instances an extra ~400G local drive
on &lt;code&gt;/mnt&lt;/code&gt;&amp;#8230; hence the &lt;code&gt;hadoop.dir.base&lt;/code&gt; of &lt;code&gt;/mnt/hadoop&lt;/code&gt;
in the config above.&lt;/p&gt;

&lt;h2 id=&quot;nodes-and-100-nodes&quot;&gt;40 nodes and 100 nodes&lt;/h2&gt;

&lt;p&gt;Both the 40-node and 100-node runs went as smooth as silk.
The only thing to note was that it took a while to get AWS to increase
our account limits to allow for 100+ nodes.&lt;/p&gt;

&lt;h2 id=&quot;nodes&quot;&gt;500 nodes&lt;/h2&gt;

&lt;p&gt;Once we had permission from Amazon to spin up 500 nodes on our account,
we initially just naively spun
up 500 instances&amp;#8230; and quickly got throttled.&lt;/p&gt;

&lt;p&gt;No particular surprise, we&amp;#8217;re not specifying multiplicity in the ec2 api,
nor are we using an auto scaling group&amp;#8230; we must look like a DoS attack.&lt;/p&gt;

&lt;p&gt;The order was eventually fulfilled, and juju waited around for it.
Everything ran as expected, it just took about an hour and 15 minutes
to spin up the stack.  This gave us a nice little cluster with HDFS
storage of almost 200TB&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/images/scale-500-50070.png&quot;&gt;
&lt;img src=&quot;/images/scale-500-50070.png&quot; width=&quot;720px&quot; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The hadoop terasort job was run from the following script&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash

SIZE=10000000000
NUM_MAPS=1500
NUM_REDUCES=1500
IN_DIR=in_dir
OUT_DIR=out_dir

hadoop jar /usr/lib/hadoop/hadoop-examples*.jar teragen -Dmapred.map.tasks=${NUM_MAPS} ${SIZE} ${IN_DIR}

sleep 10

hadoop jar /usr/lib/hadoop/hadoop-examples*.jar terasort -Dmapred.reduce.tasks=${NUM_REDUCES} ${IN_DIR} ${OUT_DIR}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which, with a replfactor of 3, engaged the entire cluster just fine, 
and ran terasort with no problems&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/images/scale-500-50030.png&quot;&gt;
&lt;img src=&quot;/images/scale-500-50030.png&quot; width=&quot;720px&quot; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Juju itself seemed to work great in this run, but this brought up a couple of basic optimizations against the EC2 api:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- pass the &#39;-n&#39; options directly to the provisioning agent&#8230; don&#39;t expand `juju deploy -n &amp;lt;num_units&amp;gt;` and `juju add-unit -n &amp;lt;num_units&amp;gt;` in the client
- pass these along all the way to the ec2 api&#8230; don&#39;t expand these into multiple api calls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;#8217;ll add those to the list of things to do.&lt;/p&gt;

&lt;h2 id=&quot;nodes-1&quot;&gt;1000 nodes&lt;/h2&gt;

&lt;p&gt;Onward, upward!&lt;/p&gt;

&lt;p&gt;To get around the api throttling, we start up
batches of 99 slaves at a time with a 2-minute wait
between each batch&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash

juju_env=${1:-&quot;-escale&quot;}
juju_root=&quot;/home/ubuntu/scale&quot;
juju_repo=&quot;$juju_root/charms&quot;

############################################

timestamp() {
  date +&quot;%G-%m-%d-%H%M%S&quot;
}

add_more_units() {
  local num_units=$1
  local service_name=$2

  echo &quot;sleeping&quot;
  sleep 120

  echo &quot;adding another $num_units units at $(timestamp)&quot;
  juju add-unit $juju_env -n $num_units $service_name
}

deploy_slaves() {
  local cluster_name=$1
  local slave_config=&quot;$juju_root/etc/hadoop-slave.yaml&quot;
  local slave_size=&quot;instance-type=m1.medium&quot;
  local slaves_at_a_time=99
  #local num_slave_batches=10

  juju deploy $juju_env &#8211;repository $juju_repo &#8211;constraints $slave_size &#8211;config $slave_config -n $slaves_at_a_time local:hadoop ${cluster_name}-slave
  echo &quot;deployed $slaves_at_a_time slaves&quot;

  juju add-relation $juju_env ${cluster_name}-master:namenode ${cluster_name}-slave:datanode
  juju add-relation $juju_env ${cluster_name}-master:jobtracker ${cluster_name}-slave:tasktracker

  for i in {1..9}; do
    add_more_units $slaves_at_a_time ${cluster_name}-slave
    echo &quot;deployed $slaves_at_a_time slaves at $(timestamp)&quot;
  done
}

deploy_cluster() {
  local cluster_name=$1
  local master_config=&quot;$juju_root/etc/hadoop-master.yaml&quot;
  local master_size=&quot;instance-type=m1.large&quot;

  juju deploy $juju_env &#8211;repository $juju_repo &#8211;constraints $master_size &#8211;config $master_config local:hadoop ${cluster_name}-master

  deploy_slaves ${cluster_name}

  juju expose $juju_env ${cluster_name}-master
}

main() {
  echo &quot;deploying stack at $(timestamp)&quot;

  juju bootstrap $juju_env &#8211;constraints=&quot;instance-type=m1.xlarge&quot;

  sleep 120
  deploy_cluster hadoop

  echo &quot;done at $(timestamp)&quot;
}
main $*
exit 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We experimented with more clever ways of doing the spinup
(too little coffee at this point of the night)&amp;#8230;
but the real fix is to get juju to take
advantage of multiplicity in api calls.
Until then, timed batches work just fine.&lt;/p&gt;

&lt;p&gt;Juju spun the cluster up in about 2 and a half hours.
It had about 380TB of HDFS storage&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/images/scale-1000-50070.png&quot;&gt;
&lt;img src=&quot;/images/scale-1000-50070.png&quot; width=&quot;720px&quot; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The terasort job that was run from the script above with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SIZE=10000000000
NUM_MAPS=3000
NUM_REDUCES=3000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&quot;/images/scale-1000-50030.png&quot;&gt;
&lt;img src=&quot;/images/scale-1000-50030.png&quot; width=&quot;720px&quot; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;eventually completed.&lt;/p&gt;

&lt;h2 id=&quot;nodes-2&quot;&gt;2000 nodes&lt;/h2&gt;

&lt;p&gt;After the 1000-node run, we chose to clean up from the
previous job and just add more nodes to that same cluster.&lt;/p&gt;

&lt;p&gt;Again, to get around the api throttling, we added
batches of 99 slaves at a time with a 2-minute wait
between each batch until we got near 2000 slaves.&lt;/p&gt;

&lt;p&gt;This gave us almost 760TB of HDFS storage&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/images/scale-2000-50070.png&quot;&gt;
&lt;img src=&quot;/images/scale-2000-50070.png&quot; width=&quot;720px&quot; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;and was running fine&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/images/scale-2000-50030.png&quot;&gt;
&lt;img src=&quot;/images/scale-2000-50030.png&quot; width=&quot;720px&quot; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;but was stopped early b/c waiting for the job to complete
would&amp;#8217;ve just been silly at this point.  With our naive job
config, we&amp;#8217;re considerably past the point of diminishing
returns for adding nodes to the actual terasort, and we&amp;#8217;d
captured the profiling info we needed at this point.&lt;/p&gt;

&lt;p&gt;Juju spun up 1972 slaves in just over seven hours total.
Profiling showed that juju was spending a &lt;em&gt;lot&lt;/em&gt; of time
serializing stuff into zookeeper nodes using yaml.  It
looks like python&amp;#8217;s yaml implementation is python, and
not just wrapping libyaml.  We tested a smaller run replacing
the internal yaml serialization with json.. 
Wham!  two orders of magnitude faster.  No particular surprise.&lt;/p&gt;

&lt;h2 id=&quot;lessons-learned&quot;&gt;Lessons Learned&lt;/h2&gt;

&lt;p&gt;Ok, so at the end of the day, what did we learn here?&lt;/p&gt;

&lt;p&gt;What we did here is the way developing for performance at scale
should be done&amp;#8230; start with a naive, flexible approach
and then spend time and effort obtaining real profiling
information.  Follow that with optimization decisions that actually
make a difference.  Otherwise it&amp;#8217;s all just a crapshoot
based on where developers think the bottlenecks might be.&lt;/p&gt;

&lt;p&gt;Things to do to juju as a result of these tests:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;streamline our implementation of &amp;#8216;-n&amp;#8217; options
    &lt;ul&gt;
      &lt;li&gt;the client should pass the multiplicity to the provisioning agent&lt;/li&gt;
      &lt;li&gt;the provisioning agent should pass the multiplicity to the EC2 api&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;don&amp;#8217;t use yaml to marshall data in and out of zookeeper&lt;/li&gt;
  &lt;li&gt;replace per-instance security groups with per-instance firewalls&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What&amp;#8217;s Next?&lt;/h2&gt;

&lt;p&gt;So that&amp;#8217;s a big enough bite for one round of scale testing.&lt;/p&gt;

&lt;p&gt;Next up:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;land a few of the changes outlined above into trunk.
Then, spin up another round of scale tests to look at the numbers.&lt;/li&gt;
  &lt;li&gt;more providers (other clouds as well as a MaaS lab too)&lt;/li&gt;
  &lt;li&gt;regular scale testing?
    &lt;ul&gt;
      &lt;li&gt;can this coincide with upstream scale testing for projects like hadoop?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;test scaling for various services?  What does this look like for other stacks
of services?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;wishlist&quot;&gt;Wishlist&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;find some better test jobs!  benchmarks are boring&amp;#8230; perhaps we can use
this compute time to mine educational data or cure cancer or something?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;perhaps push juju topology information further into zk leaf nodes?
Are there transactional features in more recent versions of zk that we can use?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;use spot instances on ec2.  This is harder because you&amp;#8217;ve gotta incorporate price monitoring.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
   <author>
     <name>Mark Mims</name>
     <uri>http://markmims.com/</uri>
   </author>
 </entry>
 
</feed>
