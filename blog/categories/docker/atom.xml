<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: docker | Software, Physics, Data, Mountains]]></title>
  <link href="http://markmims.com/blog/categories/docker/atom.xml" rel="self"/>
  <link href="http://markmims.com/"/>
  <updated>2018-07-10T10:47:29-06:00</updated>
  <id>http://markmims.com/</id>
  <author>
    <name><![CDATA[Mark M Mims, Ph.D.]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Develop Spark Apps on Yarn using Docker]]></title>
    <link href="http://markmims.com/docker/data-engineering/spark/devops/2015/10/13/docker-cdh.html"/>
    <updated>2015-10-13T15:07:00-06:00</updated>
    <id>http://markmims.com/docker/data-engineering/spark/devops/2015/10/13/docker-cdh</id>
    <content type="html"><![CDATA[<p>At svds, we’ll often run spark on yarn in production.  Add some artful tuning
and this works pretty well.  However, developers typically build and test spark
application in <em>standalone</em> mode… not on yarn.</p>

<p>Rather than get bitten by the ideosyncracies involved in running spark on yarn
-vs- standalone when you go to deploy, here’s a way to set up a development
environment for spark that more closely mimics how it’s used in the wild.</p>

<!--more-->

<h2 id="a-simple-yarn-cluster-on-your-laptop">A simple yarn “cluster” on your laptop</h2>

<p>Run a docker image for a cdh standalone instance</p>

<pre><code>docker run -d --name=mycdh svds/cdh
</code></pre>

<p>when the logs</p>

<pre><code>docker logs -f mycdh
</code></pre>

<p>stop going wild, you can run the usual hadoop-isms to set up a workspace</p>

<pre><code>docker exec -it mycdh hadoop fs -ls /
docker exec -it mycdh hadoop fs -mkdir -p /tmp/blah
</code></pre>

<h2 id="run-spark">Run spark</h2>

<p>Then, it’s pretty straightforward to run spark against yarn</p>

<pre><code>docker exec -it mycdh \
  spark-submit \
    --master yarn-cluster \
    --class org.apache.spark.examples.SparkPi \
    /usr/lib/spark/examples/lib/spark-examples-1.3.0-cdh5.4.3-hadoop2.6.0-cdh5.4.3.jar \
    1000
</code></pre>

<p>Note that you can <em>submit</em> a spark job to run in either “yarn-client” or “yarn-cluster” modes.</p>

<p>In “yarn-client” mode, the spark driver runs outside of yarn and logs to
console and all spark executors run as yarn containers.</p>

<p>In “yarn-cluster” mode, all spark executors run as yarn containers, but then
the spark driver also runs as a yarn container.  Yarn manages all the logs.</p>

<p>You can also run the spark shell so that any workers spawned run in yarn</p>

<pre><code>docker exec -it mycdh spark-shell --master yarn-client
</code></pre>

<p>or</p>

<pre><code>docker exec -it mycdh pyspark --master yarn-client
</code></pre>

<h2 id="your-application">Your Application</h2>

<p>Ok, so <code>SparkPi</code> is all fine and dandy, but how do I run a real application?</p>

<p>Let’s make up an example.  Say you build your spark project on your laptop in the
<code>/Users/myname/mysparkproject/</code> directory.</p>

<p>When you build with maven or sbt, it typically builds and leaves jars under a
<code>/Users/myname/mysparkproject/target/</code> directory… for sbt, it’ll look like
<code>/Users/myname/mysparkproject/target/scala-2.10/</code>.</p>

<p>The idea here is to make these jars directly accessible from both your laptop’s
build process as well as from inside the cdh container.</p>

<p>When you start up the <code>cdh</code> container, map this local host directory up and
into the container</p>

<pre><code>docker run -d -v ~/mysparkproject/target:/target --name=mycdh svds/cdh 
</code></pre>

<p>where the <code>-v</code> option will make <code>~/mysparkproject/target</code>
available as <code>/target</code> within the container.</p>

<p>So,</p>

<pre><code>sbt clean assembly
</code></pre>

<p>leaves a jar under <code>~/mysparkproject/target</code>, which the container sees as
<code>/target</code> and you can run jobs using something like</p>

<pre><code>docker exec -it mycdh \
  spark-submit \
    --master yarn-cluster \
    --name MyFancySparkJob-name \
    --class org.markmims.MyFancySparkJob \
    /target/scala-2.10/My-assembly-1.0.1.20151013T155727Z.c3c961a51c.jar \
    myarg
</code></pre>

<p>The <code>--name</code> arg makes it easier to find in the midst of multiple yarn jobs.</p>

<h2 id="logs">Logs</h2>

<p>While a spark job is running, you can get its yarn “applicationId” from</p>

<pre><code>docker exec -it mycdh yarn application -list
</code></pre>

<p>or if it finished already just list things out with more conditions</p>

<pre><code>docker exec -it mycdh yarn application -list -appStates FINISHED
</code></pre>

<p>You can dig through the yarn-consolidated logs after the job is done
by using</p>

<pre><code>docker exec -it mycdh yarn logs -applicationId &lt;applicationId&gt;
</code></pre>

<h2 id="consoles">Consoles</h2>

<p>Web consoles are critical for application development.  Spend time up front
getting ports open or forwarded correctly for all environments.  Don’t wait
until you’re actually trying to debug something critical to figure out how to
forward ports to see the staging UI in all environments.</p>

<h3 id="yarn-resourcemanager-ui">Yarn ResourceManager UI</h3>

<p>Yarn gives you quite a bit of info about the system right from the
ResourceManager on its ip address and webgui port (usually 8088)</p>

<pre><code>open http://&lt;resource-manager-ip&gt;:&lt;resource-manager-port&gt;/
</code></pre>

<h3 id="spark-staging-ui">Spark Staging UI</h3>

<p>Yarn also conveniently proxies access to the spark staging UI for a given
application.  This looks like</p>

<pre><code>open http://&lt;resource-manager-ip&gt;:&lt;resource-manager-port&gt;/proxy/&lt;applicationId&gt;
</code></pre>

<p>for example,</p>

<pre><code>open http://localhost:8088/proxy/application_1444330488724_0005/
</code></pre>

<h3 id="ports-and-docker">Ports and Docker</h3>

<p>There are a few ways to deal with accessing port <code>8088</code> of the yarn resource
manager from outside of the docker container.  I typically use ssh for
everything and just forward ports out to <code>localhost</code> on the host.  However,
most people will expect to access ports directly on the <code>docker-machine ip</code>
address.  To do that, you have to map each port when you first spin up the
<code>cdh</code> container using the <code>-p 8088</code> option</p>

<pre><code>docker run -d -v target -p 8088 --name=mycdh svds/cdh 
</code></pre>

<p>Then you should be good to go with something like</p>

<pre><code>open http://`docker-machine ip`:8088/
</code></pre>

<p>to access the yarn console.</p>

<hr />

<h2 id="tips-and-gotchas">Tips and Gotchas</h2>

<ul>
  <li>
    <p>The docker image <code>svds/cdh</code> is quite large (2GB).  I like to do a separate
<code>docker pull</code> from any <code>docker run</code> commands just to isolate the download.
In fact, I recommend pinning the cdh version for the same reason… so
<code>docker pull svds/cdh:5.4.0</code> for instance, then refer to it that way
throughout <code>docker run -d --name=mycdh svds/cdh:5.4.0</code> and that’ll insure
you’re not littering your laptop’s filesystem with docker layers from
multiple cdh versions.  The bare <code>svds/cdh</code> (equiv to <code>svds/cdh:latest</code>)
floats with the most recent cloudera versions</p>
  </li>
  <li>
    <p>I’m using a CDH container here… but there’s an HDP one on the way as well.
Keep an eye out for it on <a href="`https://hub.docker.com/u/svds`">svds’s dockerhub page</a></p>
  </li>
  <li>
    <p>web consoles and forwarding ports through SSH</p>
  </li>
</ul>

<h2 id="bonus">Bonus</h2>

<p>Ok, so the downside here is that the image is fat.  The upside is that it lets
you play with the full suite of CDH-based tools.  I’ve tested out (besides the
spark variations above)</p>

<h3 id="impala-shell">Impala shell</h3>

<pre><code>docker exec mycdh impala-shell
</code></pre>

<h3 id="hbase-shell">HBase shell</h3>

<pre><code>docker exec mycdh hbase shell
</code></pre>

<h3 id="hive">Hive</h3>

<pre><code>echo "show tables;" | docker exec mycdh beeline -u jdbc:hive2://localhost:10000 -n username -p password -d org.apache.hive.jdbc.HiveDriver
</code></pre>

]]></content>
  </entry>
  
</feed>
