<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: devops | Software, Physics, Data, Mountains]]></title>
  <link href="http://markmims.com/blog/categories/devops/atom.xml" rel="self"/>
  <link href="http://markmims.com/"/>
  <updated>2020-06-16T09:53:39+00:00</updated>
  <id>http://markmims.com/</id>
  <author>
    <name><![CDATA[Mark M Mims, Ph.D.]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Projects in GCP using Central Billing Accounts]]></title>
    <link href="http://markmims.com/devops/cloud/gcp/iam/2019/06/05/managed-projects-in-gcp.html"/>
    <updated>2019-06-05T00:00:00+00:00</updated>
    <id>http://markmims.com/devops/cloud/gcp/iam/2019/06/05/managed-projects-in-gcp</id>
    <content type="html"><![CDATA[<p>Many organizations recognize the benefits of empowering their developers. In a
cloud environment, that often means giving developers the ability to create and
manage their own infrastructure.</p>

<p>Of course, developers can easily create their own individual or G-Suite GCP
accounts.  They can take advantage of the free trial that Google Cloud offers.
That&#8217;s great, and everything&#8217;s hunky-dory until the credit runs out. What then?</p>

<p>In this post I describe a really simple way to set up and use centralized
billing on GCP&#8230; even across external development accounts.  Way better than
trying to get me to fill out expense reports for infradev!</p>

<!--more-->

<h2 id="contents">Contents</h2>

<ul>
  <li>Organizations and account setup</li>
  <li>Users and IAM roles</li>
  <li>Terraform templates</li>
  <li>Try it out</li>
</ul>

<h2 id="organizations-and-account-setup">Organizations and account setup</h2>

<p>Let&#8217;s consider a common example with two separate organizations in the mix.</p>

<ol>
  <li>
    <p>A <code>bigcorp.com</code> organization that&#8217;s footing the bill for everything</p>
  </li>
  <li>
    <p>An individual developer&#8217;s G-Suite organization, <code>pinkponies.io</code>, where
we&#8217;ll be doing the development</p>
  </li>
</ol>

<p>In this example, we&#8217;re assuming the developer organization <code>pinkponies.io</code> is a
full G-Suite account and not just an ordinary GCP account created using a
single email.</p>

<p>It&#8217;s easy for an individual developer to create a new G-Suite account and that
turns out to be the more typical situation for this kind of cross billing
example. I also really recommend using developer G-Suite accounts for cloud
development in general since they&#8217;ll have the same IAM capabilities and
concerns as the <code>bigcorp.com</code> account.</p>

<h2 id="users-and-iam-roles">Users and IAM roles</h2>

<p>Each developer will need accounts in both orgs to start with.</p>

<p>Take Sam for example. Sam&#8217;s already an Owner of <code>pinkponies.io</code>&#8230;  with
<code>sam@pinkponies.io</code> as a login.</p>

<p>Sam works for BigCorp and is also <code>sam@bigcorp.com</code> where they live in some
folder within the <code>bigcorp.com</code> organization&#8217;s GCP IAM.</p>

<h3 id="in-your-billing-org-bigcorpcom">In your billing org: <code>bigcorp.com</code></h3>

<p>So the <code>billing_account_user</code> (<code>sam@bigcorp.com</code>) needs to be able to create
billing accounts within the BigCorp org.</p>

<p>Sam will need to be assigned a <code>BillingAccountCreator</code> role within the
<code>bigcorp.com</code> org&#8217;s IAM on GCP.</p>

<h3 id="in-your-gsuite-org-pinkponiesio">In your gsuite org: <code>pinkponies.io</code></h3>

<p>It&#8217;s no surprise, the <code>gsuite_user</code> (<code>sam@pinkponies.io</code>) needs to be an
<code>OrganizationAdministrator</code> on that org.</p>

<p>The <code>billing_account_user</code> (<code>sam@bigcorp.com</code>) needs permissions on the
<code>pinkponies.io</code> org too. They need to be:</p>

<ul>
  <li>a <code>BillingAccountAdministrator</code> for the <code>pinkponies.io</code> org</li>
  <li>a <code>ProjectCreator</code> on the <code>pinkponies.io</code> org</li>
  <li>and I added them as an <code>OrganizationAdministrator</code> on <code>pinkponies.org</code> for
good measure</li>
</ul>

<h2 id="terraform-templates">Terraform templates</h2>

<p>I like to manage infrastructure using <a href="terraform.io">Terraform</a> and keep
all my templates and modules checked into GitHub.</p>

<p>The Terraform templates to create these projects are super simple. There&#8217;s a
provider, a resource for the managed project we want to create, and then a
couple of role binding resources</p>

<p>```
provider &#8220;google&#8221; {
  region      = &#8220;${var.region}&#8221;
}</p>

<p>resource &#8220;google_project&#8221; &#8220;gsuite_project&#8221; {
  name       = &#8220;gsuite-project-0&#8221;
  project_id = &#8220;gsuite-project-0&#8221;</p>

<p>org_id = &#8220;${var.gsuite_org_id}&#8221;
  billing_account = &#8220;${var.billing_account_id}&#8221;
}</p>

<p>resource &#8220;google_project_iam_binding&#8221; &#8220;gsuite_project_owner&#8221; {
  project = &#8220;${google_project.gsuite_project.project_id}&#8221;
  role    = &#8220;roles/owner&#8221;</p>

<p>members = [
    &#8220;user:${var.gsuite_user}&#8221;,
    &#8220;user:${var.billing_account_user}&#8221;,
  ]
}
```</p>

<p>There&#8217;s no need to get Terraform to slurp in data sources for the GCP orgs,
folders, billing accounts, etc. In this example, we&#8217;ll just create variables
for them</p>

<p>```
variable &#8220;region&#8221; {
  default = &#8220;us-central1&#8221;
}</p>

<p>variable &#8220;billing_account_user&#8221; {}
variable &#8220;billing_folder_id&#8221; {}
variable &#8220;billing_account_id&#8221; {}</p>

<p>variable &#8220;gsuite_user&#8221; {}
variable &#8220;gsuite_org_id&#8221; {}
```</p>

<p>and look up the values from the cloud consoles for both our <code>bigcorp.com</code> and
<code>pinkponies.io</code> accounts.  We&#8217;ll add these to <code>terraform.tfvars</code></p>

<p>```
billing_account_user = &#8220;sam@bigcorp.com&#8221;
billing_folder_id = &#8220;234567890123&#8221; # my-billing-folder
billing_account_id = &#8220;aaaaaa-bbbbbb-cccccc&#8221; # my-billing-account</p>

<p>gsuite_user = &#8220;sam@pinkponies.io&#8221;
gsuite_org_id = &#8220;345678901234&#8221; # pinkponies.io
```</p>

<p>Note that there&#8217;s a <code>terraform.tfvars.template</code> included in the example repo
but the actual <code>*.tfvars</code> files, with sensitive account details, are ignored by
revision control so you&#8217;ll have to copy the template and create your own
<code>terraform.tfvars</code>.</p>

<h2 id="try-it-out">Try it out</h2>

<h3 id="example-repo">Example repo</h3>

<p>You can clone and configure the example templates</p>

<ul>
  <li>clone <a href="https://github.com/mmm/gcp-managed-projects">https://github.com/mmm/gcp-managed-projects</a></li>
  <li>copy the tfvars template over to <code>terraform.tfvars</code> and edit it with your info</li>
</ul>

<h3 id="gcloud"><code>gcloud</code></h3>

<p>Terraform&#8217;s provider for GCP needs GCP credentials for your account.  The
easiest thing to do to get that working before trying to run Terraform is to
make sure gcloud is working correctly.</p>

<p>You can do that by installing gcloud and running <code>gcloud init</code> to go through
the oauth dance&#8230; that works.  You&#8217;d need to export your
<code>GOOGLE_APPLICATION_CREDENTIALS</code> as well&#8230; usual stuff.</p>

<p>However, as an easier alternative, use the cloud shell in the cloud console for
your <code>bigcorp.com</code> equivalent account.  The gcloud config and applcation
credentials are all already set up for you.</p>

<p>Side note: The cloud shell is <em>really</em> useful&#8230; check it out if you haven&#8217;t!</p>

<p>Make sure you&#8217;re driving terraform using credentials (your <code>gcloud</code> config)
from the equivalent of your <code>bigcorp.com</code> account and <em>not</em> your
<code>pinkponies.io</code> G-Suite org account.</p>

<h3 id="terraform">Terraform</h3>

<p>Download Terraform from <a href="https://terraform.io/">https://terraform.io/</a>.  Terraform is a standalone
binary so it&#8217;s simple to install&#8230; even in your GCP Cloud Shell.</p>

<p>Init terraform&#8217;s providers and state management</p>

<pre><code>terraform init
</code></pre>

<p>Then check out what changes we&#8217;re _plan_ning to make</p>

<pre><code>terraform plan
</code></pre>

<p>If all looks good from there, then <em>apply</em> that plan to actually create our
project</p>

<pre><code>terraform apply
</code></pre>

<p>Check out the project we just created</p>

<pre><code>gcloud beta billing projects list --billing-account=&lt;billing_account_id&gt;
</code></pre>

<p>Check out the same project from the Cloud Console for your <code>pinkponies.io</code>
G-Suite account.</p>

<p>Now you can use that account within your <code>pinkponies.io</code> G-Suite account and
any charges go straight to your BigCorp billing account.</p>

<h3 id="cleanup">Cleanup</h3>

<p>When you&#8217;re all done, you can clean up after yourself by removing the project
and role bindings we created</p>

<pre><code>terraform destroy
</code></pre>

<p>then deleting the billing account through the Cloud Console.  You could (and
should) totally manage the billing accounts themselves in the bigcorp.org using
Terraform templates as well, but that&#8217;s another story.</p>

<h2 id="disclaimer">Disclaimer</h2>

<p>No big corps or pink ponies were harmed in the production of this post.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Identifying User Activity from Streams of Raw Events]]></title>
    <link href="http://markmims.com/data-engineering/spark/hadoop/devops/2016/03/17/hwtb-sessions.html"/>
    <updated>2016-03-17T12:08:00+00:00</updated>
    <id>http://markmims.com/data-engineering/spark/hadoop/devops/2016/03/17/hwtb-sessions</id>
    <content type="html"><![CDATA[<p>I had a chance to speak at an online conference last weekend,
<a href="http://hadoop.withthebest.com/">Hadoop With The Best</a>.
I had fun sharing one of my total passions&#8230; data pipelines!
In particular, some techniques for catching raw user events,
acting on those events and understanding user activity from 
the sessionization of such events.</p>

<!--more-->

<p>
<div class="ratio-4-3 embed-video-container" onclick="var myAnchor = document.getElementById('7geRGzrIXRI');var tmpDiv = document.createElement('div');tmpDiv.innerHTML = '&lt;iframe style=&quot;vertical-align:top;width:100%;height:100%;position:absolute;&quot; src=&quot;http://www.youtube.com/embed/7geRGzrIXRI?autoplay=1&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;';myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;" title="click here to play">
<a class="youtube-lazy-link" style="width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/7geRGzrIXRI/0.jpg) center center no-repeat;background-size:contain;position:absolute" href="http://www.youtube.com/watch?v=7geRGzrIXRI" id="7geRGzrIXRI" onclick="return false;">
<div class="youtube-lazy-link-div"></div>
<div class="youtube-lazy-link-info">embedded youtube video 7geRGzrIXRI</div>
</a>
<div class="video-info" >embedded youtube video 7geRGzrIXRI</div>
</div>

</p>

<p><a href="svds.com">SVDS</a> is a boutique data science consulting firm.  We help folks
with their hardest Data Strategy, Data Science, and/or Data Engineering
problems.  In this role, we&#8217;re in a unique position to solve different
kinds of problems across various industries&#8230; and start to recognize
the patterns of solution that emerge.  That&#8217;s what I&#8217;d like to share.</p>

<p>This talk is about some common data pipeline patterns used
across various kinds of systems across various industries.
Key Takeaways include:</p>

<ul>
  <li>what&#8217;s needed to understand user activity</li>
  <li>pipeline architectures that support this analysis</li>
</ul>

<p>Along the way, I point out commonalities across business verticals and we see
how volume and latency requirements, unsurprisingly, turn out to be the biggest
differentiators in solution.</p>

<h2 id="agenda">Agenda</h2>

<ul>
  <li>Ingest Events</li>
  <li>Take Action</li>
  <li>Recognize Activity</li>
</ul>

<h2 id="ingest-events">Ingest Events</h2>

<p>The primary goal of an ingestion pipeline is to&#8230; ingest events.  All other
considerations are secondary.  We walk through an example pipeline and discuss
how that architecture changes as we adjust scaling up to handle billions of
events a day.  We&#8217;ll note along the way how general concepts of immutability
and lazy evaluation can have large ramifications on data ingestion pipeline
architecture.</p>

<p>I start out covering typical classes of and types of events, some common event
fields, and various ways that events are represented.  These vary <em>greatly</em>
across current and legacy systems, and you should always expect that munging
will be involved as you&#8217;re working to ingest events from various data sources
over time.</p>

<p>For our sessionization examples, we&#8217;re interested in user events such as
<code>login</code>, <code>checkout</code>, <code>add friend</code>, etc.</p>

<p>These user events can be &#8220;flat&#8221;</p>

<p><code>
    {
      "time_utc": "1457741907.959400112",
      "user_id": "688b60d1-c361-445b-b2f6-27f2eecfc217",
      "event_type": "button_pressed",
      "button_type": "one-click purchase",
      "item_sku": "1 23456 78999 9",
      "item_description": "Tony's Run-flat Tire",
      "item_unit_price": ...
      ...
    }
</code></p>

<p>or have some structure</p>

<p><code>
    {
      "time_utc": "1457741907.959400112",
      "user_id": "688b60d1-c361-445b-b2f6-27f2eecfc217",
      "event_type": "button_pressed",
      "event_details": {
        "button_type": "one-click purchase",
        "puchased_items": [
          {
            "sku": "1 23456 78999 9",
            "description": "Tony's Run-flat Tire",
            "unit_price": ...
            ...
          },
        ],
      },
      ...
    }
</code></p>

<p>and often both formats get used in the same systems in the wild so
you have to intelligently detect or classify events rather than 
just making blatant assumptions about them.  And yes, that <em>is</em> 
expensive&#8230; but it&#8217;s surprisingly common.</p>

<h3 id="ingestion-pipelines">Ingestion Pipelines</h3>

<p>So what do basic ingestion pipelines usually look like?</p>

<p>Tenants to keep in mind here&#8230; build a pipeline that&#8217;s immutable, lazy,
simple/composable, and testable.  I come back to these often throughout
the talk.</p>

<p>With our stated goal
of ingesting events, it should look pretty simple right?  Something along the
lines of
<a href="http://markmims.com/images/event-ingestion-without-streaming-with-filename.svg">
<img src="http://markmims.com/images/event-ingestion-without-streaming-with-filename.svg" width="720px" />
</a></p>

<p>I introduce the &#8220;Power of the Query Side&#8221;&#8230; query-side tools are fast
nowadays.  Tools such as Impala have really won me over.  The Ingest pipeline
needs to get the events as raw as possible as far back as possible in a format
that&#8217;s amenable to fast queries.  Let&#8217;s state that again&#8230; it&#8217;s important.
The pipeline&#8217;s core job is to get events that are as raw as possible (immutable
processing pipeline) as far back into the system as possible (lazily evaluated
analysis) before any expensive computation is done.  Modern query-side tools
support these paradigms quite well.  Better performance is obtained when events
land in query-optimized formats and are grouped into query-optimized files and
partitions where possible
<a href="http://markmims.com/images/event-ingestion-without-streaming-with-filename.svg">
<img src="http://markmims.com/images/event-ingestion-without-streaming-with-filename.svg" width="720px" />
</a></p>

<p>That&#8217;s simple enough and seems pretty straightforward in theory.  In practice
you can ingest events straight into files in hdfs only up to a certain scale
and degree of event complexity.</p>

<p>As scale increases, an ingestion pipeline has to become effectively a dynamic
impedance matching network.  It&#8217;s the funnel that&#8217;s catching events from what
can be a highly distributed, large number of data sources and trying to slam
all these events into a relatively small number of filesystem datanodes.</p>

<p>What can we we do to match those separate source sizes from target sizes?
<a href="http://markmims.com/images/events-without-streaming-question.svg">
<img src="http://markmims.com/images/events-without-streaming-question.svg" width="720px" />
</a>
use Spark!  :-)</p>

<p>No, but seriously, add a streaming solution in-between (I do like Spark
Streaming here)
<a href="http://markmims.com/images/streaming-bare.svg">
<img src="http://markmims.com/images/streaming-bare.svg" width="720px" />
</a>
and use Kafka to decouple all the bits
<a href="http://markmims.com/images/streaming-events-at-scale.svg">
<img src="http://markmims.com/images/streaming-events-at-scale.svg" width="720px" />
</a>
in such a way that your datasources on the left, and your datanodes on the
right can scale independently!  And independently from any stream computation
infrastructure you might need for in-stream decisions in the future.  I go
through that in a little more detail in the talk itself.</p>

<p>Impedance or size mismatches between data sources and data storage are really
only one half of the story.  Note that another culprit, <em>event complexity</em>, can
limit ingest throughput for a number of reasons.  A common example of where
this happens is when event &#8220;types&#8221; are either poorly defined or are changing so
much they&#8217;re hard to identify.  As event complexity increases, so does the
logic you use to group or partition the events so they&#8217;re fast to query.  In
practice this quickly grows from simple logic to full-blown event
classification algorithms.  Often those classification algorithms have to learn
from the body of events that&#8217;ve already landed.  You&#8217;re making decisions on
events in front of you based on all the events you&#8217;ve ever seen.  I&#8217;ll bump any
further discussion of that until we talk more about state in the &#8220;Recognize
Activity&#8221; section later.</p>

<p>Ingest pipelines can get complicated as you try to scale in size and
complexity&#8230; expect it!&#8230; plan for it!  The best way is to do this is to
build or use a toolchain that can let you add a streaming and queueing solution
without a lot of rearchitecture or downtime.  Folks often don&#8217;t try to solve
this problem until it&#8217;s already painful in production!  There&#8217;re great ways
to solve this in general.  My current fav atm uses a hybrid combination of
Terraform, Consul, Ansible, and ClouderaManager/Ambari.</p>

<p>Note also that we haven&#8217;t talked about any real-time processing or low-latency
business requirements here at all.  The need for a stream processing solution
arises when we&#8217;re <em>just</em> trying to catch events at scale.</p>

<hr />

<h2 id="take-action">Take Action</h2>

<p>Catching events within the system is an interesting challenge all by itself.
However, just efficiently and faithfully capturing events isn&#8217;t the end of the
story.</p>

<p><a href="http://markmims.com/images/streaming-bare.svg">
<img src="http://markmims.com/images/streaming-bare.svg" width="720px" />
</a></p>

<p>That&#8217;s sorta boring if we&#8217;re not taking <em>action</em> on events as we catch
them.</p>

<p>Actions such as </p>

<ul>
  <li>Notifications</li>
  <li>Decorations</li>
  <li>Routing / Gating</li>
  <li>Counting</li>
  <li>&#8230;</li>
</ul>

<p>can be taken in either &#8220;batch&#8221; or &#8220;real-time&#8221; modes.</p>

<p><a href="http://markmims.com/images/streaming-simple.svg">
<img src="http://markmims.com/images/streaming-simple.svg" width="720px" />
</a></p>

<p>Unfortunately, folks have all sorts of meanings for these terms.  Let&#8217;s clear
that up and be a little more precise&#8230;</p>

<p>For every action you intend to take, and really every data product of your
pipeline, you need to determine the latency requirements.  What is the
timeliness of that resulting action?  So how soon after either a.) an event was
generated, or b.) an event was seen within the system will that resulting
action be valid?  The answers might surprise you.</p>

<p>Latency requirements let you make a first-pass attempt at specifying the
<em>execution context</em> of each action.  There are two separate execution contexts we
talk about here&#8230; <em>batch</em> and <em>stream</em>.</p>

<ul>
  <li>
    <p>batch.  Asynchronous jobs that are potentially run against the entire body of
events and event histories.  These can be highly complex, computationally
expensive tasks that might involve a large amount of data from various
sources.  The implementations of these jobs can involve Spark or Hadoop
map-reduce code, Cascading-style frameworks, or even sql-based analysis via
Impala, Hive, or SparkSQL.</p>
  </li>
  <li>
    <p>stream.  Jobs that are run against either an individual event or a small
window of events.  These are typically simple, low-computation jobs that
don&#8217;t require context or information from other events.  These are typically
implemented using Spark-streaming or Storm code.</p>
  </li>
</ul>

<p>When I say &#8220;real-time&#8221; in this talk, I mean that the action will be taken from
within the stream execution context.</p>

<p>It&#8217;s important to realize that not all actions require &#8220;real-time&#8221; latency.
There are plenty of actions that are perfectly valid even if they&#8217;re operating
on &#8220;stale&#8221; day-old, hour-old, 15min-old data.  Of course, this sensitivity to
latency varies greatly by action, domain, and industry.  Also, how stale stream
-vs- batch events are depend of the actual performance characteristics of your
ingestion pipeline under load.  Measure all the things!</p>

<p>An approach I particularly like is to initially act from a batch context.
There&#8217;s generally less development effort, more computational resources, more
robustness, more flexibility, and more forgiveness involved when you&#8217;re working
in a batch execution context.  You&#8217;re less likely to interrupt or congest your
ingestion pipeline.</p>

<p>Once you have basic actions working from the batch layer, then do some
profiling and identify which of the actions you&#8217;re working with really require
less stale data.  <em>Selectively</em> bring those actions or analyses forward.  Tools
such as Spark can help tremendously with this.  It&#8217;s not all fully baked yet,
but there are ways to write spark code where the same business logic code can
be optionally bound in either stream or batch execution contexts.  You can move
code around based on pipeline requirements and performance!</p>

<p>In practice, a good deal of architecting such a pipeline is all about
preserving or protecting your stream ingestion and decision-making capabilities
for when you really need them.</p>

<p>A real system often involves additionally protecting and decoupling your stream
processing from making any service API calls (sending emails for example) by
adding kafka queues for things like outbound notifications <em>downstream</em> of
ingestion
<a href="http://markmims.com/images/streaming-with-notify-queues.svg">
<img src="http://markmims.com/images/streaming-with-notify-queues.svg" width="720px" />
</a>
as well as isolating your streaming system from writes to hdfs using 
the same trick (as we saw above)
<a href="http://markmims.com/images/streaming-two-layers.svg">
<img src="http://markmims.com/images/streaming-two-layers.svg" width="720px" />
</a></p>

<hr />

<h2 id="recognize-activity">Recognize Activity</h2>

<p>What&#8217;s user activity?  Usually it&#8217;s a <em>Sequence of one or more events</em>
associated with a user.  From an infrastructure standpoint, the key distinction
is that activity is constructed from a sequence of user events&#8230; <em>that don&#8217;t
all fit within a single window of stream processing</em>.  This can either be
because there are too many of them or because they&#8217;re spread out over too long
a period of time.</p>

<p>Another way to think of this is that event context matters.  In order to
recognize activity as such, you often need to capture or create user context
(let&#8217;s call it &#8220;state&#8221;) in such a way that it&#8217;s easily read by (and possibly
updated from) processing in-stream.</p>

<p>We add hbase to our standard stack, and use it to store state
<a href="http://markmims.com/images/classifying-with-state.svg">
<img src="http://markmims.com/images/classifying-with-state.svg" width="720px" />
</a></p>

<p>which is then accessible from either stream or batch processing.  HBase is
attractive as a fast key-value store.  Several other key-value stores could
work here&#8230; I&#8217;ll often start using one simply because it&#8217;s easier to
deploy/manage at first.  Then refine the choice of tool once more precise
performance requirements of the state store have emerged from use.</p>

<p>It&#8217;s important to note that you want fast key-based reads and writes.
Full-table scans of columns are pretty much verboten in this setup.  They&#8217;re
simply too slow for value from stream.</p>

<p>The usual approach is to update state in batch.  My favorite example when first
talking to folks about this approach is to consider a user&#8217;s credit score.
Events coming into the system are routed in stream based on the associated
user&#8217;s credit score.</p>

<p>The stream system can simply (hopefully quickly) look that up in HBase keyed
on a user id of some sort
<a href="http://markmims.com/images/hbase-state-credit-score.svg">
<img src="http://markmims.com/images/hbase-state-credit-score.svg" width="720px" />
</a>
The credit score is some number calculated by scanning across all a user&#8217;s
events over the years.  It&#8217;s a big, long-running, expensive computation.  Do
that continuously in batch&#8230; just update HBase as you go.  If you do that,
then you make that information available for decisions in stream.</p>

<p>Note that this is effectively a way to base fast-path decisions on
information learned from slow-path computation.  A way for the system to
quite literally <em>learn from the past</em>  :-)</p>

<p>Another example of this is tracking a package.  The events involved are the
various independent scans the package undergoes throughout its journey.</p>

<p>For &#8220;state&#8221; you might just want to keep an abbreviated version of the raw
history of each package
<a href="http://markmims.com/images/hbase-state-tracking-package.svg">
<img src="http://markmims.com/images/hbase-state-tracking-package.svg" width="720px" />
</a>
or just some derived notion of its state
<a href="http://markmims.com/images/hbase-state-tracking-package-derived.svg">
<img src="http://markmims.com/images/hbase-state-tracking-package-derived.svg" width="720px" />
</a>
those derived notions of state are tough to define from a single scan in a
warehouse somewhere&#8230; but make perfect sense when viewed in the context of the
entire package history.</p>

<hr />

<h2 id="wrap-up">Wrap-up</h2>

<p>I eventually come back to our agenda:</p>

<ul>
  <li>Ingest Events</li>
  <li>Take Action</li>
  <li>Recognize Activity</li>
</ul>

<p>Along the way we&#8217;ve done a nod to some data-plumbing best practices&#8230; such as</p>

<h4 id="the-power-of-the-query-side">The Power of the Query Side</h4>
<p>Query-side tools are fast &#8211; use them effectively!</p>

<h4 id="infrastructure-aspirations">Infrastructure Aspirations</h4>
<p>A datascience pipeline is</p>

<ul>
  <li>immutable</li>
  <li>lazy</li>
  <li>atomic
    <ul>
      <li>simple</li>
      <li>composable</li>
      <li>testable</li>
    </ul>
  </li>
</ul>

<p>When building datascience pipelines, these paradigms 
help you stay flexible and scalable</p>

<h4 id="automate-all-of-the-things">Automate All of the Things</h4>
<p>DevOps is your friend.  We&#8217;re using an interesting pushbutton stack that&#8217;ll be
the topic of another blog post :-)</p>

<h4 id="test-all-of-the-things">Test All of the Things</h4>
<p>TDD/BDD is your friend.  Again, I&#8217;ll add another post on &#8220;Sanity-Driven Data
Science&#8221; which is my take on TDD/BDD as applied to datascience pipelines.</p>

<h4 id="failure-is-a-first-class-citizen">Failure is a First Class Citizen</h4>
<p>Fail fast, early, often&#8230; along with the obligatory reference to the Netflix
Simian Army.</p>

<hr />

<h2 id="the-talk-itself">The Talk Itself</h2>

<p>It was a somewhat challenging presentation format.  I presented a live video
feed solo while the audience was watching live and had the ability to send
questions in via chat&#8230; no audio from the audience.  Somewhat reminiscent of
IRC-based presentations we used to do in Ubuntu community events&#8230; but with
video.</p>

<p>The moderator asked the audience to queue questions up until the end, but as
anyone who&#8217;s been in a classroom with me knows, I welcome / live for
interruptions :-) In this case, I could easily see the chat window as I
presented so asking-questions-along-the-way is supported on that presentation
platform.  I&#8217;d definitely ask for that in the future.</p>

<p>I do prefer the fireside chat nature of adding one or two more folks into the
feed&#8230; kinda like on-the-air hangouts&#8230; where the speaker can get audible
feedback from some folks.  Overall though this was a great experience and folks
asked interesting questions at the end.  I&#8217;m not sure how it&#8217;ll be published,
but questions had to be done in a second section as I dropped connectivity
right at the end of the speaking session.</p>

<p>Slides are available
<a href="http://archive.markmims.com/box/talks/2016-03-12-hwtb-sessions/slides.html">here</a>,
and you can get the video straight from the <a href="hadoop.with-the-best.com">hadoop with the
best</a> site.  Note that the slides are
<a href="https://github.com/hakimel/reveal.js/">reveal.js</a> and I make heavy use of
two-dimensional navigation.  Slides advance downwards, topics advance to the
right.</p>

<p>Update: this post has be perdied-up (thanks Meg!) and reposted as part of our
<a href="http://www.svds.com/building-pipelines-understand-user-behavior/">svds blog</a>.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Develop Spark Apps on Yarn using Docker]]></title>
    <link href="http://markmims.com/docker/data-engineering/spark/devops/2015/10/13/docker-cdh.html"/>
    <updated>2015-10-13T15:07:00+00:00</updated>
    <id>http://markmims.com/docker/data-engineering/spark/devops/2015/10/13/docker-cdh</id>
    <content type="html"><![CDATA[<p>At svds, we&#8217;ll often run spark on yarn in production.  Add some artful tuning
and this works pretty well.  However, developers typically build and test spark
application in <em>standalone</em> mode&#8230; not on yarn.</p>

<p>Rather than get bitten by the ideosyncracies involved in running spark on yarn
-vs- standalone when you go to deploy, here&#8217;s a way to set up a development
environment for spark that more closely mimics how it&#8217;s used in the wild.</p>

<!--more-->

<h2 id="a-simple-yarn-cluster-on-your-laptop">A simple yarn &#8220;cluster&#8221; on your laptop</h2>

<p>Run a docker image for a cdh standalone instance</p>

<pre><code>docker run -d --name=mycdh svds/cdh
</code></pre>

<p>when the logs</p>

<pre><code>docker logs -f mycdh
</code></pre>

<p>stop going wild, you can run the usual hadoop-isms to set up a workspace</p>

<pre><code>docker exec -it mycdh hadoop fs -ls /
docker exec -it mycdh hadoop fs -mkdir -p /tmp/blah
</code></pre>

<h2 id="run-spark">Run spark</h2>

<p>Then, it&#8217;s pretty straightforward to run spark against yarn</p>

<pre><code>docker exec -it mycdh \
  spark-submit \
    --master yarn-cluster \
    --class org.apache.spark.examples.SparkPi \
    /usr/lib/spark/examples/lib/spark-examples-1.3.0-cdh5.4.3-hadoop2.6.0-cdh5.4.3.jar \
    1000
</code></pre>

<p>Note that you can <em>submit</em> a spark job to run in either &#8220;yarn-client&#8221; or &#8220;yarn-cluster&#8221; modes.</p>

<p>In &#8220;yarn-client&#8221; mode, the spark driver runs outside of yarn and logs to
console and all spark executors run as yarn containers.</p>

<p>In &#8220;yarn-cluster&#8221; mode, all spark executors run as yarn containers, but then
the spark driver also runs as a yarn container.  Yarn manages all the logs.</p>

<p>You can also run the spark shell so that any workers spawned run in yarn</p>

<pre><code>docker exec -it mycdh spark-shell --master yarn-client
</code></pre>

<p>or</p>

<pre><code>docker exec -it mycdh pyspark --master yarn-client
</code></pre>

<h2 id="your-application">Your Application</h2>

<p>Ok, so <code>SparkPi</code> is all fine and dandy, but how do I run a real application?</p>

<p>Let&#8217;s make up an example.  Say you build your spark project on your laptop in the
<code>/Users/myname/mysparkproject/</code> directory.</p>

<p>When you build with maven or sbt, it typically builds and leaves jars under a
<code>/Users/myname/mysparkproject/target/</code> directory&#8230; for sbt, it&#8217;ll look like
<code>/Users/myname/mysparkproject/target/scala-2.10/</code>.</p>

<p>The idea here is to make these jars directly accessible from both your laptop&#8217;s
build process as well as from inside the cdh container.</p>

<p>When you start up the <code>cdh</code> container, map this local host directory up and
into the container</p>

<pre><code>docker run -d -v ~/mysparkproject/target:/target --name=mycdh svds/cdh 
</code></pre>

<p>where the <code>-v</code> option will make <code>~/mysparkproject/target</code>
available as <code>/target</code> within the container.</p>

<p>So,</p>

<pre><code>sbt clean assembly
</code></pre>

<p>leaves a jar under <code>~/mysparkproject/target</code>, which the container sees as
<code>/target</code> and you can run jobs using something like</p>

<pre><code>docker exec -it mycdh \
  spark-submit \
    --master yarn-cluster \
    --name MyFancySparkJob-name \
    --class org.markmims.MyFancySparkJob \
    /target/scala-2.10/My-assembly-1.0.1.20151013T155727Z.c3c961a51c.jar \
    myarg
</code></pre>

<p>The <code>--name</code> arg makes it easier to find in the midst of multiple yarn jobs.</p>

<h2 id="logs">Logs</h2>

<p>While a spark job is running, you can get its yarn &#8220;applicationId&#8221; from</p>

<pre><code>docker exec -it mycdh yarn application -list
</code></pre>

<p>or if it finished already just list things out with more conditions</p>

<pre><code>docker exec -it mycdh yarn application -list -appStates FINISHED
</code></pre>

<p>You can dig through the yarn-consolidated logs after the job is done
by using</p>

<pre><code>docker exec -it mycdh yarn logs -applicationId &lt;applicationId&gt;
</code></pre>

<h2 id="consoles">Consoles</h2>

<p>Web consoles are critical for application development.  Spend time up front
getting ports open or forwarded correctly for all environments.  Don&#8217;t wait
until you&#8217;re actually trying to debug something critical to figure out how to
forward ports to see the staging UI in all environments.</p>

<h3 id="yarn-resourcemanager-ui">Yarn ResourceManager UI</h3>

<p>Yarn gives you quite a bit of info about the system right from the
ResourceManager on its ip address and webgui port (usually 8088)</p>

<pre><code>open http://&lt;resource-manager-ip&gt;:&lt;resource-manager-port&gt;/
</code></pre>

<h3 id="spark-staging-ui">Spark Staging UI</h3>

<p>Yarn also conveniently proxies access to the spark staging UI for a given
application.  This looks like</p>

<pre><code>open http://&lt;resource-manager-ip&gt;:&lt;resource-manager-port&gt;/proxy/&lt;applicationId&gt;
</code></pre>

<p>for example,</p>

<pre><code>open http://localhost:8088/proxy/application_1444330488724_0005/
</code></pre>

<h3 id="ports-and-docker">Ports and Docker</h3>

<p>There are a few ways to deal with accessing port <code>8088</code> of the yarn resource
manager from outside of the docker container.  I typically use ssh for
everything and just forward ports out to <code>localhost</code> on the host.  However,
most people will expect to access ports directly on the <code>docker-machine ip</code>
address.  To do that, you have to map each port when you first spin up the
<code>cdh</code> container using the <code>-p 8088</code> option</p>

<pre><code>docker run -d -v target -p 8088 --name=mycdh svds/cdh 
</code></pre>

<p>Then you should be good to go with something like</p>

<pre><code>open http://`docker-machine ip`:8088/
</code></pre>

<p>to access the yarn console.</p>

<hr />

<h2 id="tips-and-gotchas">Tips and Gotchas</h2>

<ul>
  <li>
    <p>The docker image <code>svds/cdh</code> is quite large (2GB).  I like to do a separate
<code>docker pull</code> from any <code>docker run</code> commands just to isolate the download.
In fact, I recommend pinning the cdh version for the same reason&#8230; so
<code>docker pull svds/cdh:5.4.0</code> for instance, then refer to it that way
throughout <code>docker run -d --name=mycdh svds/cdh:5.4.0</code> and that&#8217;ll insure
you&#8217;re not littering your laptop&#8217;s filesystem with docker layers from
multiple cdh versions.  The bare <code>svds/cdh</code> (equiv to <code>svds/cdh:latest</code>)
floats with the most recent cloudera versions</p>
  </li>
  <li>
    <p>I&#8217;m using a CDH container here&#8230; but there&#8217;s an HDP one on the way as well.
Keep an eye out for it on <a href="`https://hub.docker.com/u/svds`">svds&#8217;s dockerhub page</a></p>
  </li>
  <li>
    <p>web consoles and forwarding ports through SSH</p>
  </li>
</ul>

<h2 id="bonus">Bonus</h2>

<p>Ok, so the downside here is that the image is fat.  The upside is that it lets
you play with the full suite of CDH-based tools.  I&#8217;ve tested out (besides the
spark variations above)</p>

<h3 id="impala-shell">Impala shell</h3>

<pre><code>docker exec mycdh impala-shell
</code></pre>

<h3 id="hbase-shell">HBase shell</h3>

<pre><code>docker exec mycdh hbase shell
</code></pre>

<h3 id="hive">Hive</h3>

<pre><code>echo "show tables;" | docker exec mycdh beeline -u jdbc:hive2://localhost:10000 -n username -p password -d org.apache.hive.jdbc.HiveDriver
</code></pre>

]]></content>
  </entry>
  
</feed>
